[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Handbook",
    "section": "",
    "text": "Table of Contents\n\n\n\nSr. No.\nTopic\nSummary\nStatus\nAuthor\n\n\n\n\n1\nDealing with missing-values\nThe notebook explores types of missingness and techniques for imputation.\nCompleted\nAlape Aniruddha\n\n\n2\nComprehensive Study on the Impact of Feature Scaling on Classification Models\nAn Introduction to Scaling.\nCompleted\nSherry Thomas\n\n\n3\nInvestigation of Standard Scaling Influence\nA deep dive into Standard Scaling.\nCompleted\nSherry Thomas\n\n\n4\nVisualizations for Data Science: An Overview\nA general overview on how data visualization is handled.\nCompleted\nSherry Thomas\n\n\n5\nComprehensive Data Visualization with Matplotlib and Seaborn\nCode and Implementation of various visualization techniques using Matplotlib and Seaborn.\nCompleted\nSherry Thomas\n\n\n6\nAccuracy\nThe notebook discusses accuracy in classification, its issues, calculation, and tuning using sklearn.\nCompleted\nAlape Aniruddha\n\n\n7\nF1 Score\nThe notebook discusses F1 Score in classification metrics using sklearn.\nCompleted\nAlape Aniruddha\n\n\n8\nRecall in Classification Metrics\nThe notebook explains recall in binary classification using sklearn metrics.\nCompleted\nAlape Aniruddha\n\n\n9\nPrecision in Classification Metrics\nThe notebook explores precision in classification, its calculation, and application.\nCompleted\nAlape Aniruddha\n\n\n10\nExploring the Significance of ROC AUC in Classification Models\nThe notebook explores ROC-AUC in classification, its calculation, and application.\nCompleted\nSherry Thomas\n\n\n11\nInductive Bias in Decision Trees and K-Nearest Neighbors\nExploring inductive bias impact on classifiers using synthetic datasets and algorithms.\nCompleted\nVivek Sivaramakrishnan\n\n\n12\nOrdinal Classification\nOrdinal classification explored using logistic regression and alternative encoding methods.\nCompleted\nVivek Sivaramakrishnan\n\n\n13\nEM Algorithm\nThe notebook demonstrates the Expectation-Maximization algorithm on a Gaussian Mixture Model.\nCompleted\nVivek Sivaramakrishnan\n\n\n14\nProbabilistic PCA\nThe notebook explores Probabilistic PCA through generative modeling and EM algorithm.\nCompleted\nVivek Sivaramakrishnan\n\n\n15\nPredictive Modeling of Patient Status in Primary Biliary Cirrhosis\nA case study, EDA and modeling on a medical dataset.\nCompleted\nSherry Thomas\n\n\n16\nPredicting Bank Customer Churn\nA case study, EDA and modeling on a banking dataset.\nCompleted\nSherry Thomas\n\n\n17\nAgglomerative Clustering\nHierarchical clustering with agglomerative approach, dendrograms, and linkage methods explored visually.\nCompleted\nVivek Sivaramakrishnan"
  },
  {
    "objectID": "pages/bank_churn.html",
    "href": "pages/bank_churn.html",
    "title": "Predicting Bank Customer Churn",
    "section": "",
    "text": "image\n\n\n\n\nCustomer churn, the act of customers discontinuing their relationship with a business, poses a significant challenge across industries, particularly within the banking sector. This Jupyter notebook focuses on predictive analysis of customer churn using a comprehensive dataset that captures various attributes of bank customers.\n\n\n\nThe dataset utilized in this analysis comprises crucial information about bank customers, distinguishing between those who have chosen to exit the bank and those who have remained as customers.\n\n\n\nThe primary objective of this analysis is to employ machine learning techniques to develop predictive models capable of determining whether a customer is likely to churn based on the provided dataset. By conducting exploratory data analysis (EDA), performing feature engineering, and building predictive models, this analysis aims to uncover underlying patterns and insights that can assist in identifying potential churn risks among bank customers."
  },
  {
    "objectID": "pages/bank_churn.html#introduction-and-dataset-overview",
    "href": "pages/bank_churn.html#introduction-and-dataset-overview",
    "title": "Predicting Bank Customer Churn",
    "section": "",
    "text": "image\n\n\n\n\nCustomer churn, the act of customers discontinuing their relationship with a business, poses a significant challenge across industries, particularly within the banking sector. This Jupyter notebook focuses on predictive analysis of customer churn using a comprehensive dataset that captures various attributes of bank customers.\n\n\n\nThe dataset utilized in this analysis comprises crucial information about bank customers, distinguishing between those who have chosen to exit the bank and those who have remained as customers.\n\n\n\nThe primary objective of this analysis is to employ machine learning techniques to develop predictive models capable of determining whether a customer is likely to churn based on the provided dataset. By conducting exploratory data analysis (EDA), performing feature engineering, and building predictive models, this analysis aims to uncover underlying patterns and insights that can assist in identifying potential churn risks among bank customers."
  },
  {
    "objectID": "pages/bank_churn.html#data-exploration",
    "href": "pages/bank_churn.html#data-exploration",
    "title": "Predicting Bank Customer Churn",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nDataset Snapshot\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv('train.csv', index_col='id')\ntest_df = pd.read_csv('test.csv', index_col='id')\ndata.head()\n\n\n\n\n\n\n\n\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n15674932\nOkwudilichukwu\n668\nFrance\nMale\n33.0\n3\n0.00\n2\n1.0\n0.0\n181449.97\n0\n\n\n1\n15749177\nOkwudiliolisa\n627\nFrance\nMale\n33.0\n1\n0.00\n2\n1.0\n1.0\n49503.50\n0\n\n\n2\n15694510\nHsueh\n678\nFrance\nMale\n40.0\n10\n0.00\n2\n1.0\n0.0\n184866.69\n0\n\n\n3\n15741417\nKao\n581\nFrance\nMale\n34.0\n2\n148882.54\n1\n1.0\n1.0\n84560.88\n0\n\n\n4\n15766172\nChiemenam\n716\nSpain\nMale\n33.0\n5\n0.00\n2\n1.0\n1.0\n15068.83\n0\n\n\n\n\n\n\n\n\nObservations:\n\nCustomerID: Unique identifiers for each customer.\nSurname: Last name of the customers.\nCreditScore: Numerical representation of the customer’s credit score.\nGeography: Country of residence, including France and Spain.\nGender: Gender of the customers, primarily Male in this subset.\nAge: Age of the customers, varying between 33 and 40.\nTenure: The duration, in years, of the customer’s relationship with the bank.\nBalance: Account balance, with some entries showing 0.00.\nNumOfProducts: The number of bank products utilized by customers, mostly 2.\nHasCrCard: Indicates whether the customer has a credit card (1 = yes, 0 = no).\nIsActiveMember: Indicates whether the customer is an active member (1 = yes, 0 = no).\nEstimatedSalary: The estimated salary of the customers, with varying values.\n\n\n\nInsights:\n\nSeveral customers have a balance of 0.00, suggesting potential areas for further investigation.\nMost customers have more than one bank product.\nThe dataset includes customers from different countries with varying credit scores, ages, and tenure.\n\nThis initial glimpse provides a snapshot of the dataset, setting the stage for further exploratory analysis and model development.\n\n\n\nDataset Information Summary\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 165034 entries, 0 to 165033\nData columns (total 13 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   CustomerId       165034 non-null  int64  \n 1   Surname          165034 non-null  object \n 2   CreditScore      165034 non-null  int64  \n 3   Geography        165034 non-null  object \n 4   Gender           165034 non-null  object \n 5   Age              165034 non-null  float64\n 6   Tenure           165034 non-null  int64  \n 7   Balance          165034 non-null  float64\n 8   NumOfProducts    165034 non-null  int64  \n 9   HasCrCard        165034 non-null  float64\n 10  IsActiveMember   165034 non-null  float64\n 11  EstimatedSalary  165034 non-null  float64\n 12  Exited           165034 non-null  int64  \ndtypes: float64(5), int64(5), object(3)\nmemory usage: 17.6+ MB\n\n\nThe dataset summary obtained from the train.info() function offers valuable insights into the dataset structure and characteristics:\n\nTotal Rows: The dataset contains a total of 165,034 entries, indexed from 0 to 165,033.\nColumns: There are 13 columns in total, each representing a specific attribute of the dataset.\nData Types:\n\nInteger (int64): Columns like CustomerId, CreditScore, Tenure, NumOfProducts, and Exited are stored as integers.\nFloat (float64): Attributes such as Age, Balance, HasCrCard, IsActiveMember, and EstimatedSalary are represented as floating-point numbers.\nObject (string): Surname, Geography, and Gender columns contain string (object) data types.\n\nMemory Usage: The dataset consumes approximately 17.6+ MB of memory.\n\n\nObservations:\n\nNon-Null Counts: All columns exhibit a non-null count of 165,034, indicating that there are no missing values present in the dataset.\nColumn Types: The dataset consists of a balanced combination of numerical (integer and float) and categorical (object) columns, enabling a diverse analysis of customer attributes.\n\n\n\nInsights:\n\nThe absence of missing values across all columns ensures a complete dataset, facilitating a comprehensive analysis.\nThe diverse data types suggest the necessity for appropriate preprocessing steps, such as encoding categorical variables and scaling numerical attributes, before feeding the data into machine learning models.\n\n\n\n\nStatistical Summary of the Dataset\n\ndata.describe()\n\n\n\n\n\n\n\n\nCustomerId\nCreditScore\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\ncount\n1.650340e+05\n165034.000000\n165034.000000\n165034.000000\n165034.000000\n165034.000000\n165034.000000\n165034.000000\n165034.000000\n165034.000000\n\n\nmean\n1.569201e+07\n656.454373\n38.125888\n5.020353\n55478.086689\n1.554455\n0.753954\n0.497770\n112574.822734\n0.211599\n\n\nstd\n7.139782e+04\n80.103340\n8.867205\n2.806159\n62817.663278\n0.547154\n0.430707\n0.499997\n50292.865585\n0.408443\n\n\nmin\n1.556570e+07\n350.000000\n18.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n11.580000\n0.000000\n\n\n25%\n1.563314e+07\n597.000000\n32.000000\n3.000000\n0.000000\n1.000000\n1.000000\n0.000000\n74637.570000\n0.000000\n\n\n50%\n1.569017e+07\n659.000000\n37.000000\n5.000000\n0.000000\n2.000000\n1.000000\n0.000000\n117948.000000\n0.000000\n\n\n75%\n1.575682e+07\n710.000000\n42.000000\n7.000000\n119939.517500\n2.000000\n1.000000\n1.000000\n155152.467500\n0.000000\n\n\nmax\n1.581569e+07\n850.000000\n92.000000\n10.000000\n250898.090000\n4.000000\n1.000000\n1.000000\n199992.480000\n1.000000\n\n\n\n\n\n\n\nThe statistical summary provides key insights into the numerical attributes of the dataset:\n\nCustomerID:\n\nCount: 165,034 unique customer IDs are available in the dataset.\n\nCreditScore:\n\nMean: The average credit score of customers is approximately 656.45.\nMinimum: The minimum credit score observed is 350.\nMaximum: The maximum credit score recorded is 850.\n\nAge:\n\nMean: The average age of customers is around 38.13 years.\nMinimum: The youngest customer in the dataset is 18 years old.\nMaximum: The oldest customer recorded is 92 years old.\n\nTenure:\n\nMean: The average tenure of customers with the bank is approximately 5.02 years.\nMinimum: The minimum tenure observed is 0 years.\nMaximum: The maximum tenure recorded is 10 years.\n\nBalance:\n\nMean: The mean account balance of customers is approximately 55,478.09.\nMinimum: Some customers have a zero balance in their accounts.\nMaximum: The highest account balance observed is around 250,898.09.\n\nNumOfProducts:\n\nMean: On average, customers utilize around 1.55 bank products.\nMinimum: The minimum number of bank products used is 1.\nMaximum: The maximum number of bank products used is 4.\n\nHasCrCard:\n\nMean: Around 75.4% of customers have a credit card.\n\nIsActiveMember:\n\nMean: Approximately 49.8% of customers are active members.\n\nEstimatedSalary:\n\nMean: The average estimated salary of customers is roughly 112,574.82.\nMinimum: The minimum estimated salary recorded is 11.58.\nMaximum: The highest estimated salary observed is around 199,992.48.\n\nExited:\n\nMean: Around 21.2% of customers have churned.\n\n\n\nObservations:\n\nThe dataset contains diverse ranges of attributes, including credit scores, ages, tenures, account balances, and estimated salaries.\nNotably, a significant portion of customers have zero balance in their accounts.\nThe average number of products used by customers is approximately 1.55, with a maximum of 4 products.\nRoughly 21.2% of customers in the dataset have churned.\n\n\n\nInsights:\n\nUnderstanding the distribution and range of these attributes will be crucial for feature selection and building predictive models to identify potential churn risks among customers.\n\n\n\n\nPie Plot Analysis of Categorical Variables\n\n# Define a threshold to determine categorical columns based on unique value counts\ncategorical_threshold = 10  # You can adjust this threshold based on your dataset characteristics\n\n# Get columns with less than the defined threshold as categorical columns\ncat_cols = [col for col in data.drop([\"Exited\", \"CustomerId\", \"Surname\"], axis=1) \n            if data[col].nunique() &lt; categorical_threshold]\n\n# Remaining columns are considered numerical\nnum_cols = [col for col in data.drop([\"Exited\", \"CustomerId\", \"Surname\"], axis=1) if col not in cat_cols]\n\n\ndef plot_target(data: pd.DataFrame, col: str, title: str, pie_colors: list, test_df: pd.DataFrame = pd.DataFrame()) -&gt; None:\n    if not test_df.empty:\n        fig, axes = plt.subplots(1, 4, figsize=(20, 6), gridspec_kw={'width_ratios': [2, 1, 2, 1]})\n        \n        for i, data in enumerate([data, test_df]):\n            textprops={'fontsize': 12, 'weight': 'bold', 'color': 'black'}\n            ax = axes[i * 2]\n            \n            ax.pie(data[col].value_counts().to_list(),\n                colors=pie_colors,\n                labels=data[col].value_counts().index.to_list(),\n                autopct='%1.f%%',\n                explode=([.05] * data[col].nunique()),\n                pctdistance=0.5,\n                wedgeprops={'linewidth': 1, 'edgecolor': 'black'},\n                textprops=textprops)\n\n            sns.countplot(x=col, data=data, palette='pastel6', hue=col, order=data[col].value_counts().to_dict().keys(), ax=axes[i * 2 + 1])\n\n            for p, count in enumerate(data[col].value_counts().to_dict().values(), 0):\n                axes[i * 2 + 1].text(p - 0.1, count + (np.sqrt(count)), count, color='black', fontsize=13)\n\n            plt.setp(axes[i * 2 + 1].get_xticklabels(), fontweight='bold')\n            plt.yticks([], ax=axes[i * 2 + 1])\n            axes[i * 2 + 1].set_ylabel('')\n            axes[i * 2 + 1].set_xlabel('')\n            # axes[i * 2 + 1].get_legend().remove()\n            # plt.box(False, ax=axes[i * 2 + 1])\n\n            axes[i * 2].set_title(f'Distribution in {\"Train\" if i == 0 else \"Test\"} Set', fontsize=16, fontweight='bold')\n\n        fig.suptitle(x=0.5, y=1.05, t=f'► {title} Distribution ◄', fontsize=18, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(15, 6), width_ratios=[2,1])\n\n        textprops={'fontsize': 12, 'weight': 'bold',\"color\": \"black\"}\n        ax[0].pie(data[col].value_counts().to_list(),\n                colors=pie_colors,\n                labels=data[col].value_counts().index.to_list(),\n                autopct='%1.f%%', \n                explode=([.05]*data[col].nunique()),\n                pctdistance=0.5,\n                wedgeprops={'linewidth' : 1, 'edgecolor' : 'black'}, \n                textprops=textprops)\n\n        sns.countplot(x = col, data=data, palette = \"pastel6\", hue=col, order=data[col].value_counts().to_dict().keys())\n        for p, count in enumerate(data[col].value_counts().to_dict().values(),0):\n            ax[1].text(p-0.1, count+(np.sqrt(count)), count, color='black', fontsize=13)\n        plt.setp(ax[1].get_xticklabels(), fontweight=\"bold\")\n        plt.yticks([])\n        plt.box()\n        fig.suptitle(x=0.56, t=f'► {title} Distribution ◄', fontsize=18, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n\n“Exited” Distribution\n\nplot_target(data,\n            col='Exited', \n            title='Exited', \n            pie_colors=['lightblue', 'lightgreen', 'orange'])\n\n\n\n\nThe pie chart representing the distribution of customers who have exited and those who haven’t shows a clear distinction. Approximately 21% of customers have churned (Exited), while the remaining 79% have chosen to continue their relationship with the bank. This disparity indicates an imbalance in the churn behavior within the dataset.\n\n\nCategorical Variable Distributions (Train and Test Sets)\n\nfor cat_col in cat_cols:\n    plot_target(data=data,\n                test_df=test_df, \n                col=cat_col, \n                title=cat_col, \n                pie_colors=['lightblue', 'lightgreen', 'orange'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe subsequent plots illustrate the distributions of various categorical columns in both the train and test datasets. Across the categorical attributes namely as Geography and Gender, the distributions remain notably similar between the train and test sets. This similarity suggests that the categorical characteristics of customers are consistent between the datasets, indicating a reliable alignment in terms of categorical attributes across both training and testing data\n\ndef plot_numerical_comparison(train_data, test_data, num_cols):\n    num_rows = len(num_cols)\n    fig, axes = plt.subplots(num_rows, 2, figsize=(15, num_rows * 5), gridspec_kw={'width_ratios': [1, 1]})\n\n    for i, col in enumerate(num_cols):\n        sns.kdeplot(train_data[col], ax=axes[i][0], fill=True, color='skyblue', linewidth=2.5, edgecolor='black')\n        axes[i][0].set_title(f'Train Set - {col}', fontsize=16, fontweight='bold')\n        axes[i][0].set_xlabel(col, fontsize=12)\n        axes[i][0].set_ylabel('Density', fontsize=12)\n        axes[i][0].tick_params(labelsize=10)\n\n        sns.kdeplot(test_data[col], ax=axes[i][1], fill=True, color='lightgreen', linewidth=2.5, edgecolor='black')\n        axes[i][1].set_title(f'Test Set - {col}', fontsize=16, fontweight='bold')\n        axes[i][1].set_xlabel(col, fontsize=12)\n        axes[i][1].set_ylabel('Density', fontsize=12)\n        axes[i][1].tick_params(labelsize=10)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_numerical_comparison(train_data=data, test_data=test_df, num_cols=num_cols)\n\n\n\n\nUpon comparing the numerical feature distributions between the train and test datasets, striking similarities emerge, signifying a strong alignment in the data characteristics across both sets. The KDE plots reveal overlapping distributions for features such as CreditScore, Age, Tenure, Balance, NumOfProducts, and EstimatedSalary. The proximity and consistency in the shapes, peaks, and spread of these distributions across the train and test datasets indicate a remarkable resemblance in the statistical profiles of the numerical attributes. This similarity suggests that the statistical properties and patterns captured by these numerical features are well-preserved between the two datasets, fostering confidence in the model’s ability to generalize effectively across similar numerical feature spaces in both the training and testing phases."
  },
  {
    "objectID": "pages/bank_churn.html#data-preprocessing",
    "href": "pages/bank_churn.html#data-preprocessing",
    "title": "Predicting Bank Customer Churn",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nOne-Hot Encoding and Data Preparation\n\nOne-Hot Encoding Categorical Variables\nIn preparation for the utilization of categorical variables within machine learning models, the process of one-hot encoding was applied to the categorical columns present in the bank customer churn dataset. One-hot encoding transforms categorical variables into a binary format, enabling the incorporation of categorical attributes into predictive models effectively.\n\ncat_cols = data.drop([\"CustomerId\", \"Surname\", \"Exited\"], axis=1).select_dtypes(include='object').columns.tolist()\nnum_cols = data.drop([\"CustomerId\", \"Surname\", \"Exited\"], axis=1).select_dtypes(exclude='object').columns.tolist()\n\n\ndata_cat = pd.get_dummies(data, columns=cat_cols, drop_first=True, dtype=int)\n\nThe pd.get_dummies() function from the Pandas library was used to convert categorical variables into numerical representations suitable for model training. This process expanded the categorical columns into binary columns, creating new binary features for each category within the original categorical variables. The drop_first=True parameter was employed to drop the first level of each categorical variable to prevent multicollinearity in the dataset, reducing the risk of introducing redundant information.\n\n\nFeature-Target Splitting\nFollowing the one-hot encoding, the dataset was prepared for machine learning modeling by splitting it into feature variables (X) and the target variable (y).\n\nX = data_cat.drop([\"CustomerId\", \"Surname\", \"Exited\"], axis=1)\ny = data_cat[\"Exited\"]\n\n\nFeature Variables (X): The feature variables (X) were obtained by excluding the Exited column, which serves as the target variable for prediction. These feature variables comprise both the encoded categorical attributes and numerical features, forming the input dataset ready for training machine learning models.\nTarget Variable (y): The Exited column represents the target variable containing binary labels indicating whether a customer churned (1) or not (0). The y variable consists of these binary labels, serving as the target for training the machine learning models. This setup enables the models to learn patterns and make predictions regarding customer churn based on the provided features.\n\nThis process of segregating feature variables from the target variable establishes the foundation for subsequent model training, validation, and evaluation stages in the predictive modeling workflow.\n\n\n\nScaling using StandardScaler\nIn the domain of data preprocessing, scaling serves as a critical step in standardizing numerical features, harmonizing different attributes that may possess varying scales and distributions. For this bank customer churn dataset, employing the StandardScaler technique proves beneficial in transforming the numerical features to a standardized scale without significantly affecting the underlying distribution.\n\nStandardScaler Explanation:\nStandardScaler is a scaling technique commonly used to transform features by centering them around the mean and scaling to unit variance. It achieves this by subtracting the mean and dividing by the standard deviation of each feature. Unlike some other scaling methods, such as RobustScaler or MinMaxScaler, StandardScaler is sensitive to outliers, as it is based on mean and standard deviation normalization.\n\n\nApplication of StandardScaler:\nIn the context of the bank customer churn dataset, numerical attributes such as Credit Score, Age, Tenure, Balance, NumOfProducts, and EstimatedSalary may have varying scales and distributions. Applying StandardScaler to these features facilitates the transformation to a common scale, ensuring that all features contribute equally during model training. However, it’s important to note that StandardScaler may be affected by outliers, potentially impacting the scaling process if outliers are present in the dataset.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initializing RobustScaler\nscaler = StandardScaler()\n\n# Scaling numerical features in the train dataset\nX = scaler.fit_transform(X)"
  },
  {
    "objectID": "pages/bank_churn.html#exploratory-data-analysis-eda",
    "href": "pages/bank_churn.html#exploratory-data-analysis-eda",
    "title": "Predicting Bank Customer Churn",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nCorrelation Analysis\n\n# Calculating correlation matrix\ncorrelation_matrix = pd.concat([pd.DataFrame(X, columns=data_cat.drop([\"CustomerId\", \"Surname\", \"Exited\"], axis=1).columns), pd.DataFrame(y, columns=[\"Exited\"])], axis=1).corr()\n\n# Plotting a heatmap to visualize correlations\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation Heatmap of Features')\nplt.xlabel('Features')\nplt.ylabel('Features')\nplt.show()\n\n\n\n\n\nCorrelation Coefficients:\nThe correlation matrix provides insights into the relationships between various features and the target variable ‘Exited’ in the bank customer churn dataset. Analyzing the correlation coefficients reveals notable associations and tendencies within the dataset.\n\nAge and Exited: A relatively strong positive correlation (0.341) is observed between ‘Age’ and the target variable ‘Exited’. This suggests that older customers tend to exhibit a higher likelihood of churning, indicating that age may play a significant role in customer attrition.\nNumOfProducts and Exited: There exists a moderate negative correlation (-0.215) between the number of products (‘NumOfProducts’) a customer uses and the likelihood of churning (‘Exited’). Customers with a higher number of products seem less inclined to churn, implying potential loyalty or engagement due to multiple product use.\nIsActiveMember and Exited: The ‘IsActiveMember’ feature displays a moderate negative correlation (-0.210) with ‘Exited’. This suggests that active members—customers who frequently engage with bank services—are less likely to churn, highlighting the importance of fostering customer engagement and activity.\nGeography_Germany and Exited: Notably, the ‘Geography_Germany’ feature exhibits a moderate positive correlation (0.211) with ‘Exited’. Customers from Germany show a slightly higher tendency to churn compared to other geographic regions, indicating a geographical influence on churn behavior.\nGender_Male and Exited: While ‘Gender_Male’ displays a mild negative correlation (-0.146) with ‘Exited’, the correlation suggests a slight tendency for male customers to churn less compared to female customers. However, the correlation is relatively weak compared to other features.\n\n\n\nInsights and Considerations:\n\nAge as a Predictor: The strong positive correlation between age and churn suggests that older customers are more likely to churn. Therefore, age can be a valuable predictive feature in models aimed at forecasting customer churn.\nEngagement and Product Usage: The negative correlations of ‘NumOfProducts’ and ‘IsActiveMember’ with churn indicate that higher product usage and active engagement can serve as protective factors against churn, underscoring the importance of customer engagement strategies.\nGeographic Influence: The correlation of ‘Geography_Germany’ with churn implies potential regional variations in churn behavior, warranting a deeper exploration of geographic factors impacting customer retention strategies.\n\nThese correlations provide valuable insights into potential predictors of customer churn, guiding the selection of features for predictive modeling and the development of targeted retention strategies within the banking domain."
  },
  {
    "objectID": "pages/bank_churn.html#model-building",
    "href": "pages/bank_churn.html#model-building",
    "title": "Predicting Bank Customer Churn",
    "section": "Model Building",
    "text": "Model Building\n\nCross-validation and Train-Test Split\n\nStratified K-Fold Cross-Validation\nCross-validation is a crucial technique used to assess the performance and generalizability of machine learning models. Stratified K-Fold cross-validation, implemented through StratifiedKFold, is particularly advantageous when working with classification tasks, maintaining the distribution of the target variable’s classes across folds.\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n# Initializing Stratified K-Fold with 5 folds\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nIn this code snippet, the StratifiedKFold object is created with parameters: - n_splits=5: Divides the dataset into 5 folds for cross-validation. - shuffle=True: Shuffles the data before splitting to ensure randomness. - random_state=42: Sets a random seed for reproducibility.\n\n\nTrain-Test Split\nThe train_test_split function partitions the dataset into training and testing sets, facilitating model training and evaluation.\n\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the dataset into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(X), y, test_size=0.2, random_state=3935, stratify=y)\n\nHere, train_test_split:\n\nX and y are the feature and target variables, respectively.\ntest_size=0.2: Allocates 20% of the data for testing, leaving 80% for training.\nrandom_state=3935: Sets a specific seed for reproducibility in random sampling.\nstratify=y: Ensures that the splitting preserves the proportion of classes in the target variable ‘y’.\n\nCombining StratifiedKFold for cross-validation and train_test_split for initial training and testing partitions ensures robust model validation and evaluation, contributing to more reliable model performance estimation.\nThis approach facilitates both cross-validation to assess model performance across multiple folds and the creation of distinct training and testing sets for initial model training and evaluation.\n\n\n\nTraining Multiple LightGBM Models with Cross-Validation\nThe code snippet demonstrates the training of multiple LightGBM models using StratifiedKFold for cross-validation and evaluating their performance.\n\n%%capture\n\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n\n# List to store trained LightGBM models\nlg_models = []\n\n# Parameters for the LightGBM model\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'num_iterations': 1000,\n    'random_state': 42,\n    'verbose': -1,\n    'lambda_l1': 8.585041572171024e-07,\n    'lambda_l2': 0.15953603581592163,\n    'num_leaves': 45,\n    'feature_fraction': 0.574576430132381,\n    'bagging_fraction': 0.9507639254203872,\n    'bagging_freq': 10,\n    'min_child_samples': 74,\n    'learning_rate': 0.0315188927471299,\n}\n\n# Training multiple LightGBM models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    LGBModel = lgb.LGBMClassifier(**params)\n    LGBModel.fit(X_train.iloc[x_idx, :], y_train.iloc[x_idx], eval_set=[(X_train.iloc[val_idx, :], y_train.iloc[val_idx])])\n    lg_models.append(LGBModel)\n\nExplanation:\n\nlg_models: This list stores trained LightGBM models.\nparams: Represents the hyperparameters configuration for the LightGBM model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nLGBModel.fit(): Trains the LightGBM model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\nlg_score: Initializes a variable to store the cumulative AUC score across all models.\nfor i, LGBModel in enumerate(lg_models): Loops through the trained models and evaluates each on the test set using roc_auc_score() function. It prints the ROC-AUC Score for each model.\n\n\n# Evaluating the models on the test set\nfor i, LGBModel in enumerate(lg_models):\n    y_pred = LGBModel.predict_proba(X_test)[:, 1]\n    print(f'Model {i+1} ROC-AUC Score: ', roc_auc_score(y_test, y_pred))\n\nModel 1 ROC-AUC Score:  0.8901894527481835\nModel 2 ROC-AUC Score:  0.8902050267982606\nModel 3 ROC-AUC Score:  0.8907646031603289\nModel 4 ROC-AUC Score:  0.8900732044729662\nModel 5 ROC-AUC Score:  0.8904569792190615\n\n\nThe ROC-AUC scores obtained from five LightGBM models exhibit remarkable similarity, with minimal variation among them. Models 1, 2, 3, 4, and 5 showcase highly comparable ROC-AUC scores ranging between 0.890 and 0.891. This uniformity in scores suggests consistent predictive abilities across these models in discerning between churned and non-churned customers. The closely aligned ROC-AUC scores imply a consistent performance level, signifying that these models possess similar discriminatory power in their predictions. Further exploration into potential feature importance or parameter adjustments may enhance the models’ abilities to distinguish subtle nuances in customer churn behavior.\n\n\nTraining Multiple XGBoost Models with Cross-Validation\nThe following code trains multiple XGBoost models using StratifiedKFold for cross-validation and evaluates their performance using ROC-AUC Score.\n\n%%capture\n\nimport xgboost as xgb\n\n# List to store trained XGBoost models\nxgb_models = []\n\n# Parameters for the XGBoost model\nparams ={\n    'eval_metric': 'auc',\n    'random_state': 42,\n    'use_label_encoder': False,\n    'verbose': -1,\n    'n_estimators': 980,\n    'max_depth': 6,\n    'learning_rate': 0.01758494874070708,\n    'subsample': 0.9741597683093374,\n    'colsample_bytree': 0.594454559433606,\n    'gamma': 0.5625993189587196,\n    'min_child_weight': 3.7532444898898,\n    'reg_alpha': 0.5492749440306521,\n    'reg_lambda': 3.673821292504123,\n}\n\n# Training multiple XGBoost models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    xgb_model = xgb.XGBClassifier(**params)\n    xgb_model.fit(X_train.iloc[x_idx, :], y_train.iloc[x_idx], eval_set=[(X_train.iloc[val_idx, :], y_train.iloc[val_idx])])\n    xgb_models.append(xgb_model)\n\nExplanation:\n\nxgb_models: This list stores trained XGBoost models.\nparams: Represents the hyperparameters configuration for the XGBoost model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nxgb_model.fit(): Trains the XGBoost model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\nxgb_score: Initializes a variable to store the cumulative AUC score across all models.\nfor i, xgb_model in enumerate(xgb_models): Loops through the trained models and evaluates each on the test set using roc_auc_score() function. It prints the ROC-AUC Score for each model.\n\n\n# Evaluating the models on the test set\nfor i, xgb_model in enumerate(xgb_models):\n    y_pred = xgb_model.predict_proba(X_test)[:, 1]\n    print(f'Model {i+1} ROC-AUC Score: ', roc_auc_score(y_test, y_pred))\n\nModel 1 ROC-AUC Score:  0.8914294260971625\nModel 2 ROC-AUC Score:  0.891120718217416\nModel 3 ROC-AUC Score:  0.891558865958693\nModel 4 ROC-AUC Score:  0.8912372443550356\nModel 5 ROC-AUC Score:  0.8912609837081735\n\n\nThe ROC-AUC scores obtained from five XGBoost models highlight their consistent and closely aligned predictive performance. Models 1, 2, 3, 4, and 5 exhibit remarkably similar ROC-AUC scores, ranging between 0.891 and 0.892. This uniformity in scores underscores the robust and consistent discriminatory power of these XGBoost models in distinguishing between churned and non-churned customers. The striking similarity in ROC-AUC scores across these models signifies their cohesive predictive abilities, implying a consistent performance level in capturing nuanced patterns related to customer churn behavior. Further exploration into feature importance or fine-tuning model parameters may help refine these models for even more nuanced predictive accuracy.\n\n\nTraining CatBoost Models and Evaluating Performance\nThe following code trains multiple CatBoost models using Stratified K-Fold cross-validation and assesses their performance using ROC-AUC Score:\n\nfrom catboost import CatBoostClassifier\n\n# List to store trained CatBoost models\ncat_models = []\n\n# Parameters for the CatBoost model\nparams = {\n    'objective': 'Logloss',\n    'eval_metric': 'AUC',\n    'iterations': 1000,\n    'random_seed': 42,\n    'verbose': 0,\n    'learning_rate': 0.09257074265362737,\n    'depth': 4,\n    'l2_leaf_reg': 18.394210351106825,\n    'random_strength': 0.06104405239569854,\n    'bagging_temperature': 1.2698738887388927e-07,\n    'border_count': 178,\n}\n\n# Training multiple CatBoost models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    cat_model = CatBoostClassifier(**params)\n    cat_model.fit(X_train.iloc[x_idx, :], y_train.iloc[x_idx], eval_set=[(X_train.iloc[val_idx, :], y_train.iloc[val_idx])])\n    cat_models.append(cat_model)\n\nExplanation:\n\ncat_models: This list stores trained CatBoost models.\nparams: Represents the hyperparameters configuration for the CatBoost model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nCatBoostClassifier.fit(): Trains the CatBoost model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\ncat_score: Initializes a variable to store the cumulative ROC-AUC Score across all CatBoost models.\nfor i, cat_model in enumerate(cat_models): Loops through the trained models and evaluates each on the test set using roc_auc_score() function. It prints the ROC-AUC Score for each model.\n\n\n# Evaluating the models on the test set\nfor i, cat_model in enumerate(cat_models):\n    y_pred = cat_model.predict_proba(X_test)[:, 1]\n    print(f'Model {i+1} ROC-AUC Score: ', roc_auc_score(y_test, y_pred))\n\nModel 1 ROC-AUC Score:  0.8907765649991798\nModel 2 ROC-AUC Score:  0.8908115233906881\nModel 3 ROC-AUC Score:  0.8906539341420548\nModel 4 ROC-AUC Score:  0.8909832423551305\nModel 5 ROC-AUC Score:  0.8910059555431601\n\n\nThe ROC-AUC scores obtained from five CatBoost models demonstrate strikingly similar performance across the board. Models 1, 2, 3, 4, and 5 display consistent ROC-AUC scores tightly clustered between 0.891 and 0.891. This uniformity in scores underscores the robust predictive capacity of these CatBoost models in effectively discriminating between churned and non-churned customers. The closely aligned ROC-AUC scores across these models indicate their coherent and consistent predictive performance, suggesting a stable ability to capture subtle nuances associated with customer churn behavior. To further enhance predictive accuracy, future investigations may focus on feature importance analysis or model parameter fine-tuning to uncover potential refinements.\n\n\nElevating Predictive Power with Stacked Ensemble Model\nNow, let’s explore the construction of a more robust predictive model through a technique called Stacking. Stacking involves combining multiple machine learning models, leveraging their diverse strengths to enhance overall predictive performance. In this section, we’ll build a Stacked Ensemble Model using an MLPClassifier as the final estimator.\n\nModel Configuration:\n\nMLPClassifier: A Multi-layer Perceptron (MLP) neural network with 64 and 32 neurons in its hidden layers, employing the ‘relu’ activation function, ‘adam’ solver, and various hyperparameters for optimization.\n\n\n\nStackingClassifier Configuration:\n\nEstimators: The StackingClassifier utilizes predictions from previously trained models, including LGBM, XGBoost, and CatBoost.\nFinal Estimator: The final estimator, an MLPClassifier, aggregates predictions from the base models.\nCross-Validation (cv): Employing Stratified K-Fold cross-validation ensures robustness in model evaluation and performance estimation.\n\n\n\nImplementation:\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Initializing an MLPClassifier\nmlp = MLPClassifier(\n    hidden_layer_sizes=(64, 32),\n    max_iter=1000,\n    random_state=42,\n    activation='relu',\n    learning_rate_init=0.001,\n    solver='adam',\n    validation_fraction=0.1,\n    momentum=0.9,\n    nesterovs_momentum=True,\n    batch_size=32,\n    beta_1=0.9,\n    beta_2=0.999\n)\n\n# Creating a StackingClassifier\nstacking_model = StackingClassifier(\n    estimators=[\n        ('LGBM', LGBModel),\n        ('XGB', xgb_model),\n        ('CAT', cat_model)\n    ],\n    final_estimator=mlp,\n    cv=skf\n)\n\n\n%%capture\n\n# Fitting the StackingClassifier on the training data\nstacking_model.fit(X_train, y_train)\n\n\n\nExplanation:\nThe StackingClassifier combines predictions from diverse base models (LGBM, XGBoost, CatBoost) and utilizes an MLPClassifier as the final layer to learn and make predictions based on the diverse outputs. This stacking technique aims to improve predictive accuracy by leveraging the collective knowledge of multiple models, potentially capturing a more nuanced understanding of the data and enhancing overall performance on unseen test data. The model fitting is conducted using the training data, and subsequent predictions are generated for evaluation and assessment of the ensemble model’s effectiveness."
  },
  {
    "objectID": "pages/bank_churn.html#model-evaluation",
    "href": "pages/bank_churn.html#model-evaluation",
    "title": "Predicting Bank Customer Churn",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nEvaluating Stacked Ensemble Model Performance\n\nModel Evaluation Metrics:\nTo assess the performance of the Stacked Ensemble Model, several evaluation metrics are computed using the model’s predictions on the test dataset.\n\n\nEvaluation Metrics Computed:\n\nLog Loss: A measure of uncertainty in the model’s predictions.\nAccuracy: Proportion of correctly predicted outcomes.\nPrecision: Measure of the model’s exactness in predicting each class.\nRecall: Measure of the model’s completeness in capturing each class.\nF1 Score: Harmonic mean of precision and recall, providing a balanced assessment.\n\n\n\nConfusion Matrix and Classification Report:\n\nConfusion Matrix: Tabulation of actual vs. predicted class counts, aiding in understanding misclassifications.\nClassification Report: Detailed summary showcasing precision, recall, F1 score, and support for each class.\n\n\n\nEvaluation Process and Metrics Computation:\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n\n# Evaluate the model on the test data\ny_pred = stacking_model.predict_proba(X_test)[:, 1]\n\n# Calculate and print evaluation metrics\nauc = roc_auc_score(y_test, y_pred)\n\ny_pred = stacking_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Print classification report and confusion matrix\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\nAUC: 0.8917\nAccuracy: 0.8659\nPrecision: 0.8581\nRecall: 0.8659\nF1 Score: 0.8571\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.95      0.92     26023\n           1       0.76      0.54      0.63      6984\n\n    accuracy                           0.87     33007\n   macro avg       0.82      0.75      0.77     33007\nweighted avg       0.86      0.87      0.86     33007\n\nConfusion Matrix:\n[[24817  1206]\n [ 3221  3763]]\n\n\n\n\n\nInsights from Evaluation Metrics\n\n1. Performance Metrics:\n\nAUC: The Area Under the ROC Curve (AUC) of 0.8917 signifies a relatively good discriminative ability of the model in distinguishing between positive and negative instances.\nAccuracy: The model achieved an accuracy of 0.8659, indicating the proportion of correctly predicted instances among the total predictions.\nPrecision: Precision of 0.8581 denotes the ratio of correctly predicted positive observations to the total predicted positives, reflecting the model’s ability to avoid false positives.\nRecall: A recall score of 0.8659 demonstrates the model’s capability to identify actual positives from the total actual positives, also known as sensitivity or true positive rate.\nF1 Score: The F1 score of 0.8571, which balances precision and recall, signifies the model’s overall accuracy in classification.\n\n\n\n2. Classification Report Insights:\n\nSupport: The ‘support’ column indicates the number of actual occurrences of each class in the dataset.\nPrecision, Recall, F1-Score:\n\nClass 0 (Non-churned customers): The model exhibits high precision (0.89), indicating a low false positive rate. The recall (0.95) suggests effective identification of non-churned customers. The F1-score (0.92) signifies a good balance between precision and recall for this class.\nClass 1 (Churned customers): The model displays lower precision (0.76) and recall (0.54), indicating a higher false positive rate and missed churned customers, respectively. The F1-score (0.63) for churned customers is comparatively lower, reflecting a trade-off between precision and recall.\n\n\n\n\n3. Confusion Matrix Analysis:\n\nTrue Positives (TP) and True Negatives (TN): The model correctly predicted 24,817 instances of non-churned customers (Class 0) and 3,763 instances of churned customers (Class 1).\nFalse Positives (FP) and False Negatives (FN): It misclassified 1,206 instances of non-churned customers as churned (Type I error - false alarms) and 3,221 instances of churned customers as non-churned (Type II error - missed opportunities).\n\n\n\n4. Overall Assessment:\n\nThe model demonstrates better performance in predicting non-churned customers (Class 0) compared to churned customers (Class 1).\nThere is a trade-off between precision and recall for predicting churned customers, indicating potential room for improvement in identifying churn instances more accurately.\nThe model’s F1-score, accuracy, and AUC indicate reasonable performance, yet there’s scope for enhancing the model’s ability to detect churned customers while minimizing false predictions."
  },
  {
    "objectID": "pages/bank_churn.html#conclusion-and-future-steps",
    "href": "pages/bank_churn.html#conclusion-and-future-steps",
    "title": "Predicting Bank Customer Churn",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nConclusion\nThe Stacked Ensemble Model for our dataset displayed promising performance, particularly in AUC score, a critical metric for our predictive task. It achieved an impressive AUC score of 0.8917, showcasing strong discriminatory power in distinguishing between churned and non-churned customers. However, a more detailed analysis reveals areas for potential enhancement.\n\n\nKey Findings\n\nAUC Performance: The model demonstrated a robust AUC score, indicating its ability to effectively differentiate between churned and non-churned customers.\nClass-Specific Metrics: While the model showcased favorable metrics for non-churned customers (Class 0), there is room for improvement in accurately identifying churned customers (Class 1).\n\n\n\nInsights for Improvement\n\nBalancing Class Prediction: Focusing on optimizing predictions for churned customers (Class 1) could further enhance the model’s overall predictive performance.\nFine-Tuning and Optimization: Further refinement of model parameters and ensemble techniques could lead to improved predictions, especially for the crucial churn prediction task.\nFeature Investigation: In-depth analysis and augmentation of features might unveil more influential factors, potentially strengthening the model’s predictive capabilities.\n\n\n\nFuture Steps\n\nAUC-Oriented Model Tuning: Prioritize parameter tuning specifically aimed at enhancing AUC performance to ensure a better distinction between churn and non-churn instances.\nFeature Enhancement: Investigate feature engineering techniques to unveil new informative features or transformations, directly impacting the AUC performance.\nEnsemble Diversification: Explore diverse ensemble strategies or models to enhance the diversity and robustness of predictions, particularly focused on improving AUC.\nValidation Strategies: Employ rigorous validation methods, specifically emphasizing AUC validation, to ensure consistent and robust model performance.\nInterpreting Model Decisions: Devote efforts to interpret the model’s decisions, aiding in understanding areas where improvements can be made to optimize AUC performance.\n\n\n\nFinal Note\nContinued refinement and exploration of tailored techniques focusing on improving AUC performance are paramount. By addressing the identified areas for enhancement and leveraging advanced methodologies, the model can evolve into an even more effective tool for precise churn prediction, crucial for real-world applications in the banking domain."
  },
  {
    "objectID": "pages/bank_churn.html#generating-predictions-for-test-data",
    "href": "pages/bank_churn.html#generating-predictions-for-test-data",
    "title": "Predicting Bank Customer Churn",
    "section": "Generating Predictions for Test Data",
    "text": "Generating Predictions for Test Data\n\ntest_df = pd.read_csv(\"test.csv\", index_col='id')\ntest_df = pd.get_dummies(test_df, columns=cat_cols, drop_first=True, dtype=int)\n\nX_pred = test_df.drop([\"CustomerId\", \"Surname\"], axis=1).copy()\nX_pred = scaler.transform(X_pred)\n\ny_test_pred = stacking_model.predict_proba(X_pred)[:, 1]\nclass_labels = ['Exited']\n\n# Create a DataFrame with the given output array\noutput_df = pd.DataFrame(y_test_pred, columns=class_labels)\n\n# Add ID column to the DataFrame\noutput_df['id'] = test_df.index\n\n# Reorder columns to have 'id' as the first column\noutput_df = output_df[['id'] + class_labels]\n\n# Create csv of y_test with columns \"id\" and \"Hardness\"\noutput_df.to_csv('submission.csv', index=False)"
  },
  {
    "objectID": "pages/f1.html",
    "href": "pages/f1.html",
    "title": "F1 Score",
    "section": "",
    "text": "The f1_score is a metric that combines precision and recall into a single value and can be interpreted as the harmonic mean of the two.\nThe formula for the F1 Score is:\n\\displaystyle \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\nThe F1 Score ranges from 0 to 1, where 1 indicates perfect precision and recall, and the lowest possible value is 0, if either precision or recall is 0.\n\n\n\\displaystyle \\text{F1 Score} = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}\nWhere, TP = True Positives FP = False Positives FN = False Negatives\n\n\n\nf1_score is the function in scikit-learn used to calculate the F1 Score.\nThe parameters are as follows:\n\ny_true: Ground truth (correct) labels.\ny_pred: Predicted labels, as returned by a classifier.\nlabels (None): The set of labels to include when average is not None.\npos_label (1): The label of the positive class.\naverage (‘binary’): The averaging strategy for multiclass settings.\nsample_weight (None): Sample weights.\n\n\nfrom sklearn.metrics import f1_score\n\ny_true = [0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 1, 0, 1]\n\nf1_score(y_true, y_pred)\n\n0.8571428571428571\n\n\n\n\n\nWe are going to calculate the F1 Score of a support vector machine (SVM) classifier model on the breast_cancer dataset using sklearn.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nf1_score(y_test, y_pred)\n\n0.9617486338797814\n\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7a881ea3f220&gt;\n\n\n\n\n\nIn the above matrix, we can see that: TP = 88, TN = 48, FP = 6, FN = 1\n\\begin{aligned}\n{\\displaystyle \\text{F1 Score}} & ={\\displaystyle \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}}\\\\\n& \\\\\n& {\\displaystyle =\\frac{2 \\cdot \\text{88}}{2 \\cdot \\text{88 + 6 + 1}}}\\\\\n& \\\\\n& {\\displaystyle =\\ \\frac{\\text{176}}{\\text{183}}}\\\\\n& \\\\\n& {\\displaystyle =\\ 0.9617}\n\\end{aligned}"
  },
  {
    "objectID": "pages/f1.html#calculating-f1-score-for-binary-classification-problems",
    "href": "pages/f1.html#calculating-f1-score-for-binary-classification-problems",
    "title": "F1 Score",
    "section": "",
    "text": "\\displaystyle \\text{F1 Score} = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}\nWhere, TP = True Positives FP = False Positives FN = False Negatives"
  },
  {
    "objectID": "pages/f1.html#f1-score-in-sklearn",
    "href": "pages/f1.html#f1-score-in-sklearn",
    "title": "F1 Score",
    "section": "",
    "text": "f1_score is the function in scikit-learn used to calculate the F1 Score.\nThe parameters are as follows:\n\ny_true: Ground truth (correct) labels.\ny_pred: Predicted labels, as returned by a classifier.\nlabels (None): The set of labels to include when average is not None.\npos_label (1): The label of the positive class.\naverage (‘binary’): The averaging strategy for multiclass settings.\nsample_weight (None): Sample weights.\n\n\nfrom sklearn.metrics import f1_score\n\ny_true = [0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 1, 0, 1]\n\nf1_score(y_true, y_pred)\n\n0.8571428571428571"
  },
  {
    "objectID": "pages/f1.html#f1-score-on-a-real-world-dataset",
    "href": "pages/f1.html#f1-score-on-a-real-world-dataset",
    "title": "F1 Score",
    "section": "",
    "text": "We are going to calculate the F1 Score of a support vector machine (SVM) classifier model on the breast_cancer dataset using sklearn.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nf1_score(y_test, y_pred)\n\n0.9617486338797814\n\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7a881ea3f220&gt;\n\n\n\n\n\nIn the above matrix, we can see that: TP = 88, TN = 48, FP = 6, FN = 1\n\\begin{aligned}\n{\\displaystyle \\text{F1 Score}} & ={\\displaystyle \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}}\\\\\n& \\\\\n& {\\displaystyle =\\frac{2 \\cdot \\text{88}}{2 \\cdot \\text{88 + 6 + 1}}}\\\\\n& \\\\\n& {\\displaystyle =\\ \\frac{\\text{176}}{\\text{183}}}\\\\\n& \\\\\n& {\\displaystyle =\\ 0.9617}\n\\end{aligned}"
  },
  {
    "objectID": "pages/recall.html",
    "href": "pages/recall.html",
    "title": "Recall in Classification Metrics",
    "section": "",
    "text": "Recall, also known as sensitivity or true positive rate, is a classification metric that measures the ability of a model to capture all instances of the positive class. It is defined as the ratio of true positives to the sum of true positives and false negatives.\n\\displaystyle \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\nRecall is particularly important in situations where missing positive instances is costly."
  },
  {
    "objectID": "pages/recall.html#calculating-recall-for-binary-classification-problems",
    "href": "pages/recall.html#calculating-recall-for-binary-classification-problems",
    "title": "Recall in Classification Metrics",
    "section": "Calculating Recall for Binary Classification Problems",
    "text": "Calculating Recall for Binary Classification Problems\n\\displaystyle \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\nWhere, TP = True Positives FN = False Negatives"
  },
  {
    "objectID": "pages/recall.html#where-can-recall-be-used",
    "href": "pages/recall.html#where-can-recall-be-used",
    "title": "Recall in Classification Metrics",
    "section": "Where can Recall be used?",
    "text": "Where can Recall be used?\nRecall is a useful metric when missing positive instances is more critical then incorrectly classifying negative instances. - Consider the case of disease diagnosis. If the patient has the disease(positive) and our model predicts it as negative, then our model has failed to detect the disease and this can have crucial consequences. - In Fraud detection, if the primary objective is to capture as many fraudulent transactions as possible even at the cost of flagging legitimate transactions as fraud. If our model predicts fradulent transaction as legitimate, this increases the value of False Negatives thereby reducing the value of recall"
  },
  {
    "objectID": "pages/recall.html#recall-in-sklearn",
    "href": "pages/recall.html#recall-in-sklearn",
    "title": "Recall in Classification Metrics",
    "section": "Recall in sklearn",
    "text": "Recall in sklearn\nrecall_score is the function in scikit-learn used to calculate recall.\nThe parameters are as follows:\n\ny_true: Ground truth (correct) labels.\ny_pred: Predicted labels, as returned by a classifier.\nlabels (None): The set of labels to include when average is not None.\npos_label (1): The label of the positive class.\naverage (‘binary’): The averaging strategy for multiclass settings.\nsample_weight (None): Sample weights.\n\n\nfrom sklearn.metrics import recall_score\n\ny_true = [0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 1, 0, 1]\n\nrecall_score(y_true, y_pred)\n\n0.75"
  },
  {
    "objectID": "pages/recall.html#recall-on-a-real-world-dataset",
    "href": "pages/recall.html#recall-on-a-real-world-dataset",
    "title": "Recall in Classification Metrics",
    "section": "Recall on a real-world dataset",
    "text": "Recall on a real-world dataset\nWe are going to calculate the recall of a decision tree classifier model on the breast_cancer dataset using sklearn.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nrecall_score(y_test, y_pred)\n\n0.9325842696629213\n\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7987c534c670&gt;\n\n\n\n\n\nIn the above matrix, we can see that: TP = 83, TN = 51, FP = 3, FN = 6\n\\begin{aligned}\n{\\displaystyle \\text{Recall}} & ={\\displaystyle \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}}\\\\\n& \\\\\n& {\\displaystyle =\\frac{\\text{83}}{\\text{83 + 6}}}\\\\\n& \\\\\n& {\\displaystyle =\\ \\frac{\\text{83}}{\\text{89}}}\\\\\n& \\\\\n& {\\displaystyle =\\ 0.9325}\n\\end{aligned}"
  },
  {
    "objectID": "pages/vis2.html",
    "href": "pages/vis2.html",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/vis2.html#introduction",
    "href": "pages/vis2.html#introduction",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Introduction",
    "text": "Introduction\nData visualization is an essential tool in the field of data analysis and interpretation. It allows us to gain insights from complex data by representing it in a visual format. In this Jupyter notebook, we will explore various data visualization techniques using Matplotlib and Seaborn, two popular Python libraries. These techniques cater to the needs of Computer Science and Data Science students, helping them understand and utilize visualization methods effectively."
  },
  {
    "objectID": "pages/vis2.html#table-of-contents",
    "href": "pages/vis2.html#table-of-contents",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIntroduction\nBasic Plots\n\nLine Plot\nScatter Plot\nBar Plot\nHistogram\n\nStatistical Plots\n\nBox Plot\nViolin Plot\nSwarm Plot\n\nMatrix Plots\n\nHeatmap\nClustermap\n\nDistribution Plots\n\nKDE Plot\nPair Plot\n\nTime Series Plots\n\nTime Series Plot\nAutoCorrelation Plot\n\nGeospatial Data Visualization\n\nScatter Geo Plot\nChoropleth Map\n\n3D Plots\n\n3D Scatter Plot\n3D Line Plot\n\nSpecialized Plots\n\nPolar Plot\nNetwork Plot\nWord Cloud\n\nAdvanced Data Visualization\n\nROC Curves and AUC\nt-SNE Plots\n\nConclusion\nAdditional Notes"
  },
  {
    "objectID": "pages/vis2.html#basic-plots",
    "href": "pages/vis2.html#basic-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "1: Basic Plots",
    "text": "1: Basic Plots\nIn this section, we will delve into a comprehensive exploration of basic data visualization techniques, collectively known as “Basic Plots.” These fundamental visualizations are crucial for understanding data trends, relationships, and distributions. We will cover Line Plots, Scatter Plots, Bar Plots, and Histograms, each offering a unique perspective on data representation.\n\n1.1: Line Plot (Visualizing Trends Over Time)\nLine plots are a fundamental tool for visualizing data trends, particularly those that evolve over time. In this subsection, we will use a synthetic time-series dataset, such as stock market data, to illustrate the significance of line plots.\nCreating a Line Plot:\nWe will begin by generating synthetic time-series data, including time points and corresponding stock prices. Then, we will use Matplotlib to craft an informative line plot.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic time-series data\ntime = np.arange(0, 10, 0.1)\nstock_prices = np.sin(time) + np.random.normal(0, 0.2, len(time))\n\n# Creating a line plot\nplt.figure(figsize=(10, 6))\nplt.plot(time, stock_prices, label='Stock Prices', color='b', linestyle='-', marker='o')\nplt.xlabel('Time')\nplt.ylabel('Stock Prices')\nplt.title('Stock Price Trends Over Time')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe resulting line plot provides a visual representation of stock price trends over time. It offers customization options such as line style, color, and labels to enhance clarity.\nInterpreting Line Plots:\nInterpreting a line plot involves assessing various aspects:\n\nTrends: Observe the direction of the line to identify upward, downward, or stable trends in the data.\nAmplitude: The vertical distance of the line from the baseline signifies the magnitude of changes in the variable being measured.\nCyclic Patterns: Some time-series data exhibit cyclic patterns or seasonality, which can be spotted in the plot.\nVariability: Variations in the data are reflected in the fluctuations of the line.\n\nLine plots are essential for detecting temporal patterns, understanding data evolution, and making informed decisions based on historical data.\n\n\n1.2: Scatter Plot (Visualizing Relationships Between Variables)\nScatter plots are valuable for visualizing the relationships between two numeric variables. In this subsection, we will use synthetic data representing height vs. weight to demonstrate the utility of scatter plots.\nCreating a Scatter Plot:\nWe will generate synthetic height and weight data and then employ Matplotlib to create a comprehensive scatter plot.\n\n# Generating synthetic height vs. weight data\nheight = np.random.normal(170, 10, 100)\nweight = height * 0.6 + np.random.normal(0, 5, 100)\n\n# Creating a scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(height, weight, label='Height vs. Weight', color='r', marker='o')\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')\nplt.title('Relationship between Height and Weight')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe scatter plot visually illustrates the relationship between height and weight, allowing for the identification of patterns and correlations.\nInterpreting Scatter Plots:\nInterpreting a scatter plot involves considering several key aspects:\n\nTrend Direction: Determine if the points exhibit an upward, downward, or random trend.\nScatter Density: The density of points in different areas of the plot indicates data concentration.\nOutliers: Identify any data points that deviate significantly from the general pattern, which might be outliers.\nCorrelation: Assess the overall direction and strength of the relationship between the variables.\n\nScatter plots are essential for understanding the correlation between two variables and identifying potential outliers or trends.\n\n\n1.3: Bar Plot (Visualizing Categorical Data)\nBar plots are instrumental for representing categorical data. In this subsection, we will use synthetic sales data by product category to demonstrate the effectiveness of bar plots.\nCreating a Bar Plot:\nWe will generate synthetic sales data categorized by product type and then create a bar plot using Matplotlib.\n\n# Generating synthetic sales data by product category\ncategories = ['Electronics', 'Clothing', 'Books', 'Home Decor']\nsales = [1200, 800, 1500, 900]\n\n# Creating a bar plot\nplt.figure(figsize=(10, 6))\nplt.bar(categories, sales, color='g', alpha=0.7)\nplt.xlabel('Product Categories')\nplt.ylabel('Sales')\nplt.title('Sales by Product Category')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe bar plot visually represents the sales data by product category, offering insights into categorical data representation.\nInterpreting Bar Plots:\nInterpreting a bar plot involves considering the following aspects:\n\nCategory Comparison: Compare the heights of bars to understand variations in sales among different categories.\nCategorical Representation: Observe how categories are represented on the x-axis.\nColor Usage: Color can be utilized to highlight specific categories or add visual appeal to the plot.\nStacked vs. Grouped Bars: Depending on the data representation, bars can be stacked or grouped for better comprehension.\n\nBar plots are essential for comparing categorical data and understanding the distribution of values within categories.\n\n\n1.4: Histogram (Visualizing Data Distribution)\nHistograms are powerful tools for visualizing the distribution of a single variable. In this subsection, we will use synthetic exam score data to create a histogram.\nCreating a Histogram:\nWe will generate synthetic exam scores and then employ Matplotlib to create an informative histogram.\n\n# Generating synthetic exam score data\nexam_scores = np.random.normal(70, 10, 300)\n\n# Creating a histogram\nplt.figure(figsize=(10, 6))\nplt.hist(exam_scores, bins=20, color='purple', edgecolor='black', alpha=0.7)\nplt.xlabel('Exam Scores')\nplt.ylabel('Frequency')\nplt.title('Distribution of Exam Scores')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe histogram visually represents the distribution of exam scores, offering customization options for bin size and normalization.\nInterpreting Histograms:\nInterpreting a histogram involves considering several key aspects:\n\nData Distribution: Assess whether the data is normally distributed, skewed, or exhibits other patterns.\nCentral Tendency: Identify the central tendency of the data, such as the mean or median.\nDispersion: Examine the spread or variability of the data.\nBin Width: The width of histogram bins can affect the visual representation of the distribution.\n\nHistograms are essential for understanding the distribution of a single variable and identifying patterns in the data."
  },
  {
    "objectID": "pages/vis2.html#statistical-plots",
    "href": "pages/vis2.html#statistical-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "2: Statistical Plots",
    "text": "2: Statistical Plots\nIn this section, we will dive into a comprehensive exploration of statistical data visualization techniques, collectively known as “Statistical Plots.” These visualizations are particularly suited for gaining insights into data distributions, identifying outliers, and understanding the central tendencies and variations within datasets. We will cover Box Plots, Violin Plots, and Swarm Plots, each offering a unique perspective on data distribution and statistical characteristics.\n\n2.1: Box Plot (Visualizing Distribution Characteristics)\nBox plots, often referred to as box-and-whisker plots, are powerful tools for visualizing the distribution and central tendencies of a dataset. They provide valuable information about the quartiles, outliers, and the spread of data. To illustrate the utility of box plots, we will utilize a synthetic dataset representing income distribution.\nCreating a Box Plot:\nWe will commence by generating synthetic income distribution data and then proceed to create an informative box plot using Matplotlib.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic income distribution data\nincome_data = np.random.normal(50000, 10000, 500)\n\n# Creating a box plot\nplt.figure(figsize=(10, 6))\nplt.boxplot(income_data, vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'))\nplt.xlabel('Income')\nplt.title('Income Distribution')\nplt.grid(axis='x')\nplt.show()\n\n\n\n\nThe resulting box plot offers an intuitive representation of income distribution, where the box’s boundaries denote the interquartile range, the median is indicated by the central line, and whiskers extend to minimum and maximum values. The use of color adds an additional layer of visualization.\nInterpreting Box Plots:\nInterpreting a box plot involves analyzing several key aspects:\n\nMedian (Q2): The central line inside the box represents the median income, providing insight into the dataset’s central tendency.\nInterquartile Range (IQR): The span of the box represents the IQR, indicating the spread of data between the 25th and 75th percentiles.\nWhiskers: The whiskers extend from the box to the minimum and maximum values within the dataset, highlighting potential outliers.\nOutliers: Any data points beyond the whiskers are considered outliers, which may warrant further investigation.\n\nBox plots are valuable for comparing the distributions of different datasets and identifying variations in data characteristics.\n\n\n2.2: Violin Plot (Combining Box Plot and KDE)\nViolin plots are a hybrid of box plots and Kernel Density Estimation (KDE) plots, offering a more detailed view of data distribution. These plots are especially useful when you need to visualize the shape and density of the dataset. To demonstrate the capabilities of violin plots, we will continue using the synthetic income distribution data.\nCreating a Violin Plot:\nWe will take the income distribution data and craft a violin plot that combines the benefits of box plots and KDE to provide a richer representation.\n\n# Creating a violin plot\nplt.figure(figsize=(10, 6))\nplt.violinplot(income_data, vert=False, showmedians=True, showextrema=True)\nplt.xlabel('Income')\nplt.title('Income Distribution (Violin Plot)')\nplt.grid(axis='x')\nplt.show()\n\n\n\n\nIn the resulting plot, you can observe a combination of the classic box plot and a KDE representation, providing a more comprehensive understanding of data distribution.\nInterpreting Violin Plots:\nWhen interpreting violin plots, consider the following:\n\nWidth of the Violin: The width of the violin at any given value indicates the density of data points at that level. Wider sections represent higher data density.\nBox within the Violin: Just like in a box plot, the central box in the violin plot represents the IQR, and the central line is the median.\nViolin Extrema: The extrema, represented as small lines or points, highlight the minimum and maximum values in the dataset.\n\nViolin plots are effective for capturing both the central tendencies and the variations in data, making them a powerful tool in exploratory data analysis.\n\n\n2.3: Swarm Plot (Visualizing Categorical Data)\nSwarm plots are excellent for visualizing categorical data with multiple categories, showcasing individual data points within these categories. To exemplify the utility of swarm plots, we will employ synthetic survey response data, which is often categorical and offers a prime use case for this type of visualization.\nCreating a Swarm Plot:\nWe will generate synthetic survey response data and construct a swarm plot using the Seaborn library, which excels in creating aesthetically pleasing and informative categorical plots.\n\n# Generating synthetic survey response data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncategories = ['Category A', 'Category B', 'Category C', 'Category D']\nresponses = np.random.choice(categories, size=100)\n\n# Creating a swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x=responses, y=np.random.normal(0, 1, 100), palette='Set2', hue=responses, legend=False)\nplt.xlabel('Survey Categories')\nplt.ylabel('')\nplt.title('Survey Responses')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe resulting swarm plot showcases individual survey responses distributed along the categorical axis, revealing the distribution of data points within each category.\nInterpreting Swarm Plots:\nSwarm plots are particularly useful for:\n\nVisualizing Distribution: The positions of individual data points offer a clear view of how responses are distributed within each category.\nIdentifying Clustering: Patterns or clustering of responses within categories can be observed, aiding in the identification of trends or commonalities among responses.\n\nSwarm plots are an excellent choice when working with categorical data and seeking insights into the distribution and clustering of responses."
  },
  {
    "objectID": "pages/vis2.html#matrix-plots",
    "href": "pages/vis2.html#matrix-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "3: Matrix Plots",
    "text": "3: Matrix Plots\nMatrix plots are essential for visualizing relationships and patterns in data, particularly when dealing with multivariate datasets. This section will provide an in-depth exploration of matrix plots, focusing on Heatmaps and Clustermaps. These visualization techniques offer a comprehensive view of data interactions and similarities, aiding in the discovery of hidden insights within complex datasets.\n\n3.1: Heatmap (Visualizing Correlations)\nHeatmaps are powerful tools for visualizing correlation matrices of variables. These visualizations allow us to gain insights into how variables interact with each other, identify patterns, and assess the strength and direction of these relationships. For our demonstration, we will create a synthetic correlation matrix and generate an informative heatmap.\nCreating a Heatmap:\nWe begin by generating a synthetic correlation matrix and then proceed to create a compelling heatmap using Seaborn.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Generating a synthetic correlation matrix\ncorrelation_matrix = np.corrcoef(np.random.rand(5, 5))\n\n# Creating a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\nThe resulting heatmap visually represents correlations between variables. It uses a color map to accentuate the strength of the relationships. In this example, warmer colors indicate positive correlations, cooler colors represent negative correlations, and the annotation provides precise correlation values.\nInterpreting Heatmaps:\nInterpreting a heatmap involves analyzing the following aspects:\n\nColor Intensity: The intensity of color at the intersection of two variables signifies the strength of their correlation. Darker colors represent stronger correlations.\nColor Direction: Warm colors (e.g., red and orange) indicate positive correlations, while cool colors (e.g., blue and green) denote negative correlations.\nAnnotation: Annotation within the heatmap provides specific correlation values, enabling precise quantitative assessment.\n\nHeatmaps are instrumental in identifying significant relationships in datasets, making them invaluable in fields like finance, biology, and social sciences.\n\n\n3.2: Clustermap (Hierarchical Clustering)\nClustermaps are a specialized form of heatmap that combines data visualization with hierarchical clustering. They are exceptionally useful for grouping and ordering data based on similarity, revealing underlying structures in the dataset. Dendrograms are often employed to illustrate the clustering hierarchy.\nCreating a Clustermap:\nWe will utilize the same synthetic correlation matrix to create a clustermap, which employs hierarchical clustering to group and order data.\n\n# Creating a clustermap without specifying cbar_pos\nsns.clustermap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Clustermap of Correlation Matrix')\nplt.show()\n\n\n\n\nThe clustermap visually presents the clustered relationships among variables. It employs dendrograms to showcase the hierarchical structure within the data. By using dendrograms, the clustermap provides insights into how data points are grouped based on their similarity.\nInterpreting Clustermaps:\nInterpreting a clustermap involves focusing on the following components:\n\nDendrograms: Dendrograms in the row and column margins show the hierarchical structure of clustered data points. The closer data points are on the dendrogram, the more similar they are.\nOrdering: The order of rows and columns reflects the clustering hierarchy, allowing us to identify groups of variables with similar relationships.\n\nClustermaps are a valuable tool for identifying and visualizing patterns within datasets, making them indispensable in fields such as genomics and social network analysis. They help unveil the underlying structure of complex data, enabling informed decision-making and insightful data exploration."
  },
  {
    "objectID": "pages/vis2.html#distribution-plots",
    "href": "pages/vis2.html#distribution-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "4: Distribution Plots",
    "text": "4: Distribution Plots\nIn this section, we will delve into the realm of distribution plots, a set of visualization techniques designed to provide insights into the distribution of data. These plots are invaluable for understanding the underlying structure of datasets, exploring the shape of distributions, and detecting important statistical properties. We will explore two distribution plots: Kernel Density Estimate (KDE) Plot and Pair Plot.\n\n4.1: Kernel Density Estimate (KDE) Plot (Visualizing Probability Density)\nKernel Density Estimate (KDE) plots offer an effective means of visualizing the probability density function of a single variable. They provide a smooth representation of data distribution, allowing us to explore underlying patterns and characteristics. To illustrate the utility of KDE plots, we will use a synthetic dataset of exam scores.\nCreating a Kernel Density Estimate (KDE) Plot:\nLet’s begin by generating synthetic exam score data and then create a KDE plot using Seaborn.\n\n# Importing necessary libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic exam score data\nexam_scores = np.random.normal(75, 10, 200)\n\n# Creating a KDE plot\nplt.figure(figsize=(10, 6))\nsns.kdeplot(exam_scores, fill=True, color='orange')\nplt.xlabel('Exam Scores')\nplt.ylabel('Probability Density')\nplt.title('Kernel Density Estimate (KDE) of Exam Scores')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe resulting KDE plot provides a smooth representation of the exam scores’ probability density, highlighting potential peaks and trends in the data. The shade area under the curve represents the estimated probability.\nInterpreting KDE Plots:\nInterpreting a KDE plot involves recognizing key elements:\n\nKernel Smoothness: The smoothness of the curve is determined by the choice of the kernel function. Smoother curves indicate a more generalized representation of the data.\nPeaks: Peaks in the KDE plot represent modes or significant clusters within the data. These peaks indicate areas where data points are more concentrated.\nTails: The tails of the KDE plot extend towards the data’s minimum and maximum values, providing insights into the data’s spread.\n\nKDE plots are crucial for understanding the underlying data distribution, especially when dealing with single-variable datasets.\n\n\n4.2: Pair Plot (Exploring Multivariate Relationships)\nPair plots are a powerful tool for exploring the relationships between multiple numeric variables within a dataset. They provide a comprehensive overview of variable interactions, including scatterplots, histograms, and correlation coefficients. To demonstrate the utility of pair plots, we will use a synthetic dataset with multiple features.\nCreating a Pair Plot:\nLet’s generate synthetic data with multiple numeric features and use Seaborn to create a pair plot.\n\n# Generating synthetic dataset with multiple features\nimport pandas as pd\n\ndata = pd.DataFrame({\n  'Feature1': np.random.normal(0, 1, 100),\n  'Feature2': np.random.normal(0, 1, 100),\n  'Feature3': np.random.normal(0, 1, 100),\n  'Feature4': np.random.normal(0, 1, 100)\n})\n\n# Creating a pair plot\nsns.pairplot(data)\nplt.suptitle('Pair Plot of Multiple Features')\nplt.show()\n\n\n\n\nThe resulting pair plot offers a matrix of scatterplots for pairwise variable comparisons, histograms along the diagonal, and correlation coefficients.\nInterpreting Pair Plots:\nInterpreting a pair plot involves examining various components:\n\nScatterplots: The scatterplots in the upper and lower triangles of the matrix illustrate the relationships between pairs of variables. They help identify trends and correlations.\nHistograms: The diagonal of the pair plot consists of histograms for each variable, revealing the distribution of each feature individually.\nCorrelation Coefficients: If desired, correlation coefficients can be displayed within the scatterplots, indicating the strength and direction of linear relationships.\n\nPair plots are instrumental in identifying relationships between variables, detecting outliers, and gaining insights into dataset characteristics."
  },
  {
    "objectID": "pages/vis2.html#time-series-plots",
    "href": "pages/vis2.html#time-series-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "5: Time Series Plots",
    "text": "5: Time Series Plots\nTime series data is a fundamental component of various fields, including finance, economics, and environmental sciences. Visualizing time-dependent trends is crucial for understanding patterns, making predictions, and conducting in-depth analyses. In this section, we will explore a range of time series visualization techniques that empower us to decode and interpret the dynamics of temporal data.\n\n5.1: Time Series Plot (Unveiling Temporal Trends)\nTime series plots are a go-to choice for unveiling temporal trends in data. By tracking changes over time, we can uncover patterns, fluctuations, and anomalies. For this demonstration, we will employ a synthetic time series dataset representing stock prices over time.\nCreating a Time Series Plot:\nLet’s initiate our exploration by generating a synthetic time series dataset and crafting an informative time series plot using Matplotlib.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic time series data\ntime = np.arange(0, 10, 0.1)\nstock_prices = np.sin(time) + np.random.normal(0, 0.2, len(time))\n\n# Creating a time series plot\nplt.figure(figsize=(10, 6))\nplt.plot(time, stock_prices, label='Stock Prices', color='b', linestyle='-', marker='o')\nplt.xlabel('Time')\nplt.ylabel('Stock Prices')\nplt.title('Stock Price Trends Over Time')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe resulting time series plot beautifully illustrates stock price trends over time. This visualization is instrumental for detecting long-term trends, seasonal patterns, and short-term fluctuations in time series data.\nInterpreting Time Series Plots:\nInterpreting time series plots involves analyzing various aspects:\n\nTrends: Examining the overall direction of the time series to identify upward, downward, or stationary trends.\nSeasonality: Detecting recurring patterns or cycles within the data, which may occur daily, weekly, monthly, or seasonally.\nVolatility: Observing the degree of variability in the data, which is crucial for risk assessment and financial analysis.\nAnomalies: Identifying unusual data points that deviate significantly from the expected patterns.\n\nTime series plots are foundational for analyzing historical data and can guide decision-making in areas such as investment and resource allocation.\n\n\n5.2: AutoCorrelation Plot (Unmasking Time-Dependent Dependencies)\nAutoCorrelation plots are essential tools for unveiling time-dependent dependencies in time series data. They help us understand the relationship between a time series and its past observations. In this demonstration, we will utilize a synthetic time series dataset representing monthly sales data.\nCreating an AutoCorrelation Plot:\nTo illustrate the concept of auto-correlation, we will generate synthetic monthly sales data and craft an informative auto-correlation plot using Matplotlib.\n\n# Generating synthetic monthly sales data\nmonths = np.arange(1, 13)\nmonthly_sales = np.sin(months) + np.random.normal(0, 0.2, 12)\n\n# Creating an auto-correlation plot\nplt.figure(figsize=(10, 6))\npd.plotting.autocorrelation_plot(monthly_sales)\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Plot of Monthly Sales')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe resulting auto-correlation plot unveils insights into the temporal dependencies within the monthly sales data. It is instrumental for identifying seasonal patterns, lags, and potential predictive features.\nInterpreting AutoCorrelation Plots:\nInterpreting auto-correlation plots involves examining several key components:\n\nLags: On the x-axis, the lag represents the number of time periods between observations. It helps identify time-dependent relationships.\nAutocorrelation Values: The y-axis displays autocorrelation values, which indicate the strength and direction of the relationship. Peaks and valleys in this plot reveal time-dependent patterns.\nSeasonality: Peaks at regular intervals in the auto-correlation plot suggest the presence of seasonal patterns. The width of these peaks may reveal the season’s duration.\n\nAuto-correlation plots are indispensable for understanding the time-dependent dynamics of data, identifying seasonality, and guiding the selection of appropriate forecasting models."
  },
  {
    "objectID": "pages/vis2.html#geospatial-data-visualization",
    "href": "pages/vis2.html#geospatial-data-visualization",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "6: Geospatial Data Visualization",
    "text": "6: Geospatial Data Visualization\nIn this section, we will embark on an in-depth exploration of geospatial data visualization, a crucial domain for understanding and interpreting data in geographic contexts. Geospatial data visualization techniques enable us to represent data with latitude and longitude coordinates, visualize patterns in geographical data, and gain insights into the distribution and relationships of spatial data points. We will cover Scatter Geo Plots and Choropleth Maps, each offering a unique perspective on geospatial data representation.\n\n6.1: Scatter Geo Plot with Actual Earthquake Data\nIn this section, we will retrieve real-time earthquake data from the US Geological Survey (USGS) API and visualize the locations using a scatter geo plot on a world map.\nTo begin, we’ll utilize the requests library to fetch earthquake data from the USGS API. Ensure you have requests and other necessary libraries installed in your Python environment.\n\nimport requests\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Define the USGS API URL for earthquake data\nurl = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.geojson'\n\n# Fetch earthquake data from the USGS API\nresponse = requests.get(url)\nif response.status_code == 200:\n    earthquake_data = response.json()\n    # Extract necessary data for plotting\n    coordinates = [(feature['geometry']['coordinates'][0], feature['geometry']['coordinates'][1]) \n                   for feature in earthquake_data['features']]\n    # Create a GeoDataFrame from earthquake data\n    earthquake_df = gpd.GeoDataFrame(geometry=gpd.points_from_xy([coord[0] for coord in coordinates],\n                                                                [coord[1] for coord in coordinates]))\n    # Load world map data from GeoPandas datasets\n    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n    # Create a base world map plot\n    fig, ax = plt.subplots(figsize=(12, 8))\n    world.plot(ax=ax, color='lightgrey', edgecolor='black')\n\n    # Plot the earthquake locations on the map\n    earthquake_df.plot(ax=ax, markersize=5, color='red', alpha=0.7, marker='o', label='Earthquake Locations')\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    ax.set_title('Recent Earthquake Locations Worldwide')\n    plt.legend()\n    plt.show()\nelse:\n    print(\"Failed to fetch earthquake data from the USGS API\")\n\n\n\n\nThis code retrieves recent earthquake data from the USGS API and creates a scatter geo plot displaying the geographical distribution of earthquakes on a world map. The red points represent earthquake locations, allowing for a visual understanding of their spatial distribution and density.\nInterpreting Scatter Geo Plots:\nInterpreting a scatter geo plot involves:\n\nSpatial Distribution: Observing the distribution of data points across the geographical area. Clusters or patterns may indicate regions with a higher concentration of events.\nOutliers: Identifying isolated data points that deviate significantly from the main cluster, which may denote unique or extreme events.\nGeographic Relationships: Understanding the relationships between data points based on their geographic proximity.\n\nScatter Geo Plots are vital for understanding and analyzing geospatial data, making them invaluable for applications such as seismology, epidemiology, and environmental studies.\n\n\n6.2: Choropleth Map (Color-Coded Data Distribution)\nChoropleth Maps are a powerful visualization tool for representing geographic data in regions or administrative boundaries. They use color gradients to depict variations in data values across different geographic areas. Choropleth Maps are instrumental for understanding regional disparities, population distributions, and data patterns at a macroscopic level.\nCreating a Choropleth Map:\nTo illustrate the application of Choropleth Maps, we will utilize synthetic population data by country and generate a color-coded map to visualize population distribution.\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport requests\nimport pycountry\n\n# Loading the 'naturalearth_lowres' dataset\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n# Function to fetch population data using the World Bank API\ndef get_population(country_code):\n    try:\n        url = f'http://api.worldbank.org/v2/country/{country_code}/indicator/SP.POP.TOTL?format=json'\n        response = requests.get(url)\n        data = response.json()[1]\n        for item in data:\n            if item['value'] is not None:\n                return int(item['value'])\n        return None\n    except Exception as e:\n        print(f\"Error fetching population data: {e}\")\n        return None\n\n# Fetching population data for all countries\nfor index, row in world.iterrows():\n    country_name = row['name']\n    try:\n        country_code = pycountry.countries.lookup(country_name).alpha_3\n        population = get_population(country_code)\n        if population is not None:\n            world.loc[index, 'Population'] = population\n    except LookupError:\n        continue\n    \n# Creating a choropleth map\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nworld.boundary.plot(ax=ax, linewidth=1)\nworld.plot(column='Population', cmap='YlOrRd', ax=ax, legend=True)\nplt.title('Population by Country')\nplt.show()\n\nError fetching population data: 'NoneType' object is not iterable\nError fetching population data: list index out of range\n\n\n\n\n\nIn this example, we load a world map with country boundaries and overlay it with color-coded regions based on population. The color intensity reflects population density, allowing us to visualize variations in population across different countries.\nInterpreting Choropleth Maps:\nInterpreting a choropleth map involves:\n\nColor Gradients: Understanding the color spectrum used to represent data values. Darker colors typically denote higher values, while lighter colors indicate lower values.\nRegional Patterns: Observing variations in data distribution across different regions. Darker regions indicate higher population or data values.\nGeographic Trends: Identifying regional trends, disparities, or clusters within the dataset.\n\nChoropleth Maps are indispensable for visualizing data associated with geographic regions, and they find extensive use in fields such as demographics, economics, and public health."
  },
  {
    "objectID": "pages/vis2.html#d-plots-visualizing-three-dimensional-data",
    "href": "pages/vis2.html#d-plots-visualizing-three-dimensional-data",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "7: 3D Plots (Visualizing Three-Dimensional Data)",
    "text": "7: 3D Plots (Visualizing Three-Dimensional Data)\nIn this section, we will embark on an exploration of three-dimensional (3D) data visualization techniques. Visualizing data in three dimensions allows us to understand complex relationships and patterns that cannot be effectively represented in two dimensions. We will cover two fundamental 3D plot types: 3D Scatter Plots and 3D Line Plots.\n\n7.1: 3D Scatter Plot (Visualizing Data Clusters in 3D Space)\n3D scatter plots are a valuable tool for visualizing data with three numeric variables. They enable us to explore data points in a three-dimensional space, making it easier to identify clusters, patterns, and relationships among variables.\nCreating a 3D Scatter Plot:\nTo illustrate the concept, we will generate synthetic 3D data and create an insightful 3D scatter plot using Matplotlib. The generated data includes three numeric variables: X, Y, and Z coordinates.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic 3D data\nx = np.random.normal(0, 1, 100)\ny = np.random.normal(0, 1, 100)\nz = np.random.normal(0, 1, 100)\n\n# Creating a 3D scatter plot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c='b', marker='o')\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Z-axis')\nax.set_title('3D Scatter Plot')\nplt.show()\n\n\n\n\nThe 3D scatter plot portrays the data in a three-dimensional space, offering an intuitive perspective on the distribution of data points. The use of color, markers, and labels enhances the visualization.\nInterpreting 3D Scatter Plots:\nInterpreting a 3D scatter plot involves several key considerations:\n\nData Clusters: Examine the distribution of data points in 3D space to identify clusters or patterns. Data points that are close to each other may represent a cohesive group or relationship.\nOutliers: Look for data points that deviate significantly from the main cluster, as these may indicate outliers or special cases.\nVariable Relationships: Understand how the three numeric variables (X, Y, and Z) interact in the 3D space. Observing their positions can reveal relationships and correlations.\n\n3D scatter plots are valuable for a wide range of applications, including data clustering, spatial analysis, and the visualization of multidimensional data.\n\n\n7.2: 3D Line Plot (Visualizing Data Trajectories in 3D Space)\n3D line plots are instrumental in visualizing data trajectories or data with time-dependent coordinates in three-dimensional space. These plots help us understand how data points evolve in a 3D environment.\nCreating a 3D Line Plot:\nTo illustrate the concept, we will generate synthetic 3D trajectory data and create a 3D line plot using Matplotlib. The data includes time, X, Y, and Z coordinates, which can represent a variety of phenomena, such as particle motion, aircraft paths, or spatial trajectories.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic trajectory data\nt = np.linspace(0, 10, 100)\nx = np.sin(t)\ny = np.cos(t)\nz = t\n\n# Creating a 3D line plot\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot(x, y, z, c='r')\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Z-axis')\nax.set_title('3D Line Plot')\nplt.show()\n\n\n\n\nThe 3D line plot portrays the trajectory or path of data points in a three-dimensional space. This visualization provides insights into the evolution and spatial characteristics of the data.\nInterpreting 3D Line Plots:\nInterpreting a 3D line plot involves several key considerations:\n\nTrajectories: Observe the path followed by the data points over time or in 3D space. Identify any loops, patterns, or trends within the trajectories.\nSpatial Relationships: Analyze how the data points are distributed in the 3D space. Investigate whether certain regions are densely populated or sparsely populated.\nCustomization: Explore customization options for line style and color to enhance the clarity and visual appeal of the plot.\n\n3D line plots are invaluable for studying phenomena with three-dimensional characteristics, and they offer a unique perspective on the data’s behavior in space and time."
  },
  {
    "objectID": "pages/vis2.html#specialized-plots",
    "href": "pages/vis2.html#specialized-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "8: Specialized Plots",
    "text": "8: Specialized Plots\nIn this section, we will explore a range of specialized data visualization techniques that cater to specific data types and analysis needs. Specialized plots offer unique insights and enable the visualization of data that may not be adequately represented by standard plot types. We will delve into Polar Plots, Network Plots, and Word Clouds, each serving distinct purposes in data analysis.\n\n8.1: Polar Plot (Visualizing Circular Data)\nPolar plots are a specialized form of data visualization ideal for representing data with angular coordinates, such as wind direction, compass bearings, or circular data. These plots are invaluable for revealing patterns and trends in cyclical datasets.\nCreating a Polar Plot:\nTo illustrate the creation of a polar plot, we will use a synthetic dataset representing wind direction and wind speeds. This plot will provide insights into wind speed distribution in different directions.\n\n# Generating synthetic wind direction data\nangles = np.linspace(0, 2 * np.pi, 8)\nwind_speeds = np.random.uniform(0, 10, 8)\n\n# Creating a polar plot\nplt.figure(figsize=(8, 8))\nplt.polar(angles, wind_speeds, label='Wind Speeds', linestyle='-', marker='o')\nplt.fill(angles, wind_speeds, alpha=0.3)\nplt.thetagrids(angles * 180 / np.pi, labels=['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\nplt.rgrids(np.arange(0, 10, 2), angle=45)\nplt.legend()\nplt.title('Wind Speeds in Different Directions')\nplt.show()\n\n\n\n\nThe polar plot above represents wind speeds in various directions. The circular nature of the plot is well-suited for visualizing angular data.\nInterpreting Polar Plots:\nInterpreting a polar plot involves understanding the following elements:\n\nAngular Coordinates: The angles on the plot’s perimeter represent the directional data, with labels denoting the corresponding directions.\nRadial Axes: The radial axes extending from the center indicate values, in this case, wind speeds.\nData Representation: Each data point is plotted at its angular position, and the radial distance from the center corresponds to the value being represented.\n\nPolar plots are excellent for visualizing cyclical patterns and identifying trends in circular data, making them valuable in fields such as meteorology and environmental science.\n\n\n8.2: Network Plot (Visualizing Complex Relationships)\nNetwork plots, also known as graph visualizations, are designed to represent complex relationships and connections between entities. They are particularly useful for visualizing social networks, communication structures, and various interconnected data.\nCreating a Network Plot:\nTo showcase the creation of a network plot, we will use a synthetic dataset representing social network relationships. This plot will reveal the connections between individuals within the network.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n# Constructing a matrix to represent connections between individuals\nconnections = np.array([\n    [0, 1, 1, 0, 0, 0],  # Alice\n    [1, 0, 1, 1, 0, 0],  # Bob\n    [1, 1, 0, 1, 0, 0],  # Charlie\n    [0, 1, 1, 0, 1, 1],  # David\n    [0, 0, 0, 1, 0, 1],  # Emma\n    [0, 0, 0, 1, 1, 0]   # Frank\n])\n\n# Names of individuals in the network\nnames = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Emma\", \"Frank\"]\n\n# Creating a graph from the matrix\nG = nx.Graph()\nG.add_nodes_from(names)\n\n# Adding edges based on the connections matrix\nfor i in range(connections.shape[0]):\n    for j in range(i + 1, connections.shape[1]):\n        if connections[i, j] == 1:\n            G.add_edge(names[i], names[j])\n\n# Creating a network plot\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', font_weight='bold', edge_color='gray', width=1.5)\nplt.title('Complex Network Relationships')\nplt.show()\n\n\n\n\nThe resulting network plot visually represents the relationships between individuals within the social network.\nInterpreting Network Plots:\nInterpreting a network plot involves considering the following aspects:\n\nNodes: Nodes represent individual entities, such as people or objects within the network.\nEdges: Edges, often depicted as lines connecting nodes, signify relationships or connections between entities.\nLayout: The arrangement of nodes and edges within the plot reflects the structure of the network. Different layout algorithms can reveal various network properties.\nClustering: Patterns of clustering and connectivity can provide insights into the network’s structure.\n\nNetwork plots are essential for understanding complex relationships and can be applied in diverse fields, including social sciences, biology, and information technology.\n\n\n8.3: Word Cloud (Visualizing Text Data)\nWord clouds are a specialized form of data visualization used to represent text data, specifically word frequency within a corpus or document. They provide an intuitive way to grasp the most common words and their relative importance.\nCreating a Word Cloud:\nTo demonstrate the creation of a word cloud, we will use a synthetic text data sample. This word cloud will visualize word frequency in the provided text.\n\n# Generating synthetic text data\nfrom wordcloud import WordCloud\n\ntext_data = \"This is a sample text data for creating a word cloud. Word clouds are a fun way to visualize word frequency.\"\n\n# Creating a word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Word Cloud of Text Data')\nplt.show()\n\n\n\n\nThe resulting word cloud visually emphasizes words by size, with more frequent words appearing larger.\nInterpreting Word Clouds:\nInterpreting a word cloud involves considering the following aspects:\n\nWord Size: The size of each word in the cloud corresponds to its frequency within the text. Larger words are more frequently used.\nColor: Word clouds can employ color to further emphasize certain words or categories.\nContext: Understanding the context of the word cloud is crucial to extract meaningful insights.\n\nWord clouds are an engaging way to uncover prominent terms within text data, making them valuable in text analysis, content marketing, and sentiment analysis."
  },
  {
    "objectID": "pages/vis2.html#advanced-data-visualization",
    "href": "pages/vis2.html#advanced-data-visualization",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "9: Advanced Data Visualization",
    "text": "9: Advanced Data Visualization\nIn this section, we will explore specialized data visualization techniques that cater to distinct data analysis needs. These visualizations offer unique insights into specific aspects of data analysis, such as model evaluation and dimensionality reduction.\n\n9.1: ROC Curves and AUC (Model Evaluation)\nROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve) are powerful tools for evaluating the performance of binary classification models. They provide a visual representation of a model’s ability to discriminate between positive and negative classes over various thresholds.\nCreating ROC Curves and Calculating AUC:\nTo illustrate the use of ROC curves and AUC, we will follow these steps:\n\nGenerate a synthetic dataset for binary classification.\nSplit the dataset into training and testing sets.\nTrain a logistic regression model.\nCalculate the ROC curve and AUC.\n\n\n# Generating synthetic binary classification data and training a model\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Generating synthetic binary classification data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Creating a ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Plotting the ROC curve\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\nThe ROC curve illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate as the classification threshold varies. AUC quantifies the overall model performance, with higher AUC values indicating better classification ability.\nInterpreting ROC Curves and AUC:\n\nTrue Positive Rate (TPR): The TPR represents the proportion of true positive predictions concerning all actual positive instances. It reflects the model’s ability to correctly classify positive cases.\nFalse Positive Rate (FPR): The FPR represents the proportion of false positive predictions concerning all actual negative instances. A low FPR is desired, as it indicates minimal misclassification of negative cases.\nROC Curve Shape: The shape of the ROC curve and its proximity to the top-left corner indicate the model’s performance. A curve that approaches the top-left corner indicates a superior model.\nArea Under the Curve (AUC): AUC summarizes the ROC curve’s performance in a single value. It ranges from 0.5 (random classification) to 1.0 (perfect classification). An AUC value greater than 0.5 suggests that the model outperforms random chance.\n\nROC curves and AUC are invaluable for assessing the quality of binary classification models and selecting the optimal threshold for specific application requirements.\n\n\n9.2: t-SNE Plots (Dimensionality Reduction)\nt-SNE (t-distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique that is particularly useful for visualizing high-dimensional data in lower dimensions while preserving the structure of data clusters. It is an excellent tool for exploring patterns and relationships within complex datasets.\nCreating t-SNE Scatter Plots:\nTo demonstrate the use of t-SNE, we will perform the following steps:\n\nGenerate synthetic high-dimensional data.\nApply t-SNE to reduce data to two dimensions.\nCreate a scatter plot of the reduced data.\n\n\n# Generating synthetic high-dimensional data and reducing it using t-SNE\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Generating synthetic high-dimensional data\nX, y = make_classification(n_samples=100, n_features=50, random_state=42)\n\n# Reducing the data to two dimensions using t-SNE\nX_embedded = TSNE(n_components=2, random_state=42).fit_transform(X)\n\n# Creating a t-SNE scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='viridis')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.title('t-SNE Scatter Plot')\nplt.show()\n\n\n\n\nThe resulting t-SNE scatter plot provides a simplified representation of the original high-dimensional data while preserving data patterns and clusters.\nInterpreting t-SNE Plots:\n\nClusters: Data points that are close together in the t-SNE scatter plot belong to the same clusters in the high-dimensional space, revealing natural groupings within the data.\nDimensionality Reduction: t-SNE effectively reduces the data’s dimensionality, making it easier to explore and understand complex datasets.\nOutliers: Outliers or anomalies may appear as data points that are isolated from the main clusters in the scatter plot.\n\nt-SNE is a valuable tool for data exploration, visualization, and gaining insights into high-dimensional data structures. It is particularly useful in fields such as machine learning, biology, and text analysis.\nAlthough extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. By exploring how it behaves in simple cases, we can learn to use it more effectively. Refer to this article for more info: How to Use t-SNE Effectively"
  },
  {
    "objectID": "pages/vis2.html#conclusion",
    "href": "pages/vis2.html#conclusion",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Conclusion",
    "text": "Conclusion\nIn this extensive Jupyter notebook, we have explored various data visualization techniques using Matplotlib and Seaborn. We began with basic plots, including line plots, scatter plots, bar plots, and histograms. Then, we delved into statistical plots like box plots, violin plots, and swarm plots. The matrix plots section covered heatmaps and clustermaps. We also explored distribution plots, time series plots, geospatial data visualization, 3D plots, specialized plots, custom visualizations, interactive visualizations, and specialized plots like ROC curves and t-SNE plots.\nData visualization is an integral part of data analysis, helping us gain insights, make informed decisions, and communicate our findings effectively. Choosing the right visualization technique for a given dataset is crucial, and this notebook provides a comprehensive overview to aid Computer Science and Data Science students in their data visualization journey."
  },
  {
    "objectID": "pages/vis2.html#additional-notes",
    "href": "pages/vis2.html#additional-notes",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nFor interactive visualizations, consider using libraries like Plotly, Bokeh, or Dash.\nTo enhance your data visualization skills, practice with real-world datasets and explore more advanced techniques and libraries.\nAlways strive for clear and informative visualizations that convey the intended message effectively."
  },
  {
    "objectID": "pages/vis2.html#excercise",
    "href": "pages/vis2.html#excercise",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Excercise",
    "text": "Excercise\nPractise your visualization skills on the following Stocks dataset: link"
  },
  {
    "objectID": "pages/vis.html",
    "href": "pages/vis.html",
    "title": "Visualizations for Data Science: An Overview",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/vis.html#table-of-contents",
    "href": "pages/vis.html#table-of-contents",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIntroduction\nWhy Data Visualization Matters\nGetting Started with Matplotlib\n\nBasic Line Plot\nScatter Plot\nBar Chart\nHistogram\nBox Plot\nPie Chart\n\nEnhancing Visualizations with Seaborn\n\nSeaborn vs. Matplotlib\nSeaborn’s Datasets\nSeaborn Styles\nCategorical Plots\nPair Plots\nHeatmaps\nFacetGrid\n\nReal-World Data Visualization Examples\n\nExploratory Data Analysis (EDA)\nTime Series Data Visualization\nGeographic Data Visualization\nAdvanced Plots for Correlation Analysis\n\nInteractive Visualizations\n\nPlotly: A Brief Introduction\nDash: Building Interactive Web Applications\nBokeh\n\nCustomizing and Styling Plots\n\nLabels, Titles, and Legends\nColor Palettes\nPlot Annotations\n\nConclusion"
  },
  {
    "objectID": "pages/vis.html#introduction",
    "href": "pages/vis.html#introduction",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Introduction",
    "text": "Introduction\nData visualization is an indispensable tool in the field of data science. It serves as a powerful means to convey information, explore data, make informed decisions, and communicate results effectively. This notebook aims to provide an academic and comprehensive guide to visualizing data using Matplotlib and Seaborn, two fundamental libraries in data science."
  },
  {
    "objectID": "pages/vis.html#why-data-visualization-matters",
    "href": "pages/vis.html#why-data-visualization-matters",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Why Data Visualization Matters",
    "text": "Why Data Visualization Matters\nHuman beings possess an innate ability to process and understand visual information more efficiently than textual or numerical data. Visualization leverages this cognitive advantage, enabling us to:\n\nGain Insight: Visualizations can reveal patterns, trends, and outliers in data that might be elusive in raw numbers.\nSimplify Complex Data: They simplify complex datasets, making them more comprehensible.\nTell a Story: Visualizations facilitate storytelling, making it easier to convey findings and insights to diverse audiences."
  },
  {
    "objectID": "pages/vis.html#getting-started-with-matplotlib",
    "href": "pages/vis.html#getting-started-with-matplotlib",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Getting Started with Matplotlib",
    "text": "Getting Started with Matplotlib\nIn this section, we will delve into the fundamentals of Matplotlib, one of the most widely used Python libraries for data visualization. Matplotlib provides a versatile framework for creating a wide range of static and animated plots, making it an indispensable tool for data scientists and students in the field of computer science and data science. We will cover several essential plot types with detailed end-to-end examples to help you grasp the concepts and practices effectively.\n\nBasic Line Plot\nA line plot is a fundamental type of visualization that is used to represent data points as a series of connected line segments. It is particularly useful for visualizing trends or patterns in data.\nLet’s create a simple line plot using Matplotlib with end-to-end code and explanations:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 20]\n\n# Create a line plot\nplt.plot(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Simple Line Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we import Matplotlib, define sample data for the x and y coordinates, create a line plot using plt.plot(), add labels and a title for clarity, and finally, display the plot using plt.show().\n\n\nScatter Plot\nScatter plots are effective for visualizing the relationship between two variables, making them ideal for data exploration and analysis. Each data point is represented as a dot on the plot.\nHere’s an end-to-end example of creating a scatter plot:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 20]\n\n# Create a scatter plot\nplt.scatter(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Scatter Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.scatter() to create a scatter plot, with the same labeling and title setup as before.\n\n\nBar Chart\nBar charts are an effective way to compare categories or discrete data points. They visually represent data using rectangular bars, with the length of each bar corresponding to the value of the data it represents.\nHere’s how you can create a bar chart from scratch:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\ncategories = ['A', 'B', 'C', 'D']\nvalues = [30, 50, 20, 40]\n\n# Create a bar chart\nplt.bar(categories, values)\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Bar Chart')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use plt.bar() to create a bar chart, customize it with labels and a title, and finally, display the chart.\n\n\nHistogram\nHistograms are essential for visualizing the distribution of a dataset, particularly in cases where the data’s frequency distribution is of interest.\nHere’s an end-to-end example of creating a histogram:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data\ndata = np.random.normal(0, 1, 1000)\n\n# Create a histogram\nplt.hist(data, bins=30)\n\n# Adding labels and title\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.hist() to create a histogram, where we first generate random data using NumPy, specify the number of bins for the histogram, and add labels and a title.\n\n\nBox Plot\nBox plots are excellent for visualizing the distribution and spread of data, helping to identify outliers and assess the central tendency and variability of a dataset.\nHere’s an example of creating a box plot:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data\ndata = np.random.normal(0, 1, 100)\n\n# Create a box plot\nplt.boxplot(data)\n\n# Adding labels and title\nplt.ylabel('Value')\nplt.title('Box Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use plt.boxplot() to create a box plot, generate random data for illustration, and include labels and a title.\n\n\nPie Chart\nPie charts are effective for displaying the proportion of different categories within a dataset. They are particularly useful for illustrating data in a way that emphasizes the relationship between parts and the whole.\nHere’s how to create a pie chart with Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nlabels = 'Category A', 'Category B', 'Category C', 'Category D'\nsizes = [15, 30, 45, 10]\n\n# Create a pie chart\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\n\n# Adding title\nplt.title('Pie Chart')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.pie() to create a pie chart, specify the labels and sizes of the categories, and include a title for clarity.\nThese examples demonstrate how to create various types of plots using Matplotlib, from basic line plots to more complex charts like histograms and pie charts. Matplotlib provides extensive customization options to tailor your visualizations to your specific needs."
  },
  {
    "objectID": "pages/vis.html#enhancing-visualizations-with-seaborn",
    "href": "pages/vis.html#enhancing-visualizations-with-seaborn",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Enhancing Visualizations with Seaborn",
    "text": "Enhancing Visualizations with Seaborn\nSeaborn is a powerful Python library that builds on Matplotlib and offers a high-level interface for creating informative and aesthetically pleasing statistical visualizations. In this section, we will delve into Seaborn’s capabilities and explore various aspects of enhancing data visualizations using this library. We will provide comprehensive end-to-end examples for each subtopic to illustrate how Seaborn can be employed effectively.\n\nSeaborn vs. Matplotlib\nUnderstanding the Distinction\nBefore diving into Seaborn, it’s essential to understand the key differences between Seaborn and Matplotlib. While Matplotlib is a versatile but somewhat low-level library for creating plots, Seaborn is designed for statistical data visualization and offers:\n\nSimplified syntax and concise API.\nBuilt-in themes and color palettes for better aesthetics.\nSpecialized functions for creating complex plots with minimal code.\n\nExample: Comparing Matplotlib and Seaborn\nLet’s illustrate the difference with a basic example. We’ll create a * simple histogram using both Matplotlib and Seaborn.\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(0, 1, 1000)\nplt.hist(data, bins=30)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram (Matplotlib)')\nplt.show()\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport numpy as np\n\ndata = np.random.normal(0, 1, 1000)\nsns.histplot(data, bins=30, kde=True)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram (Seaborn)')\nplt.show()\n\n\n\n\nIn this example, Seaborn simplifies the creation of a histogram with an optional kernel density estimation (KDE) curve, enhancing both readability and aesthetics.\n\n\nSeaborn’s Datasets\nSeaborn comes with several built-in datasets that are useful for practice and experimentation. These datasets cover a wide range of scenarios and are readily available for analysis and visualization.\nLet’s load the famous iris dataset provided by Seaborn and create a pair plot to explore the relationships between the features.\n\nimport seaborn as sns\n\n# Load the iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Create a pair plot for exploring feature relationships\nsns.pairplot(iris, hue=\"species\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we load the “iris” dataset and use Seaborn to create a pair plot that visualizes the relationships between the various species of iris flowers. The hue parameter allows us to distinguish different species with color.\n\n\nSeaborn Styles\nSeaborn provides various built-in styles to improve the aesthetics of your plots. You can easily set the style using the sns.set_style() function.\nLet’s change the plotting style using Seaborn’s built-in styles and visualize the same data with different styles.\n\nimport seaborn as sns\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a violin plot with different styles\nsns.set_style(\"darkgrid\")\nsns.violinplot(x=\"day\", y=\"total_bill\", data=tips, hue=\"day\", palette=\"Set1\", inner=\"stick\", split=True, legend=False)\nplt.title('Violin Plot with \"darkgrid\" Style')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n# Change the style to \"whitegrid\"\nsns.set_style(\"whitegrid\")\nsns.violinplot(x=\"day\", y=\"total_bill\", data=tips, hue=\"day\", palette=\"Set1\", inner=\"stick\", split=True, legend=False)\nplt.title('Violin Plot with \"whitegrid\" Style')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we change the plotting style from “darkgrid” to “whitegrid” and visualize the same data using a violin plot. Seaborn’s styles offer visual diversity to suit your preferences and the context of your data.\n\n\nCategorical Plots\nSeaborn offers a range of categorical plots for data exploration. These plots are particularly useful when dealing with categorical or discrete data. We’ll demonstrate the creation of a bar plot to visualize the average tips given by day.\nLet’s use Seaborn to create a bar plot that shows the average tips given on different days of the week.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a bar plot to visualize the average tips by day\nsns.barplot(x=\"day\", y=\"tip\", data=tips)\n\n# Adding labels and title\nplt.xlabel('Day')\nplt.ylabel('Average Tip')\nplt.title('Average Tip by Day')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use Seaborn’s barplot function to create a bar plot that displays the average tips given on different days. The ci parameter is set to None to remove confidence intervals.\n\n\nPair Plots\nPair plots are an excellent tool for visualizing relationships between variables in a dataset. They provide a quick overview of how features are related to each other.\nLet’s create a pair plot to visualize the relationships between different numerical features in the iris datas\n\nimport seaborn as sns\n\n# Load the iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Create a pair plot to explore feature relationships\nsns.pairplot(iris, hue=\"species\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use Seaborn’s pairplot function to create a pair plot that illustrates how different species of iris flowers are related based on features like sepal length, sepal width, petal length, and petal width. The hue parameter allows us to distinguish different species with color.\n\n\nHeatmaps\nHeatmaps are ideal for displaying relationships between data points. They are particularly useful for visualizing correlations between variables.\nLet’s create a heatmap to visualize the correlation matrix of the “tips” dataset, which shows how different numerical features are correlated.\n\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Encode categorical variables\ntips_encoded = pd.get_dummies(tips, columns=[\"sex\", \"smoker\", \"day\", \"time\"])\n\n# Create a correlation matrix\ncorrelation_matrix = tips_encoded.corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True)\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we calculate the correlation matrix of the “tips” dataset and use Seaborn to create a heatmap that visualizes the correlations between features. The annot parameter is set to True to display the correlation values on the heatmap.\n\n\nFacetGrid\nFacetGrid in Seaborn allows you to create a grid of subplots based on the values of one or more variables. It is useful for visualizing relationships in subgroups of data.\nLet’s create a FacetGrid to visualize the relationship between the total bill and tip, differentiating by the time of day and whether the customer is a smoker.\n\nimport seaborn as sns\n\n# Create a FacetGrid\ng = sns.FacetGrid(tips, col=\"time\", row=\"smoker\")\n\n# Map a scatter plot to the grid\ng.map(sns.scatterplot, \"total_bill\", \"tip\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use a FacetGrid to create a grid of scatter plots. The FacetGrid is segmented by the time of day (lunch or dinner) and whether the customer is a smoker or not. This allows us to explore the relationship between the total bill and tip for different subsets of the data.\nSeaborn’s capabilities extend far beyond what is covered in this section. It is a versatile library that empowers data scientists to create visually appealing and informative visualizations with ease. Experiment with Seaborn to discover its full potential and enhance your data analysis and presentation."
  },
  {
    "objectID": "pages/vis.html#real-world-data-visualization-examples",
    "href": "pages/vis.html#real-world-data-visualization-examples",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Real-World Data Visualization Examples",
    "text": "Real-World Data Visualization Examples\n\nExploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) is the initial phase of data analysis where we aim to understand the dataset’s structure, detect anomalies, and identify initial trends. Visualization is a key component of EDA. Let’s consider a real-world dataset, the “Iris” dataset, and perform EDA using Seaborn.\n\nLoad the Dataset:\nWe start by loading the Iris dataset, a classic dataset in data science, which contains measurements of three different species of iris flowers: setosa, versicolor, and virginica.\n\n\nimport seaborn as sns\n\n# Load the Iris dataset\niris = sns.load_dataset(\"iris\")\n\n\nUnivariate Analysis:\nWe can begin by visualizing the distribution of a single variable. For instance, let’s create a histogram to understand the distribution of petal lengths for all three species:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram\nsns.histplot(data=iris, x=\"petal_length\", hue=\"species\")\nplt.title(\"Petal Length Distribution by Species\")\nplt.show()\n\n\n\n\nThis histogram provides insights into the petal length distribution for each species.\n\nBivariate Analysis:\nBivariate analysis helps in understanding relationships between two variables. We can create a pair plot to visualize pairwise relationships between numeric variables:\n\n\nimport seaborn as sns\n\n# Create a pair plot\nsns.pairplot(iris, hue=\"species\")\n\n\n\n\nThe pair plot shows scatterplots for all possible pairs of numeric features and provides a quick overview of how variables relate to each other.\n\nMultivariate Analysis:\nFor a more comprehensive view, we can use a heatmap to visualize the correlation between numeric features:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Filter the DataFrame to include only numeric columns\nnumeric_columns = iris.select_dtypes(include=['float64'])\n\n# Create a correlation matrix\ncorr_matrix = numeric_columns.corr()\n\n# Create a heatmap\nsns.heatmap(corr_matrix, annot=True)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\nThe heatmap visually represents the correlation between different features. This can be especially helpful when dealing with high-dimensional datasets.\n\n\nTime Series Data Visualization\nTime series data often involves data points recorded at regular intervals, such as stock prices over time. Let’s visualize stock price data for a hypothetical company using Seaborn.\n\nLoad the Time Series Data:\nWe’ll create a dataset with timestamps and stock prices for a fictional company:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndata = {\n    'date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n    'price': 100 + 2 * np.random.randn(100)\n}\n\nstock_data = pd.DataFrame(data)\n\n\nVisualize Stock Prices Over Time:\nNext, we can create a line plot to visualize how the stock price of the company changes over time:\n\n\n# Create a line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=\"date\", y=\"price\", data=stock_data)\nplt.title(\"Stock Price Over Time\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThis line plot provides a visual representation of how the stock price fluctuates over the specified time period.\n\n\nGeographic Data Visualization\nVisualizing geographic data is crucial when working with location-based information. Let’s consider a simple example of visualizing cities on a map.\n\nPrepare Geographic Data:\nSuppose you have a dataset with information about cities, including their names, latitudes, and longitudes.\n\n\nimport pandas as pd\n\ndata = {\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n    'Latitude': [40.7128, 34.0522, 41.8781, 29.7604],\n    'Longitude': [-74.0060, -118.2437, -87.6298, -95.3698]\n}\n\ncities_data = pd.DataFrame(data)\n\n\nVisualize Cities on a Map:\nTo visualize these cities on a map, you can use libraries like Folium or geospatial data visualization tools. Here’s a simplified example using Folium:\n\n\nimport folium\n\n# Create a map object centered on the United States\nm = folium.Map(location=[37.0902, -95.7129], zoom_start=4)\n\n# Add markers for each city\nfor index, row in cities_data.iterrows():\n    folium.Marker([row['Latitude'], row['Longitude']], popup=row['City']).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThis code generates a map with markers representing the cities’ locations.\n\n\nAdvanced Plots for Correlation Analysis\nFor advanced correlation analysis, Seaborn provides various plots. Let’s consider a scenario where we want to explore the correlation between features in a real-world dataset.\n\nLoad the Dataset:\nWe can load a dataset that contains numeric variables to analyze their correlation. For example, we can use Seaborn’s built-in “diamonds” dataset:\n\n\nimport seaborn as sns\n\n# Load the Diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n\nCreate Advanced Correlation Plots:\nSeaborn offers advanced plots for correlation analysis. For instance, we can create a pair plot with regression lines to understand relationships between numeric features, considering factors like carat, cut, and price:\n\n\nimport seaborn as sns\n\n# Create a pair plot with regression lines\nsns.pairplot(diamonds, vars=['carat', 'price'], hue='cut', kind='reg')\n\n\n\n\nThis pair plot provides insights into how carat, price, and cut are correlated.\nThese examples demonstrate how to apply data visualization techniques to real-world scenarios, such as exploratory data analysis, time series data, geographic data, and advanced correlation analysis. Effective visualization is essential for gaining insights and making data-driven decisions in data science and analysis."
  },
  {
    "objectID": "pages/vis.html#interactive-visualizations",
    "href": "pages/vis.html#interactive-visualizations",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\nIn the world of data science, static visualizations are undeniably powerful for understanding and communicating insights. However, there are times when you need to take your data visualization to the next level by making it interactive. Interactive visualizations allow users to explore data on their terms, providing a dynamic and engaging experience. In this section, we will explore two prominent libraries for creating interactive visualizations: Plotly and Dash, and Bokeh.\n\nPlotly: A Brief Introduction\nPlotly is a versatile library for creating interactive, web-based visualizations. It supports a wide range of chart types and is known for its user-friendly API. Here’s an end-to-end example of creating an interactive line plot using Plotly:\n\nimport plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = 'notebook'\n\n# Sample data\nimport pandas as pd\ndata = pd.DataFrame({\n    'X': [1, 2, 3, 4, 5],\n    'Y': [10, 15, 13, 18, 20]\n})\n\n# Create an interactive line plot\nfig = px.line(data, x='X', y='Y', title='Interactive Line Plot')\nfig.show()\n\n\n                                                \n\n\nIn this example, we use Plotly Express to create a line plot from a pandas DataFrame. The resulting plot is interactive, allowing users to zoom, pan, and hover over data points for more information.\n\n\nDash: Building Interactive Web Applications\nDash is a framework built on top of Plotly that enables you to create interactive web applications for data visualization. Dash allows you to build interactive dashboards, data exploration tools, and more. Here’s a simple example of a Dash web application that displays a dynamic line plot:\n\nimport dash\nfrom dash import dcc\nfrom dash import html\nfrom dash.dependencies import Input, Output\nimport pandas as pd\nimport plotly.graph_objs as go\n\n# Sample data\ndata = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [10, 15, 13, 18, 20]})\n\n# Create a Dash web application\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    html.H1('Interactive Dash Line Plot'),\n    dcc.Graph(id='line-plot'),\n])\n\n@app.callback(\n    Output('line-plot', 'figure'),\n    [Input('line-plot', 'relayoutData')]\n)\ndef update_line_plot(relayoutData):\n    # Your data processing logic here\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=data['X'], y=data['Y'], mode='lines'))\n    fig.update_layout(title='Interactive Line Plot')\n    return fig\n\nif __name__ == '__main__':\n    # app.run_server(debug=True)    # Uncomment this line to run the server\n    pass\n\nIn this example, we create a Dash web application that renders an interactive line plot. Users can interact with the plot, and any changes they make are reflected dynamically. Dash provides extensive capabilities for building custom, interactive data applications.\n\n\nBokeh\nBokeh is another library for creating interactive visualizations. It is designed for constructing interactive plots, dashboards, and applications in Python. Bokeh offers a high level of interactivity and customization. Here’s an example of creating an interactive scatter plot with Bokeh:\n\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.models import ColumnDataSource, HoverTool\nimport pandas as pd\n\n# Output the plot to the Jupyter Notebook\noutput_notebook()\n\n# Sample data\ndata = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [10, 15, 13, 18, 20]})\n\n# Create a Bokeh figure\np = figure(title=\"Interactive Scatter Plot\", tools=\"pan,box_zoom,reset,hover\")\n\n# Add data source\nsource = ColumnDataSource(data=data)\n\n# Create a scatter plot\nscatter = p.circle(x='X', y='Y', source=source, size=10)\n\n# Add hover tool for interactivity\nhover = HoverTool()\nhover.tooltips = [(\"X\", \"@X\"), (\"Y\", \"@Y\")]\np.add_tools(hover)\n\n# Show the plot within the Jupyter Notebook\nshow(p, notebook_handle=True)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\n  \n\n\n\n\n\n&lt;Bokeh Notebook handle for In[55]&gt;\n\n\nIn this Bokeh example, we create an interactive scatter plot with a hover tool that displays data values when hovering over data points. Bokeh provides a wide range of interactive tools and widgets to enhance your visualizations.\nInteractive visualizations with Plotly, Dash, and Bokeh open up exciting possibilities for data exploration, analysis, and presentation. They allow users to dive deeper into the data and interact with visualizations in a way that static plots cannot achieve. Whether you need to build interactive dashboards, explore complex datasets, or create dynamic reports, these libraries are valuable tools in your data science toolkit."
  },
  {
    "objectID": "pages/vis.html#customizing-and-styling-plots",
    "href": "pages/vis.html#customizing-and-styling-plots",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Customizing and Styling Plots",
    "text": "Customizing and Styling Plots\nCustomizing and styling plots is a critical aspect of data visualization that significantly enhances the clarity and aesthetics of your visualizations. In this section, we will explore three key subtopics:\n\nLabels, Titles, and Legends\nColor Palettes\nPlot Annotations\n\nWe will provide detailed end-to-end examples for each subtopic to demonstrate their importance in creating informative and visually appealing data visualizations.\n\nLabels, Titles, and Legends\nLabels, titles, and legends are essential components of a well-structured data visualization. They provide context and help the audience understand the information presented. Let’s look at an example using Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny1 = [10, 15, 13, 18, 20]\ny2 = [5, 8, 6, 9, 10]\n\n# Create a line plot with labels and legends\nplt.plot(x, y1, label='Series A')\nplt.plot(x, y2, label='Series B')\n\n# Adding labels, title, and legend\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Customizing Labels, Titles, and Legends')\nplt.legend()\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we have added labels to the x and y-axes, a title to the plot, and a legend to differentiate between two series. These components make the visualization self-explanatory.\n\n\nColor Palettes\nChoosing the right color palette is crucial for improving the visual appeal of your plots. Seaborn provides various color palettes to suit different types of data. Here’s an example using Seaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = sns.load_dataset(\"iris\")\n\n# Create a pair plot with a custom color palette\ncustom_palette = ['red', 'green', 'blue']\nsns.set_palette(custom_palette)\nsns.pairplot(data, hue='species')\n\n# Adding a title\nplt.suptitle('Custom Color Palette for Pair Plot', y=1.02)\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we’ve selected a custom color palette to style a pair plot, making it visually appealing and distinctive. Seaborn’s palettes offer a wide range of choices to fit the tone and theme of your visualizations.\n\n\nPlot Annotations\nAnnotations are valuable for highlighting specific data points or features within your visualizations. They improve interpretability and guide the viewer’s attention. Here’s an example using Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 16]\n\n# Create a line plot\nplt.plot(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Plot with Annotations')\n\n# Annotating a data point\nplt.annotate('Peak Value', xy=(4, 18), xytext=(4.1, 16),\n             arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we’ve added an annotation to highlight a specific data point in the plot. Annotations can be used to provide additional context or emphasize key findings.\nCustomizing and styling plots not only enhances the aesthetics but also aids in conveying your data-driven message effectively. These techniques are valuable in creating professional and informative data visualizations."
  },
  {
    "objectID": "pages/vis.html#conclusion",
    "href": "pages/vis.html#conclusion",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, data visualization plays a pivotal role in data science, enabling data practitioners to explore, analyze, and communicate their findings effectively. This notebook has provided a comprehensive overview of data visualization using Matplotlib and Seaborn, from basic plots to advanced techniques. We encourage you to practice and apply your knowledge to real-world projects, as data visualization is an essential skill for any data scientist or analyst."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html",
    "href": "pages/dealing_with_missing_values.html",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#introduction",
    "href": "pages/dealing_with_missing_values.html#introduction",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#bias",
    "href": "pages/dealing_with_missing_values.html#bias",
    "title": "Dealing with missing-values",
    "section": "Bias",
    "text": "Bias\nWhen data is missing, the remaining data may not always accurately reflect the true population. The type of missingness and the way we deal with it can dictate bias in our results.\nFor example, in a survey recording income of individuals, those with low income may choose not to respond. The use of average income to impute the missing values can lead to bias as the average of the available data may not represent that of the population."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-completely-at-random-mcar",
    "href": "pages/dealing_with_missing_values.html#missing-completely-at-random-mcar",
    "title": "Dealing with missing-values",
    "section": "Missing Completely At Random (MCAR)",
    "text": "Missing Completely At Random (MCAR)\nIn situations characterized by Missing Completely At Random (MCAR), the absence of data occurs randomly and is unrelated to any variable in the dataset or the missing values themselves. The probability of missingness is same for all observations. There exists no underlying pattern to the missingness. The missing values are completely independent of other data. All statistical analysis performed on the dataset will remain unbiased in this case.\nFor example, during data collection, if some responses were not collected due to a technical error, then the missing data is completely at random."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-at-random-mar",
    "href": "pages/dealing_with_missing_values.html#missing-at-random-mar",
    "title": "Dealing with missing-values",
    "section": "Missing At Random (MAR)",
    "text": "Missing At Random (MAR)\nIn instances of Missing At Random (MAR), the absence of data can be entirely explained by the values of other known variables in the dataset. There exists some form of pattern in the missing values.\nFor example, In a survey, women might be unwilling to disclose their age. Here the missingness of the variable age can be explained by another observed variable “gender”."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-not-at-random-mnar",
    "href": "pages/dealing_with_missing_values.html#missing-not-at-random-mnar",
    "title": "Dealing with missing-values",
    "section": "Missing Not At Random (MNAR)",
    "text": "Missing Not At Random (MNAR)\nIn this case, the missingness is neither MCAR nor MAR. The fact that a datapoint is missing is dependent on the value of the data point. In order to correct for the bias we would have to make modelling assumptions about the nature of the bias.\nFor example, in a social survey where individuals are asked about their income, respondents may not disclose it if it is too high or too low. Thus the missingness in the feature income is directly related to the values of that feature.\nFor example, if a patient’s measurement was not taken because the doctor felt he was too sick, that observation would not be MAR or MCAR. In this case the missing data mechanism causes our observed training data to give a distorted picture of the true population, and data imputation is dangerous in this instance."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#identifying-the-type-of-missingness",
    "href": "pages/dealing_with_missing_values.html#identifying-the-type-of-missingness",
    "title": "Dealing with missing-values",
    "section": "Identifying the type of missingness",
    "text": "Identifying the type of missingness\n\nUnderstanding the data collection process, the features involved and the research domain can help identify possible patterns as to why data is missing\nGraphical representation of the missing data and heatmaps can help identify relationships between features that can be utilized to make better imputation of the missing data\nWe can impute the data using different techniques while also making assumptions on the nature of missingness. Subsequently, we analyze the results to observe the assumptions that have led to consistent results.\nFor categorical features, we can code missing as an additional class(feature). We can then fit our model on the training data and see if the class “Missing” is predictive of the response(label)"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#dealing-with-missing-values",
    "href": "pages/dealing_with_missing_values.html#dealing-with-missing-values",
    "title": "Dealing with missing-values",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nIf we assume that the features are missing completely at random, there are three approaches that we can follow\n\nDiscard the observations with any missing values\nChoose an algorithm that inherently deals with missing values\nImpute the missing values before training"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-value-imputation",
    "href": "pages/dealing_with_missing_values.html#missing-value-imputation",
    "title": "Dealing with missing-values",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\nIn many cases it might not be feasible to discard observations that contain missing values. This could be due to the availability of lesser number of samples or due to the importance of each observation. In such cases we can employ imputation which deals with replacing the missing values with some predicted value. We will have a look at two simple imputation techniques that can be implemented using the sklearn package.\n\nSimple Imputer\nK Nearest neighbours Imputer"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#simple-imputer",
    "href": "pages/dealing_with_missing_values.html#simple-imputer",
    "title": "Dealing with missing-values",
    "section": "Simple Imputer",
    "text": "Simple Imputer\nSimple Imputation is a univariate imputation technique where the missing values of a feature are imputed using the non-missing values of the same feature. It offers a basic approach to filling missing data wherein we replace the missing data with a constant value or utilize statistics such as “mean”, “mode”, or “median” of the available values.\nThe technique is useful for its simplicity and can serve as a reference technique. It is also worth noting that this can lead to biased or unrealistic results\n\nSimple Imputer - Mean\n\nimport numpy as np\nimport pandas as pd\n\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mean strategy. The mean value of that feature will be \\frac{( 1+1+2+4)}{4} = 2\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 2 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\ndata = {\"feature_1\":[1,2,3,4,5],\n        \"feature_2\":[1,1,np.nan,2,4],\n        \"feature_3\":[3,3,3,4,5]}\n\n\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1\n1.0\n3\n\n\n1\n2\n1.0\n3\n\n\n2\n3\nNaN\n3\n\n\n3\n4\n2.0\n4\n\n\n4\n5\n4.0\n5\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom sklearn.impute import SimpleImputer\n\n\nimputer = SimpleImputer(strategy='mean')\nimputed_data = imputer.fit_transform(df)\n\n\nimputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nimputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n2.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSimple Imputer - Mode\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mode strategy. The value that appears most often is 1.\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nmode_imputer = SimpleImputer(strategy='most_frequent')\nimputed_data = mode_imputer.fit_transform(df)\n\n\nmode_imputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nmode_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#k-nearest-neighbours",
    "href": "pages/dealing_with_missing_values.html#k-nearest-neighbours",
    "title": "Dealing with missing-values",
    "section": "K Nearest Neighbours",
    "text": "K Nearest Neighbours\nThis technique operates on the principle of finding the K most similar data points to the one with missing values and using their known values to estimate and impute the missing values.\nKNN imputation is especially useful when there is a pattern or structure in the data that can be captured by considering the relationships between neighboring data points. It’s a flexible method that can be applied to various types of data, but the choice of K and the distance metric are critical parameters that can impact imputation accuracy.\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix}   We can fill the missing value using KNN Imputer with value of k = 2. Firstly, we need to find the distance of our entry(row) containing the missing value, with other entries(rows).   The distance of \\begin{bmatrix} 3 & nan & 3 \\end{bmatrix} with \\begin{bmatrix} 1 & 1 & 3 \\end{bmatrix} is given by ,  \\displaystyle \\sqrt{\\frac{3}{2}\\left(( 3-1)^{2} +( 3-3)^{2}\\right)} =\\ \\sqrt{6}  In this example the nearest neighbors based on Euclidean distance will be the points \\begin{bmatrix} 2 & 1 & 3 \\end{bmatrix} and \\begin{bmatrix} 4 & 2 & 4 \\end{bmatrix}.Now to fill the missing value, we take the average of the values of the feature from the 2 nearest neighbors identified above.\n\n\\begin{align*}\n\\frac{\\text{Sum of values of the feature from 2 neares neighbors}}{\\text{Number of values}} & =\\frac{1+2}{2} \\ =\\ 1.5\n\\end{align*}\n\nThus the updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1.5 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nfrom sklearn.impute import KNNImputer\n\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nknn_imputed_data = knn_imputer.fit_transform(df)\n\n\nknn_imputed_df = pd.DataFrame(knn_imputed_data, columns=['feature_1','feature_2','feature_3'])\nknn_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.5\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#exercises",
    "href": "pages/dealing_with_missing_values.html#exercises",
    "title": "Dealing with missing-values",
    "section": "Exercises",
    "text": "Exercises\nUse the links provided below to access the datasets. Treat the dataset like an unsupervised learning problem with the only “constraint” being that findings/results should pertain to missingness. Share your findings/results in a ipynb file.\n\nLink to dataset 1: https://drive.google.com/file/d/1Fk5V6GNNfdgm8qB4sA1EinB9gQhNdg4Z/view?usp=drive_link\nLink to dataset 2: https://drive.google.com/file/d/15HYh3p6SZjic_Ytef84b1vsgYcWoqh4L/view?usp=drive_link"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#additional-info",
    "href": "pages/dealing_with_missing_values.html#additional-info",
    "title": "Dealing with missing-values",
    "section": "Additional Info",
    "text": "Additional Info\n\nDealing with missingness in Tree-Based Methods\nFor tree based models we can try a couple of different approaches\nApproach 1: The first is applicable to categorical predictors: we simply make a new category for “missing.” From this we might discover that observations with missing values for some measurement behave differently than those with nonmissing values.\nApproach 2: The second more general approach is the construction of surrogate variables. Surrogate splits use alternative variables when the primary variable has missing values. The idea is to leverage the correlation or similarity between variables, allowing them to substitute for each other in the splitting process. This enables the inclusion of records with missing values in the training data, ensuring they are split based on the best available variable. The effectiveness of this technique is dependent on the quality and availability of surrogate variables.\n\n\nMissingness Indicator\nThe idea of missingness Indicator is to make use of a binary indicator that highlights the missing values in your dataset. This can be used when we want the ML models to capture information about the missingness\n\n\nEstimators that handle NaN values\nCART, MARS, PRIM, GBM"
  },
  {
    "objectID": "pages/Bivariate_EM.html",
    "href": "pages/Bivariate_EM.html",
    "title": "EM Algorithm",
    "section": "",
    "text": "Colab: Click here!\n\n!wget https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/hw/hw02/old_faithful.csv\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom scipy.stats import multivariate_normal\nimport pandas as pd\nfrom matplotlib import mlab\n\nX = pd.read_csv('old_faithful.csv')[['eruptions', 'waiting']].to_numpy()\nplt.scatter(X[:, 0], X[:, 1])\n\nM, N = np.mgrid[min(X[:, 0])-1:max(X[:, 0])+1:0.01, min(X[:, 1])-1:max(X[:, 1])+1:0.01]\ngrid = np.dstack((M, N))\n\n--2023-11-09 15:07:06--  https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/hw/hw02/old_faithful.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2270 (2.2K) [text/plain]\nSaving to: ‘old_faithful.csv.5’\n\nold_faithful.csv.5    0%[                    ]       0  --.-KB/s               old_faithful.csv.5  100%[===================&gt;]   2.22K  --.-KB/s    in 0s      \n\n2023-11-09 15:07:06 (38.5 MB/s) - ‘old_faithful.csv.5’ saved [2270/2270]\n\n\n\n\n\n\n\nK = 2\n\nrng = np.random.default_rng()\ncentres = X[rng.integers(low=0, high=len(X), size=K)]\nl = np.array([(lambda i: [int(i==j) for j in range(K)])(np.argmin([np.linalg.norm(p-centre) for centre in centres])) for p in X])\npi = l.sum(axis=0)/l.sum()\n\ncov = []\nfor i in range(K):\n  cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\ncov = np.array(cov)\n\ndef plot():\n\n  plt.scatter(X[:, 0], X[:, 1], color='black', marker='+')\n  plt.scatter(centres[:, 0], centres[:, 1], color='red', marker='x')\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  plt.contour(M, N, probability_grid)\n  plt.show()\n\nplot()\n\n\n\n\n\n# Expectation Step\ndef Exp(l):\n  pi = l.sum(axis=0)/l.sum()\n  centres, cov = [], []\n  for i in range(K):\n    centres.append(np.dot(l[:, i], X)/l[:, i].sum())\n    cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\n\n  return pi, np.array(centres), np.array(cov)\n\n# Maximization step\ndef Max(pi, centres, cov):\n  l = []\n  for i in X:\n    p = np.array([pi[k] * multivariate_normal.pdf(i, mean=centres[k], cov=cov[k]) for k in range(K)])\n    p = p/p.sum()\n    l.append(p)\n\n  return np.array(l)\n\n# Convergence criterion\nnorm_theta = lambda pi, centres, cov: np.linalg.norm(np.r_[pi, centres.reshape(-1), cov.reshape(-1)])\n\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\nprev_norm = norm_theta(pi, centres, cov)\n\nfig, ax = plt.subplots()\nartists = []\n\nwhile True:\n  l = Max(pi, centres, cov)\n  pi, centres, cov = Exp(l)\n\n  frame = []\n  frame.append(ax.scatter(X[:, 0], X[:, 1], color='black', marker='+'))\n  frame.append(ax.scatter(centres[:, 0], centres[:, 1], color='red', marker='x'))\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  frame += list(ax.contour(M, N, probability_grid).collections)\n  artists.append(frame)\n\n  curr_norm = norm_theta(pi, centres, cov)\n\n  if abs(curr_norm-prev_norm) &lt; 1:\n    break\n  else:\n    prev_norm = curr_norm\n\nplt.close()\nanim = animation.ArtistAnimation(fig, artists, interval=200, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "pages/Ordinal_Classification.html",
    "href": "pages/Ordinal_Classification.html",
    "title": "Ordinal Classification",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Ordinal_Classification.html#synthetic-data-creation",
    "href": "pages/Ordinal_Classification.html#synthetic-data-creation",
    "title": "Ordinal Classification",
    "section": "Synthetic Data Creation",
    "text": "Synthetic Data Creation\nWe will consider an ordinal classification problem with 4 classes. We randomly sample 1000 points from a standard normal distribution. We fix \\beta = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, and cutpoints \\alpha_0 = -\\infty, \\alpha_1 = -2, \\alpha_2 = -1, \\alpha_3 = 2, \\alpha_4 = \\infty\n\nimport numpy as np\nnp.random.seed(69)\n\nX = np.random.normal(scale=1, size=(1000, 2))\nbeta = np.array([-1, 1])\ncutpoints = np.array([-np.inf, -2, -1, 2, np.inf])\n\n\nfrom sklearn.model_selection import train_test_split\n\nY = X@beta\n\ndef ordify(cutpoints):\n\n  def hlo(x):\n\n    for i in range(len(cutpoints)-1):\n      if cutpoints[i] &lt;= x &lt; cutpoints[i+1]:\n        return i+1\n\n  return hlo\n\nordinate = ordify(cutpoints)\nY_ord = np.array([ordinate(i) for i in Y])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, Y_ord, test_size=0.33, random_state=42)"
  },
  {
    "objectID": "pages/Ordinal_Classification.html#how-to-predict",
    "href": "pages/Ordinal_Classification.html#how-to-predict",
    "title": "Ordinal Classification",
    "section": "How to predict?",
    "text": "How to predict?\nFor a validation datapoint x_i, use the trained models to calculate estimates \\hat{N_i} \\text{ for } i = 1, 2, \\cdots, K-1. We then use these to calculate probabilities of each class as follows:\n\n\n\nClass\nProbability\n\n\n\n\n1\n1 - \\hat{N_1}\n\n\n2\n\\hat{N_1} - \\hat{N_2}\n\n\ni\n\\hat{N_{i-1}} - \\hat{N_i}\n\n\nK-1\n\\hat{N_{K-1}}\n\n\n\nThe first and last class probabilites are from a single classifier, where as the others are the difference of the outputs from a pair of consecutive (w.r.t i) classifiers.\nNote that \\hat{N_i} = \\text{Pr}(y_i &gt; i).\n\nfrom sklearn.base import clone, BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted, check_array\nfrom sklearn.utils.multiclass import check_classification_targets\nimport numpy as np\n\nclass OrdinalClassifier(BaseEstimator, ClassifierMixin):\n\n    def __init__(self,learner):\n        self.learner = learner\n        self.ordered_learners = dict()\n        self.classes = []\n\n    def fit(self,X,y):\n        self.classes = np.sort(np.unique(y))\n        assert self.classes.shape[0] &gt;= 3, f'OrdinalClassifier needs at least 3 classes, only {self.classes.shape[0]} found'\n\n        for i in range(self.classes.shape[0]-1):\n            N_i = np.vectorize(int)(y &gt; self.classes[i])\n            learner = clone(self.learner).fit(X,N_i)\n            self.ordered_learners[i] = learner\n\n    def predict(self,X):\n        return np.vectorize(lambda i: self.classes[i])(np.argmax(self.predict_proba(X), axis=1))\n\n    def predict_proba(self,X):\n        predicted = [self.ordered_learners[k].predict_proba(X)[:,1].reshape(-1,1) for k in self.ordered_learners]\n\n        N_1 = 1-predicted[0]\n        N_K  = predicted[-1]\n        N_i= [predicted[i] - predicted[i+1] for i in range(len(predicted) - 1)]\n\n        probs = np.hstack([N_1, *N_i, N_K])\n\n        return probs\n\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = OrdinalClassifier(LogisticRegression())\nmodel.fit(X, Y_ord)\n\nmodel.score(X, Y_ord)\n\n0.975"
  },
  {
    "objectID": "pages/standard.html",
    "href": "pages/standard.html",
    "title": "Investigation of Standard Scaling Influence",
    "section": "",
    "text": "Colab Link: Click here!\nThis Jupyter Notebook is dedicated to an in-depth investigation of feature scaling’s significance, specifically focusing on standard scaling, also known as Z-score normalization. Feature scaling is an indispensable preprocessing step in numerous machine learning algorithms. It often enhances model performance. This study is vital as it sheds light on the practical implications of feature scaling in real-world applications. The wine dataset from the UCI Machine Learning Repository will be employed to demonstrate the effects of feature scaling."
  },
  {
    "objectID": "pages/standard.html#overview",
    "href": "pages/standard.html#overview",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Overview",
    "text": "Overview\nFeature scaling is a vital preprocessing step in various machine learning algorithms, and one of the most used ones is the Standard Scaler. It involves rescaling each feature in the dataset to have a standard deviation of 1 and a mean of 0. This normalization is necessary for several reasons, although tree-based models are less affected by feature scaling. Other algorithms might require feature normalization for different purposes, such as improving convergence or creating different model fits."
  },
  {
    "objectID": "pages/standard.html#the-wine-dataset",
    "href": "pages/standard.html#the-wine-dataset",
    "title": "Investigation of Standard Scaling Influence",
    "section": "The Wine Dataset",
    "text": "The Wine Dataset\nThe wine dataset from UCI will be used in this study. This dataset contains continuous features that measure different properties, such as alcohol content, malic acid, amongst others. These features are heterogeneous in scale, making it an excellent example to illustrate the effects of standard scaling.\n\nData Loading and Preparation\nWe will start by loading and preparing the wine dataset for our analysis. We will also split the data into training and testing sets. This is a common practice in machine learning to evaluate the performance of a model on unseen data.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the wine dataset\nX, y = load_wine(return_X_y=True, as_frame=True)\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nscaler.set_output(transform=\"pandas\")\nscaled_X_train = scaler.fit_transform(X_train)"
  },
  {
    "objectID": "pages/standard.html#analysis-of-standard-scaling-effects",
    "href": "pages/standard.html#analysis-of-standard-scaling-effects",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Analysis of Standard Scaling Effects",
    "text": "Analysis of Standard Scaling Effects\n\nVisualizing the Effect on a K-Neighbors Model\nTo visually demonstrate the effect of standard scaling on a K-Neighbors Classifier, we select a subset of two features, “proline” and “hue,” which have values with different orders of magnitude. We will visualize the decision boundary of the classifier with and without scaling. The K-Neighbors Classifier is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# Define the features for visualization\nX_plot = X[[\"proline\", \"hue\"]]\nX_plot_scaled = scaler.fit_transform(X_plot)\n\n# Create K-Neighbors Classifier\nclf = KNeighborsClassifier(n_neighbors=20)\n\n# Define a function to fit and plot the model\ndef fit_and_plot_model(X_plot, y, clf, ax):\n    clf.fit(X_plot, y)\n    disp = DecisionBoundaryDisplay.from_estimator(\n        clf, X_plot, response_method=\"predict\", alpha=0.5, ax=ax\n    )\n    disp.ax_.scatter(X_plot[\"proline\"], X_plot[\"hue\"], c=y, s=20, edgecolor=\"k\")\n    disp.ax_.set_xlim((X_plot[\"proline\"].min(), X_plot[\"proline\"].max()))\n    disp.ax_.set_ylim((X_plot[\"hue\"].min(), X_plot[\"hue\"].max()))\n    return disp.ax_\n\n# Plot the decision boundaries\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nfit_and_plot_model(X_plot, y, clf, ax1)\nax1.set_title(\"KNN without scaling\")\nfit_and_plot_model(X_plot_scaled, y, clf, ax2)\nax2.set_xlabel(\"scaled proline\")\nax2.set_ylabel(\"scaled hue\")\n_ = ax2.set_title(\"KNN with scaling\")\n\n\n\n\nThe visualizations depict a significant change in the decision boundary when we scale the features. Without scaling, the variable “proline” dominates the decision boundary due to its higher magnitude, while “hue” is comparatively ignored. After scaling, both variables have similar impacts on the decision boundary.\n\n\nImpact of Standard Scaling on PCA\nNext, we will examine the effect of standard scaling on Principal Component Analysis (PCA). PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance. Scaling is crucial as it ensures that features with different scales do not dominate the principal components.\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\npca = PCA(n_components=2).fit(X_train)\nscaled_pca = PCA(n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\n# Visualize the weights of the first principal component\nfirst_pca_component = pd.DataFrame(\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n_ = plt.tight_layout()\n\n\n\n\nAs observed, the “proline” feature dominates the direction of the first principal component without scaling, being about two orders of magnitude above the other features. This is contrasted when observing the first principal component for the scaled version of the data, where the orders of magnitude are roughly the same across all the features.\nWe can visualize the distribution of the principal components in both cases:\n\n# Visualize the distribution of principal components\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\ntarget_classes = range(0, 3)\ncolors = (\"blue\", \"red\", \"green\")\nmarkers = (\"^\", \"s\", \"o\")\nfor target_class, color, marker in zip(target_classes, colors, markers):\n    ax1.scatter(\n        x=X_train_transformed[y_train == target_class, 0],\n        y=X_train_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\n    ax2.scatter(\n        x=X_train_std_transformed[y_train == target_class, 0],\n        y=X_train_std_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\nax1.set_title(\"Unscaled training dataset after PCA\")\nax2.set_title(\"Standardized training dataset after PCA\")\nfor ax in (ax1, ax2):\n    ax.set_xlabel(\"1st principal component\")\n    ax.set_ylabel(\"2nd principal component\")\n    ax.legend(loc=\"upper right\")\n    ax.grid()\n_ = plt.tight_layout()\n\n\n\n\nIn the above visualizations, we can see the impact of scaling on PCA. Without scaling, one feature dominates the first principal component, while scaling results in components with similar orders of magnitude across all features."
  },
  {
    "objectID": "pages/standard.html#conclusion",
    "href": "pages/standard.html#conclusion",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Conclusion",
    "text": "Conclusion\nThis Jupyter Notebook has explored the effects of standard scaling on machine learning models using the wine dataset. We observed how standard scaling influences decision boundaries and the behavior of PCA. Scaling the features ensures that no single feature dominates the analysis and can lead to improved model performance. It is an important preprocessing step to consider when working with machine learning algorithms. Future research could focus on the effects of other scaling methods and their impact on different types of machine learning models."
  },
  {
    "objectID": "pages/standard.html#references",
    "href": "pages/standard.html#references",
    "title": "Investigation of Standard Scaling Influence",
    "section": "References",
    "text": "References\nThis notebook is based on the sklearn document titled “Importance of Feature Scaling”. You can find more information at the following link: Importance of Feature Scaling."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html",
    "href": "pages/Scaling multi-dataset multi-algo.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "href": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "href": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "href": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Datasets",
    "text": "Loading the Datasets\nIn this section, we will start by loading four distinct datasets, each with its unique characteristics. These datasets are commonly used for various classification tasks and will serve as the foundation for our comprehensive study on the impact of feature scaling on classification models.\n\n# Load the datasets\niris = load_iris()\ndigits = load_digits()\nwine = load_wine()\nbreast_cancer = load_breast_cancer()\n\n# Create DataFrames for the datasets\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\ndigits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\nbreast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n\n\n1. Iris Dataset\nDescription: The Iris dataset is a classic dataset in the field of machine learning and consists of 150 samples of iris flowers, each from one of three species: Iris setosa, Iris virginica, and Iris versicolor. There are four features—sepal length, sepal width, petal length, and petal width—measured in centimeters.\nUse Case: This dataset is often used for practicing classification techniques, especially for building models to distinguish between the three iris species based on their feature measurements.\n\n# Display the first few rows of iris dataset\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\n\n2. Digits Dataset\nDescription: The Digits dataset is a collection of 8x8 pixel images of handwritten digits (0 through 9). There are 1,797 samples, and each sample is an 8x8 image, resulting in 64 features. The goal is to correctly classify the digits based on these pixel values.\nUse Case: This dataset is a fundamental resource for pattern recognition and is frequently used for exploring image classification and digit recognition algorithms.\n\n\n3. Wine Dataset\nDescription: The Wine dataset comprises 178 samples of wine classified into three classes based on their cultivar. The dataset contains 13 feature variables, including measurements related to chemical composition, making it a valuable resource for wine classification tasks.\nUse Case: Wine quality prediction and classification are common applications for this dataset, as it allows for distinguishing between different wine types.\n\n# Display the first few rows of wine dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\n\n\n4. Breast Cancer Dataset\nDescription: The Breast Cancer dataset is used for breast cancer diagnosis. It includes 569 samples with 30 feature variables, primarily related to characteristics of cell nuclei present in breast cancer biopsies. The dataset is labeled to indicate whether a sample is benign or malignant.\nUse Case: This dataset is often employed for building classification models to assist in the early detection of breast cancer, aiding in medical diagnosis.\n\n# Display the first few rows of breast cancer dataset\nbreast_cancer_df.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows × 31 columns\n\n\n\nThe datasets contain various features related to their respective domains, with a ‘target’ column indicating the class labels."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "href": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Define a function to create noisy datasets\ndef create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n    X = dataset.data\n    y = dataset.target\n\n    rng = np.random.default_rng(seed=random_state)\n    noise = rng.normal(0, noise_std, size=X.shape)\n    X_noisy = X + noise\n\n    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n\n    return X_train_noisy, X_test_noisy, y_train, y_test\n\n# Create noisy datasets for all four datasets\nX_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\nX_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\nX_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\nX_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "href": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Define a function to apply Standard Scaler to a dataset\ndef apply_standard_scaler(X_train, X_test):\n    standard_scaler = StandardScaler()\n    X_train_scaled = standard_scaler.fit_transform(X_train)\n    X_test_scaled = standard_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Standard Scaler to all four datasets\nX_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Define a function to apply Min-max Scaler to a dataset\ndef apply_min_max_scaler(X_train, X_test):\n    min_max_scaler = MinMaxScaler()\n    X_train_scaled = min_max_scaler.fit_transform(X_train)\n    X_test_scaled = min_max_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Min-max Scaler to all four datasets\nX_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Define a function to apply Maximum Absolute Scaler to a dataset\ndef apply_max_abs_scaler(X_train, X_test):\n    max_abs_scaler = MaxAbsScaler()\n    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n    X_test_scaled = max_abs_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Maximum Absolute Scaler to all four datasets\nX_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Define a function to apply Robust Scaler to a dataset\ndef apply_robust_scaler(X_train, X_test):\n    robust_scaler = RobustScaler()\n    X_train_scaled = robust_scaler.fit_transform(X_train)\n    X_test_scaled = robust_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Robust Scaler to all four datasets\nX_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Define a function to apply Quantile Transformer to a dataset\ndef apply_quantile_transformer(X_train, X_test):\n    quantile_transformer = QuantileTransformer(output_distribution='normal')\n    X_train_scaled = quantile_transformer.fit_transform(X_train)\n    X_test_scaled = quantile_transformer.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Quantile Transformer to all four datasets\nX_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "href": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of six classification models on the different scaled datasets. The models we will use are:\n\nRandom Forest\nSupport Vector Machine (SVM)\nDecision Tree\nNaive Bayes (GaussianNB)\nK-Nearest Neighbors (KNN)\nLogistic Regression\n\nFor each scaling method, we will train and evaluate all six models for all four datasets.\n\n# Initialize the classifiers\nrf_classifier = RandomForestClassifier(random_state=42)\nsvm_classifier = SVC(random_state=42)\ndt_classifier = DecisionTreeClassifier(random_state=42)\nnb_classifier = GaussianNB()\nknn_classifier = KNeighborsClassifier()\nlr_classifier = LogisticRegression()\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each dataset and scaling method, and evaluate the models\ndatasets = [\n    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n]\n\nscaling_methods = {\n    \"No Scaling\": {\n        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n    },\n    \"Standard Scaler\": {\n        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n    },\n    \"Min-max Scaler\": {\n        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n    },\n    \"Maximum Absolute Scaler\": {\n        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n    },\n    \"Robust Scaler\": {\n        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n    },\n    \"Quantile Transformer\": {\n        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n    }\n}\n\n# Loop through datasets and scaling methods\nfor dataset_name, X_train, X_test, y_train, y_test in datasets:\n    for scaler_name, scaled_data in scaling_methods.items():\n        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n\n        # Train and evaluate all six models\n        for classifier, classifier_name in zip([rf_classifier, svm_classifier, dt_classifier, nb_classifier, knn_classifier, lr_classifier], [\"Random Forest\", \"SVM\", \"Decision Tree\", \"Naive Bayes\", \"K-Nearest Neighbors\", \"Logistic Regression\"]):\n            classifier.fit(X_train_scaled, y_train)\n            predictions = classifier.predict(X_test_scaled)\n            accuracy = accuracy_score(y_test, predictions)\n            accuracy_scores.append([dataset_name, scaler_name, classifier_name, accuracy])"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "href": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet’s analyze the results of our experiment and discuss the impact of different scaling methods on multiple classification models for each dataset.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Classifier', 'Accuracy'])\nresults_df\n\n# Create a new DataFrame to display the results\nresults_pivoted_df = results_df.pivot(index=['Dataset', 'Scaling Method'], columns='Classifier', values='Accuracy')\nresults_pivoted_df.reset_index(inplace=True)\nresults_pivoted_df.columns.name = None  # Remove the column name\n\n# Fill any missing values with NaN (for clarity)\nresults_pivoted_df.fillna(value=np.nan, inplace=True)\n\nresults_pivoted_df\n\n\n\n\n\n\n\n\nDataset\nScaling Method\nDecision Tree\nK-Nearest Neighbors\nLogistic Regression\nNaive Bayes\nRandom Forest\nSVM\n\n\n\n\n0\nBreast Cancer\nMaximum Absolute Scaler\n0.938596\n0.903509\n0.929825\n0.956140\n0.964912\n0.929825\n\n\n1\nBreast Cancer\nMin-max Scaler\n0.938596\n0.912281\n0.964912\n0.956140\n0.964912\n0.956140\n\n\n2\nBreast Cancer\nNo Scaling\n0.938596\n0.956140\n0.973684\n0.956140\n0.964912\n0.947368\n\n\n3\nBreast Cancer\nQuantile Transformer\n0.938596\n0.956140\n0.964912\n0.947368\n0.964912\n0.956140\n\n\n4\nBreast Cancer\nRobust Scaler\n0.938596\n0.964912\n0.964912\n0.956140\n0.964912\n0.973684\n\n\n5\nBreast Cancer\nStandard Scaler\n0.938596\n0.938596\n0.964912\n0.956140\n0.964912\n0.964912\n\n\n6\nDigits\nMaximum Absolute Scaler\n0.858333\n0.983333\n0.961111\n0.905556\n0.972222\n0.983333\n\n\n7\nDigits\nMin-max Scaler\n0.858333\n0.988889\n0.963889\n0.905556\n0.972222\n0.983333\n\n\n8\nDigits\nNo Scaling\n0.858333\n0.986111\n0.966667\n0.905556\n0.972222\n0.991667\n\n\n9\nDigits\nQuantile Transformer\n0.858333\n0.966667\n0.941667\n0.900000\n0.969444\n0.975000\n\n\n10\nDigits\nRobust Scaler\n0.858333\n0.819444\n0.963889\n0.905556\n0.972222\n0.913889\n\n\n11\nDigits\nStandard Scaler\n0.858333\n0.975000\n0.977778\n0.905556\n0.972222\n0.986111\n\n\n12\nIris\nMaximum Absolute Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n13\nIris\nMin-max Scaler\n0.933333\n0.966667\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n14\nIris\nNo Scaling\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n15\nIris\nQuantile Transformer\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n16\nIris\nRobust Scaler\n0.933333\n0.933333\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n17\nIris\nStandard Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.966667\n\n\n18\nWine\nMaximum Absolute Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n19\nWine\nMin-max Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n20\nWine\nNo Scaling\n0.916667\n0.722222\n0.972222\n1.000000\n1.000000\n0.805556\n\n\n21\nWine\nQuantile Transformer\n0.916667\n0.944444\n1.000000\n0.972222\n1.000000\n0.972222\n\n\n22\nWine\nRobust Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n23\nWine\nStandard Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# Define a list of scaling methods and their Material Design colors\nscaling_methods = results_pivoted_df['Scaling Method'].unique()\nmaterial_colors = sns.color_palette(\"Set2\")\ndatasets = results_pivoted_df['Dataset'].unique()\n\n# Set Seaborn style for a modern, beautiful look\nsns.set_style(\"whitegrid\")\n\n# Create grouped bar charts for each dataset\nfor dataset in datasets:\n    plt.figure(figsize=(12, 8))\n    \n    # Filter the data for the current dataset\n    dataset_data = results_pivoted_df[results_pivoted_df['Dataset'] == dataset]\n    \n    # Define the x-axis labels (ML algorithms)\n    ml_algorithms = dataset_data.columns[2:].tolist()\n    \n    bar_width = 0.12  # Width of each bar\n    index = range(len(ml_algorithms))\n\n    # Create a bar for each scaling method\n    for i, method in enumerate(scaling_methods):\n        try:\n            accuracies = dataset_data[dataset_data['Scaling Method'] == method].values[0][2:]\n            plt.bar([x + i * bar_width for x in index], accuracies, bar_width, label=method, color=material_colors[i])\n        except IndexError:\n            print(f\"No data found for {method} in {dataset}\")\n    \n    # Set the x-axis labels, title, and legend\n    plt.xlabel('ML Algorithms')\n    plt.ylabel('Accuracy')\n    plt.title(f'Accuracy Comparison for {dataset}')\n    plt.xticks([x + bar_width * (len(scaling_methods) / 2) for x in index], ml_algorithms)\n    plt.legend(scaling_methods, title='Scaling Method')\n\n    # Adjust layout to prevent overlapping x-axis labels\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    # Show the plot or save it as an image\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of Results\nThe evaluation of results is based on the performance of six classification algorithms across different datasets and scaling methods. The accuracy scores are presented for Decision Tree, K-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Random Forest, and Support Vector Machine (SVM). Here’s an analysis of the findings:\n\nBreast Cancer Dataset\n\nMaximum Absolute Scaler: This scaling method produced competitive accuracy scores for all algorithms. Naive Bayes and Logistic Regression achieved the highest accuracy of approximately 95.6%, while other algorithms also performed well.\nMin-max Scaler: Similar to the Maximum Absolute Scaler, this method yielded strong accuracy results across all algorithms, with Logistic Regression and SVM reaching the highest scores of 96.4% and 95.6%, respectively.\nNo Scaling: Surprisingly, this dataset demonstrated that some algorithms, particularly Naive Bayes and Logistic Regression, do not benefit from feature scaling. They achieved high accuracy without any scaling, indicating that the original feature values were suitable for these models.\nQuantile Transformer: The Quantile Transformer showed consistent accuracy, with Logistic Regression and SVM achieving the highest scores of 96.4% and 95.6%, respectively.\nRobust Scaler: Robust scaling led to competitive accuracy for most algorithms, with SVM achieving the highest accuracy of 97.4%.\nStandard Scaler: Standard scaling demonstrated similar results to other scaling methods, with Logistic Regression and SVM achieving the highest accuracy of 96.4%.\n\n\n\nDigits Dataset\n\nMaximum Absolute Scaler: This scaling method had a positive impact on KNN, which achieved a high accuracy of approximately 98.3%. However, Decision Tree and Logistic Regression showed lower performance.\nMin-max Scaler: Min-max scaling improved the accuracy of KNN to nearly 98.9%. Other algorithms also benefited from this scaling.\nNo Scaling: Surprisingly, the Digits dataset, particularly for KNN, demonstrated that feature scaling is not necessary for achieving high accuracy. KNN reached 99.2% accuracy without any scaling.\nQuantile Transformer: While other algorithms performed well with this scaling method, Decision Tree and KNN showed slightly reduced accuracy.\nRobust Scaler: Robust scaling did not benefit KNN, with its accuracy dropping to 81.9%. Other algorithms showed consistent performance.\nStandard Scaler: Standard scaling improved the accuracy of KNN to 98.6%, making it one of the best-performing algorithms for this dataset.\n\n\n\nIris Dataset\n\nMaximum Absolute Scaler: Scaling had minimal impact on the accuracy of algorithms for the Iris dataset. SVM achieved the highest accuracy of 96.7%, regardless of scaling.\nMin-max Scaler: Similar to Maximum Absolute Scaling, min-max scaling had a limited effect on the accuracy of algorithms. SVM consistently achieved the highest accuracy of 96.7%.\nNo Scaling: The Iris dataset was naturally well-scaled, and most algorithms, particularly SVM, achieved high accuracy without any scaling.\nQuantile Transformer: Scaling had little influence on accuracy. SVM remained the best-performing algorithm, with an accuracy of 96.7%.\nRobust Scaler: Robust scaling slightly improved the accuracy of Decision Tree and SVM but had limited impact overall.\nStandard Scaler: Standard scaling resulted in consistent accuracy for all algorithms, with SVM maintaining the highest accuracy of 96.7%.\n\n\n\nWine Dataset\n\nMaximum Absolute Scaler: This scaling method had a substantial impact on SVM, boosting its accuracy to 100%. Other algorithms also reached high accuracy levels.\nMin-max Scaler: Min-max scaling had a similar effect on SVM, resulting in perfect accuracy. Decision Tree, KNN, and Logistic Regression also reached maximum accuracy.\nNo Scaling: The Wine dataset revealed the significance of scaling, particularly for SVM. Without scaling, SVM’s accuracy was relatively low at 80.6%, highlighting the sensitivity of SVM to feature values.\nQuantile Transformer: Quantile transformation improved the accuracy of Decision Tree and Logistic Regression. However, SVM remained sensitive to scaling.\nRobust Scaler: Robust scaling had a positive impact, with SVM reaching perfect accuracy. Other algorithms also performed well.\nStandard Scaler: Standard scaling had a similar effect to other scaling methods, with SVM achieving perfect accuracy.\n\n\n\n\nConclusion\nIn summary, the impact of feature scaling on machine learning algorithms varies depending on the dataset and the algorithm used:\n\nSome datasets, like the Iris dataset, are naturally well-scaled, and most algorithms perform consistently well without any scaling.\nFeature scaling, particularly min-max and maximum absolute scaling, has a positive impact on algorithms in datasets like Breast Cancer and Digits, resulting in improved accuracy.\nThe Wine dataset demonstrated that certain algorithms, notably SVM, are highly sensitive to feature scaling. Without proper scaling, SVM’s performance can be significantly compromised.\nSurprisingly, some algorithms, such as Naive Bayes and Logistic Regression, performed well without any scaling in the Breast Cancer dataset, indicating that the original feature values were suitable for these models.\n\nIn practice, it’s essential to consider the characteristics of the dataset and the algorithm’s sensitivity to feature values when deciding whether to apply feature scaling. While scaling can improve the performance of many machine learning algorithms, there are cases where it may not be necessary and could even have a negligible or detrimental effect on model accuracy."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "href": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling",
    "text": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling\nIn the context of our machine learning analysis, it’s fascinating to observe that Naive Bayes and tree-based algorithms, such as Decision Trees and Random Forests, exhibit remarkable resilience to feature scaling. This resilience stems from the inherent characteristics of these algorithms and their method of decision-making.\n\nNaive Bayes Classifier\nNaive Bayes is a probabilistic algorithm that’s based on the Bayes’ theorem. It operates under the “naive” assumption that features are conditionally independent, given the class label. This fundamental assumption simplifies the calculations and often leads to surprisingly good classification results, especially in text and categorical data analysis.\nThe reason why Naive Bayes remains largely unaffected by feature scaling is twofold:\n\nProbabilistic Nature: Naive Bayes calculates probabilities based on the distribution of features within each class. The relative scaling of individual features does not impact the probability ratios significantly. In other words, as long as the relationships between features and classes remain consistent, the algorithm can adapt to different feature scales.\nNormalization in Probability Calculation: When computing probabilities, Naive Bayes often involves normalizing terms. This means that even if feature values are on different scales, the normalization process effectively scales them down to a common scale during probability calculations.\n\n\n\nDecision Trees and Random Forests\nDecision Trees and their ensemble counterpart, Random Forests, are non-parametric algorithms that make decisions by recursively splitting data based on feature values. They are highly interpretable and capable of capturing complex relationships within the data.\nThe key reasons why Decision Trees and Random Forests are generally insensitive to feature scaling include:\n\nSplitting Criteria: Decision Trees make decisions based on feature values relative to certain thresholds. The order or magnitude of these thresholds doesn’t affect the decision-making process. The algorithm focuses on finding the most discriminative features and their optimal split points.\nEnsemble Nature (Random Forests): Random Forests combine multiple Decision Trees. The ensemble nature of Random Forests further reduces sensitivity to feature scaling. When individual trees make errors due to scaling, the ensemble tends to compensate for them.\nImpurity Measures: Decision Trees use impurity measures like Gini impurity and entropy to determine the quality of a split. These measures are based on class proportions within a split and are independent of feature scales."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About Our Project\nHey there, fellow learners! 🚀 We’re a bunch of ML enthusiasts on a mission to make your learning journey more exciting and practical.\n🔍 What We Do We’re all about breaking down those complex ML models and datasets, helping you understand the ‘whys’ and ‘hows’ in a fun way.\n📚 Why We’re Here Traditional courses teach the theory, but we’re here to fill in the gaps, one Colab notebook at a time.\n🎁 What You Get Explore our user-friendly notebooks, detailed reports, and a platform designed just for you.\nJoin us in unraveling the magic of machine learning, one experiment at a time. Happy learning! 🤖📈"
  },
  {
    "objectID": "pages/demo.html#why",
    "href": "pages/demo.html#why",
    "title": "ML Handbook",
    "section": "Why?",
    "text": "Why?\n\nDecision trees make the following presumption about the structure of data:\n\n\n\nCan figure class out based on a series of binary questions (yes/no) on individual features\n\n\n\n\nInductive Bias:  Anything which makes an algorithm learn one pattern over another\n\n\n\n\nInductive bias of decision trees entails the use of axis-parallel splits to construct the decision boundary\nSensitive to rotations\nAlgorithm invariant to rotation?"
  },
  {
    "objectID": "pages/Inductive_Bias.html",
    "href": "pages/Inductive_Bias.html",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "",
    "text": "Slides: Click here!"
  },
  {
    "objectID": "pages/Inductive_Bias.html#inductive-bias",
    "href": "pages/Inductive_Bias.html#inductive-bias",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "Inductive bias",
    "text": "Inductive bias\n\nAnything which makes the algorithm learn one pattern instead of another pattern.\n\nDecision trees use a step-function collection for classification; but these step functions utilize one feature/variable only. Is this phenomenon sensitive to the nature of the dataset?\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\n\nDTree = DecisionTreeClassifier()\nDTree.fit(X, y)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax = ax1)\ndisp.ax_.scatter(X[:, 0][y==0], X[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Tree Depth: {DTree.get_depth()}');\n\nplot_tree(DTree, label='none', filled=True, feature_names=['F1', 'F2'], class_names=['Red', 'Blue'], node_ids=False, rounded=True, impurity=False, ax=ax2);\n\n\n\n\nThe 4x4 checkerboard dataset with alternating classes requires a tree of depth=7 to capture its structure respectively.\nBut what will happen if we try to train a tree on the rotated variant of this dataset?\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nDTree_rotated = DecisionTreeClassifier()\nDTree_rotated.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Overfit) Tree Depth: {DTree_rotated.get_depth()}')\n\nDTree_rotated_constrained = DecisionTreeClassifier(max_depth=7)\nDTree_rotated_constrained.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated_constrained, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax2)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Constrained) Tree Depth: {DTree_rotated_constrained.get_depth()}');\n\n\n\n\n\nOh!\nThe model fails to understand the generation rationale of the dataset as it suffers an inductive bias of axis-parallel splitting."
  },
  {
    "objectID": "pages/Inductive_Bias.html#being-mindful-of-the-bias",
    "href": "pages/Inductive_Bias.html#being-mindful-of-the-bias",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "Being Mindful of the Bias",
    "text": "Being Mindful of the Bias\nThe above dataset has a seperator corresponding to a second order function of the features.\nTransform the dataset and apply perceptron! Alter inductive bias to our advantage\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Perceptron\n\npoly_perceptron = make_pipeline(PolynomialFeatures(2), Perceptron(alpha=0, max_iter=int(1e6), tol=None))\npoly_perceptron.fit(X, y)\n\nfig, (ax1) = plt.subplots(1, 1)\ndisp = DecisionBoundaryDisplay.from_estimator(poly_perceptron, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X[:, 0][y==-1], X[:, 1][y==-1], color='red', label='y==-1', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Poly-Perceptron | Score: {poly_perceptron.score(X, y)}');"
  },
  {
    "objectID": "pages/Deliverable_1.html",
    "href": "pages/Deliverable_1.html",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#introduction",
    "href": "pages/Deliverable_1.html#introduction",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#bias",
    "href": "pages/Deliverable_1.html#bias",
    "title": "Dealing with missing-values",
    "section": "Bias",
    "text": "Bias\nWhen data is missing, the remaining data may not always accurately reflect the true population. The type of missingness and the way we deal with it can dictate bias in our results.\nFor example, in a survey recording income of individuals, those with low income may choose not to respond. The use of average income to impute the missing values can lead to bias as the average of the available data may not represent that of the population."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "href": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "title": "Dealing with missing-values",
    "section": "Missing Completely At Random (MCAR)",
    "text": "Missing Completely At Random (MCAR)\nIn this case, the data is missing randomly and is not related to any variable in the dataset or to the missing value themselves. The probability of missingness is same for all observations. There exists no underlying pattern to the missingness. The missing values are completely independent of other data.\nFor example, during data collection, if some responses were not collected due to a technical error, then the missing data is completely at random.\nAll statistical analysis performed on the dataset will remain unbiased in this case."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-at-random-mar",
    "href": "pages/Deliverable_1.html#missing-at-random-mar",
    "title": "Dealing with missing-values",
    "section": "Missing At Random (MAR)",
    "text": "Missing At Random (MAR)\nIn this case, the missingness of the data can be fully accounted for by the other known data values. Here there exists some form of pattern in the missing values.\nFor example, In a survey, women might be unwilling to disclose their age. Here the missingness of the variable age can be explained by another observed variable “gender”."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "href": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "title": "Dealing with missing-values",
    "section": "Missing Not At Random (MNAR)",
    "text": "Missing Not At Random (MNAR)\nIn this case, the missingness is neither MCAR nor MAR. The fact that a datapoint is missing is dependent on the value of the data point. In order to correct for the bias we would have to make modelling assumptions about the nature of the bias.\nFor example, in a social survey where individuals are asked about their income, respondnets may not disclose it if it is too high or too low. Thus the missingness in the feature income is directly related to the values of that feature."
  },
  {
    "objectID": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "href": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "title": "Dealing with missing-values",
    "section": "Identifying the type of missingness",
    "text": "Identifying the type of missingness\n\nUnderstanding the data collection process, the features involved and the research domain can help identify possible patterns as to why data is missing\nGraphical representation of the missing data and heatmaps can help identify relationships between features that can be utilized to make better imputation of the missing data\nWe can impute the data using different techniques while also making assumptions on the nature of missingness. Subsequently, we analyze the results to observe the assumptions that have led to consistent results."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-value-imputation",
    "href": "pages/Deliverable_1.html#missing-value-imputation",
    "title": "Dealing with missing-values",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\nIn many cases it might not be feasible to discard observations that contain missing values. This could be due to the availability of lesser number of samples or due to the importance of each observation. In such cases we can employ imputation which deals with replacing the missing values with some predicted value. We will have a look at two simple imputation techniques that can be implemented using the sklearn package.\n\nSimple Imputer\nK Nearest neighbours Imputer"
  },
  {
    "objectID": "pages/Deliverable_1.html#simple-imputer",
    "href": "pages/Deliverable_1.html#simple-imputer",
    "title": "Dealing with missing-values",
    "section": "Simple Imputer",
    "text": "Simple Imputer\nThe Simple Imputation technique offers a basic approach to filling missing data wherein we replace the missing data with a constant value or utilize statistics such as “mean”, “mode”, or “median” of the available values. The technique is useful for its simplicity and can serve as a reference technique. It is also worth noting that this can lead to biased or unrealistic results\n\nSimple Imputer - Mean\n\nimport numpy as np\nimport pandas as pd\n\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mean strategy. The mean value of that feature will be \\frac{( 1+1+2+4)}{4} = 2\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 2 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\ndata = {\"feature_1\":[1,2,3,4,5],\n        \"feature_2\":[1,1,np.nan,2,4],\n        \"feature_3\":[3,3,3,4,5]}\n\n\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1\n1.0\n3\n\n\n1\n2\n1.0\n3\n\n\n2\n3\nNaN\n3\n\n\n3\n4\n2.0\n4\n\n\n4\n5\n4.0\n5\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom sklearn.impute import SimpleImputer\n\n\nimputer = SimpleImputer(strategy='mean')\nimputed_data = imputer.fit_transform(df)\n\n\nimputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nimputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n2.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSimple Imputer - Mode\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mode strategy. The value that appears most often is 1.\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nmode_imputer = SimpleImputer(strategy='most_frequent')\nimputed_data = mode_imputer.fit_transform(df)\n\n\nmode_imputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nmode_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/Deliverable_1.html#k-nearest-neighbours",
    "href": "pages/Deliverable_1.html#k-nearest-neighbours",
    "title": "Dealing with missing-values",
    "section": "K Nearest Neighbours",
    "text": "K Nearest Neighbours\nThis technique is an extension of the KNN classifier we have seen in MLT to perform imputation. In this technique we identify the points that are similar to the observation we wish to impute based on the available features. We can then use the values of these neighboring points fill in the missing values\n\nfrom sklearn.impute import KNNImputer\n\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nknn_imputed_data = knn_imputer.fit_transform(df)\n\n\nknn_imputed_df = pd.DataFrame(knn_imputed_data, columns=['feature_1','feature_2','feature_3'])\nknn_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.5\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/EM_Algorithm.html",
    "href": "pages/EM_Algorithm.html",
    "title": "EM Algorithm",
    "section": "",
    "text": "Colab: Click here!"
  },
  {
    "objectID": "pages/EM_Algorithm.html#introduction-of-latent-variable",
    "href": "pages/EM_Algorithm.html#introduction-of-latent-variable",
    "title": "EM Algorithm",
    "section": "Introduction of Latent Variable",
    "text": "Introduction of Latent Variable\nWe introduce a K-dimensional latent variable \\mathbf{z}; only one of the elements of \\mathbf{z} will be 1 (1-of-K encoding) (also a standard basis vector).\n\\mathbf{z} = \\mathbf{e}_k\n\nReformulating problem in terms of latent variable \\mathbf{z}\nWe model the marginal distribuon over \\mathbf{z} using our mixing coefficients \\pi_k:\np(\\mathbf{z}=\\mathbf{e}_k) = \\pi_k\nThe conditional distribution of \\mathbf{x} over \\mathbf{z} can be similarly modelled:\np(\\mathbf{x} | \\mathbf{z}=\\mathbf{e}_k) = \\mathcal{N}(\\mathbf{x}|\\mu_k, Σ_k)\nWe then obtain p(\\mathbf{x}) by summing the joint distribution over all possible \\mathbf{z} states:\np(x) = \\sum_\\mathbf{z}p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) = \\sum_{k=1}^K\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Σ_k)\nAnd done! We have formulated our goal using this latent variable \\mathbf{z}. One more quantity of use is the conditional probablity of \\mathbf{z}=\\mathbf{e}_k given \\mathbf{x}_i (which we will call \\lambda_ik):\n\\lambda_{ik} \\equiv p(\\mathbf{z}=\\mathbf{e}_k) = \\frac{p(\\mathbf{z}=\\mathbf{e}_k)p(\\mathbf{x}|\\mathbf{z}_k=\\mathbf{e}_k)}{\\sum_{j=1}^K p(\\mathbf{z}=\\mathbf{e}_j)p(\\mathbf{x}|\\mathbf{z}_j=\\mathbf{e}_j)} = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Σ_k)}{\\sum_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j, Σ_j)}\nBy the reformulation above, we have enabled the application of EM on our problem.\n\n\nLet’s begin!\nTo get started, we perform the following initialization: - Pick 2 points at random as cluster centres - Hard-assign points based on proximity to centres\n\nK = 2\n\nrng = np.random.default_rng(seed=12)\ncentres = X[rng.integers(low=0, high=len(X), size=K)]\nl = np.array([(lambda i: [int(i==j) for j in range(K)])(np.argmin([np.linalg.norm(p-centre) for centre in centres])) for p in X])\npi = l.sum(axis=0)/l.sum()\n\ncov = []\nfor i in range(K):\n  cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\ncov = np.array(cov)\n\ndef plot():\n\n  plt.scatter(X[:, 0], X[:, 1], color='black', marker='+')\n  plt.scatter(centres[:, 0], centres[:, 1], color='red', marker='x')\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  plt.contour(M, N, probability_grid)\n  plt.show()\n\nplot()"
  },
  {
    "objectID": "pages/EM_Algorithm.html#posing-a-maximum-likelihood-problem",
    "href": "pages/EM_Algorithm.html#posing-a-maximum-likelihood-problem",
    "title": "EM Algorithm",
    "section": "Posing a Maximum Likelihood problem:",
    "text": "Posing a Maximum Likelihood problem:\nThe log of the likelihood is the following:\n\\ln p(\\mathbf{X}|\\pi, \\mu, Σ) = \\sum_{n=1}^N\\ln \\left\\{{\\sum_{k=1}^K\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mu_k, \\Sigma_k)}\\right\\}\n\nMaximization step\nTaking derivate of above equation with \\mu_k and setting to zero yields the following:\n\n0 = -\\sum_{n=1}^N \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^K\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mu_j, \\Sigma_j)} Σ_k(\\mathbf{x}_n-\\mu_k) ≡ -\\sum_{n=1}^N \\lambda_{nk}Σ_k(\\mathbf{x}_n-\\mu_k)\n\nMultiplying both sides by Σ_k^{-1} (non-singular; invertible) and rearranging, we get:\n\\mathbf{\\mu}_k = \\frac{1}{N_k}\\sum_{n=1}^{N}\\lambda_{nk}\\mathbf{x}_n\nWhere we define N_k to be the effective number of points assigned to cluster k: N_k = \\sum_{n=1}^N \\lambda_{nk}\nWe follow a similar approach and derive the following ML estimates for \\Sigma_k and \\pi_k:\nΣ_k = \\frac{1}{N_k}\\sum_{n=1}^N\\lambda_{nk}(\\mathbf{x}_n-\\mathbf{\\mu}_k)(\\mathbf{x}_n-\\mathbf{\\mu}_k)^T\n\\\\\\pi_k = \\frac{N_k}{N}\n\n\n\nExpectation step\nRecompute \\lambda using the parameter values. Forumula mentioned again for completion sake:\n\\lambda_{ik} = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Σ_k)}{\\sum_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j, Σ_j)}\n\n# Expectation Step\ndef Max(l):\n  pi = l.sum(axis=0)/l.sum()\n  centres, cov = [], []\n  for i in range(K):\n    centres.append(np.dot(l[:, i], X)/l[:, i].sum())\n    cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\n\n  return pi, np.array(centres), np.array(cov)\n\n# Maximization step\ndef Exp(pi, centres, cov):\n  l = []\n  for i in X:\n    p = np.array([pi[k] * multivariate_normal.pdf(i, mean=centres[k], cov=cov[k]) for k in range(K)])\n    p = p/p.sum()\n    l.append(p)\n\n  return np.array(l)\n\n# Convergence criterion\nnorm_theta = lambda pi, centres, cov: np.linalg.norm(np.r_[pi, centres.reshape(-1), cov.reshape(-1)])\n\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\nprev_norm = norm_theta(pi, centres, cov)\n\nfig, ax = plt.subplots()\nartists = []\n\nwhile True:\n  frame = []\n  frame.append(ax.scatter(X[:, 0], X[:, 1], color='black', marker='+'))\n  frame.append(ax.scatter(centres[:, 0], centres[:, 1], color='red', marker='x'))\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  frame += list(ax.contour(M, N, probability_grid).collections)\n  artists.append(frame)\n\n  l = Exp(pi, centres, cov)\n  pi, centres, cov = Max(l)\n\n  curr_norm = norm_theta(pi, centres, cov)\n\n  if abs(curr_norm-prev_norm) &lt; 0.00001:\n    break\n  else:\n    prev_norm = curr_norm\n\nplt.close()\nanim = animation.ArtistAnimation(fig, artists, interval=200, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nRelation to K-Means?\nThere is a very close similarity. In fact K-Means is a restricted GMM clustering (initalize all Σ_k’s to ϵ\\mathbf{I}, with ϵ → 0, and do not update in maximization step)\nHow does the above make it K-Means? We investigate \\lambda_{ik}:\n\\lambda_{ik} = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Σ_k)}{\\sum_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j, Σ_j)} = \\frac{\\pi_k\\exp \\{-||\\mathbf{x}_i-\\mathbf{\\mu}_k||^2/2ϵ\\}}{\\sum_{j=1}^K \\pi_j\\exp \\{-||\\mathbf{x}_i-\\mathbf{\\mu}_j||^2/2ϵ\\}}\nLet \\phi = \\arg \\min f(j) = ||\\mathbf{x}_i-\\mathbf{\\mu}_j||^2. Setting ϵ → 0, we see that in the denominator the Φ’th term goes to zero the slowest. Hence \\lambda_{n\\phi} → 1 while the others → 0 (note that this results in a hard-clustering).\nThe above is equivalent to the K-means clustering paradigm; assign clusters based on proximity from cluster centers.\n\n# (Restricted) Maximization Step\ndef KM_Max(l):\n  pi = l.sum(axis=0)/l.sum()\n  centres, cov = [], []\n  for i in range(K):\n    centres.append(np.dot(l[:, i], X)/l[:, i].sum())\n    cov.append(eps*np.eye(l.shape[1]))\n\n  return pi, np.array(centres), np.array(cov)\n\nK = 2\neps = 0.005\n\nrng = np.random.default_rng(seed=72)\ncentres = X[rng.integers(low=0, high=len(X), size=K)]\nl = np.array([(lambda i: [int(i==j) for j in range(K)])(np.argmin([np.linalg.norm(p-centre) for centre in centres])) for p in X])\npi = l.sum(axis=0)/l.sum()\n\ncov = []\nfor i in range(K):\n  cov.append(eps*np.eye(l.shape[1]))\ncov = np.array(cov)\n\ndef plot():\n\n  plt.scatter(X[:, 0], X[:, 1], color='black', marker='+')\n  plt.scatter(centres[:, 0], centres[:, 1], color='red', marker='x')\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  plt.contour(M, N, probability_grid)\n  plt.show()\n\nplot()\n\n\n\n\n\na, b, c = KM_Max(l)\n[[round(i, 5) for i in j] for j in Exp(a, b, c)[:5]]\n\n[[0.99911, 0.00089], [0.0, 1.0], [0.00082, 0.99918], [0.0, 1.0], [1.0, 0.0]]\n\n\nWe see that the assignments are close to hard-clustering. Setting epsilon to a much smaller value will ensure this better.\nFor visual presentability, epsilon has been set as small as possible.\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\nprev_norm = norm_theta(pi, centres, cov)\n\nfig, ax = plt.subplots()\nartists = []\n\nwhile True:\n\n  frame = []\n  frame.append(ax.scatter(X[:, 0], X[:, 1], color='black', marker='+'))\n  frame.append(ax.scatter(centres[:, 0], centres[:, 1], color='red', marker='x'))\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  frame += list(ax.contour(M, N, probability_grid).collections)\n  artists.append(frame)\n\n  l = Exp(pi, centres, cov)\n  pi, centres, cov = KM_Max(l)\n\n  curr_norm = norm_theta(pi, centres, cov)\n\n  if abs(curr_norm-prev_norm) &lt; 0.0001:\n    print(curr_norm-prev_norm)\n    break\n  else:\n    prev_norm = curr_norm\n\nplt.close()\nanim = animation.ArtistAnimation(fig, artists, interval=1000, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n-7.535850650119968e-05\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "pages/Probabilistic_PCA.html",
    "href": "pages/Probabilistic_PCA.html",
    "title": "Probabilistic PCA",
    "section": "",
    "text": "Colab Link: Click here!\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom scipy.stats import norm\nimport seaborn as sns\n\nrng = np.random.default_rng(seed=12)"
  },
  {
    "objectID": "pages/Probabilistic_PCA.html#generative-modelling",
    "href": "pages/Probabilistic_PCA.html#generative-modelling",
    "title": "Probabilistic PCA",
    "section": "Generative Modelling",
    "text": "Generative Modelling\nIn generative modelling, we try to induce a latent variable \\mathbf{Z} of a lower dimension, which will help represent high-dimensional variable \\mathbf{X}.\nIn the context of PCA, \\mathbf{Z} will represent the mappings of \\mathbf{X} on a lower dimensional subspace, which corresponds to the principal subspace.\nSo how do we introduce \\mathbf{Z}?\n\nA generative story\nLet L and D be the dimension of the latent space \\mathbf{Z} and data space \\mathbf{X} respectively.\nThe following steps are followed to generate a new sample \\mathbf{x}:\n\nSample \\mathbf{z} from the prior defined as p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}|0, \\mathbf{I})\nFurther, sample \\mathbf{x} using \\mathbf{z} from the conditional distribution p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{W}\\mathbf{z}+\\mathbf{\\mu}, σ^2\\mathbf{I})\n\nWe require \\mathbf{W} to span a linear subspace, corresponding to the principal subspace of \\mathbf{X}.\nWe illustrate this below for the simple case where D=2, L=1. We obtain parameter values for the distribution from PCA performed above in the notebook. We see that the dataset can effectively be regenerated after estimating parameters of our model.\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\nimport warnings \n\nwarnings.filterwarnings(\"ignore\")\n\n\nM, N = np.mgrid[-6:10:0.1, -6:10:0.1]\ngrid = np.dstack((M, N))\n\nx = np.linspace(-4, 4)\nz = rng.standard_normal()\nw = pc_vec/np.sqrt(pc_vec.T@pc_vec)\nn, d = X.shape\n\nmu = X.mean(axis=0)\ns2 = eig_val[0]\nX_gen = []\n\ntheta = np.linspace(0, 2*np.pi, 1000)\ncircle = np.c_[np.cos(theta), np.sin(theta)]\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 100/36))\nnorm_pdf = ax1.plot(x, norm.pdf(x, 0, 1))\nax2_line = ax2.plot(x_range + X_mean[0], x_range * pc_vec[1] + X_mean[1], color='black')\n\n# zeroth frame - pdf, line graph\ndefault = norm_pdf + ax2_line\nartists = [list(default)]\n\nfor _ in range(15):\n  # sampling x\n  x_mean = np.expand_dims(z*w+mu, -1).T\n  x = rng.multivariate_normal(z*w+mu, cov=s2*np.eye(2))\n\n  frame = list(default)\n\n  # first frame - sample z\n  z = rng.standard_normal()\n  frame.append(ax1.scatter(z, 0, marker='x', color='red'))\n\n  artists.append(list(frame))\n\n  # second frame - visualize p(x|z)\n  frame += ax2.plot((x_mean + 0.5*s2*circle)[:, 0], (x_mean + 0.5*s2*circle)[:, 1], color='red')\n  frame += ax2.plot((x_mean + 1*s2*circle)[:, 0], (x_mean + 1*s2*circle)[:, 1], color='red')\n  frame += ax2.plot((x_mean + 1.5*s2*circle)[:, 0], (x_mean + 1.5*s2*circle)[:, 1], color='red')\n\n  artists.append(list(frame))\n\n  # third frame - sample x, show collection\n  frame.append(ax2.scatter(x[0], x[1], color='orange', marker='x'))\n  new_point_plot = ax3.scatter(x[0], x[1], color='orange', s=10)\n  frame.append(new_point_plot)\n  artists.append(list(frame))\n\n  # preserve new point by incrementing default collection\n  default.append(new_point_plot)\n\nelse:\n  frame = list(default)\n  probability_grid = multivariate_normal(X.mean(axis=0), w@w.T + s2*np.eye(d)).pdf(grid)\n  frame += ax2.contour(M, N, probability_grid).collections\n  frame += ax3.contour(M, N, probability_grid).collections\n\nartists.append(list(frame))\n\nplt.close()\nax3.sharex(ax2);\nax3.sharey(ax2);\nax2.axis('scaled');ax3.axis('scaled');\n\nax1.title.set_text('Sampling z from latent space')\nax2.title.set_text('Sampling x from data space')\nax3.title.set_text('Collection of sampled points')\n\nanim = animation.ArtistAnimation(fig, artists, interval=600, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nIn the final frame of the above animation, we see that the genrative story effectively reduces to the sampling of \\mathbf{X} from the following distribution: \n\\begin{alignat}{2}\n&& p(\\mathbf{x}) &= \\int{p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z}) d\\mathbf{z}} \\\\\n&& &= \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}, \\mathbf{W}\\mathbf{W}^T + σ^2\\mathbf{I})\n\\end{alignat}\n\nThe above derivation is easy by noting that p(\\mathbf{x}) is gaussian. So one can just find the mean and covariance:\n\n\\begin{alignat}{2}\n&& \\mathbb{E}[\\mathbf{x}] &= \\mathbb{E}[\\mathbf{Wz + \\mu + ϵ}] = \\mathbf{\\mu} \\\\\n&& \\mathrm{cov}[\\mathbf{x}] &= \\mathbb{E}[(\\mathbf{Wz + ϵ})(\\mathbf{Wz + ϵ})^T] \\\\\n&& &= \\mathbb{E}[\\mathbf{Wzz^TW^T}] + \\mathbb{E}[\\mathbf{ϵϵ^T}] = \\mathbf{WW^T} + σ^2\\mathbf{I}\n\\end{alignat}"
  },
  {
    "objectID": "pages/Probabilistic_PCA.html#e-step",
    "href": "pages/Probabilistic_PCA.html#e-step",
    "title": "Probabilistic PCA",
    "section": "E-step",
    "text": "E-step\nThe E-step estimates the conditional distribution of the hidden variables \\mathbf{Z} given the data \\mathbf{X} and the current values of the model parameters \\mathbf{W, σ^2} (note that ML estimate of \\mathbf{\\mu} is simply the sample mean \\mathbf{\\overline{X}}, so it can simly be mean-centered in the preprocessing step)\n\n\\begin{alignat}{2}\nΣ_\\mathbf{Z} &= σ^2(σ^2\\mathbf{I}+\\mathbf{W^TW})^{-1}, \\\\\n\\mathbf{Z} &= \\frac{1}{σ^2}Σ_\\mathbf{Z}\\mathbf{W}^T\\mathbf{Y}\n\\end{alignat}"
  },
  {
    "objectID": "pages/Probabilistic_PCA.html#m-step",
    "href": "pages/Probabilistic_PCA.html#m-step",
    "title": "Probabilistic PCA",
    "section": "M-step",
    "text": "M-step\nThe M-Step re-estimates the model paramters as\n\\begin{aligned}\n\\mathbf{W} & =\\mathbf{Y} {\\mathbf{Z}}^{\\mathrm{T}}\\left({\\mathbf{Z Z}}^{\\mathrm{T}}+n \\Sigma_{\\mathbf{Z}}\\right)^{-1}, \\\\\nσ^2 & =\\frac{1}{ND} \\sum_{i=1}^d \\sum_{j=1}^n\\left(y_{i j}-\\mathbf{w}_i^{\\mathrm{T}} {\\mathbf{z}}_j\\right)^2+\\frac{1}{d} \\operatorname{tr}\\left(\\mathbf{W} \\Sigma_{\\mathbf{Z}} \\mathbf{W}^{\\mathrm{T}}\\right) .\n\\end{aligned}\n\n# Initialization\nX_ = (X-X.mean(axis=0)).T\nW = np.array([[1, -1]]).T\ns2 = 0.1\n\nartists = []\nfig, ax = plt.subplots(dpi=150)\nax.sharex(ax2)\nax.sharey(ax2)\n\ndefault = [ax.scatter(X[:, 0], X[:, 1], s=10)]\n\nfor _ in range(7):\n  frame = list(default)\n  pc = W.T[0]/W.T[0][0]\n  probability_grid = multivariate_normal(X.mean(axis=0), W@W.T + s2*np.eye(d)).pdf(grid)\n\n  frame += ax.contour(M, N, probability_grid).collections\n  frame +=  ax.plot(x_range + X_mean[0], x_range * pc[1] + X_mean[1], color='black')\n\n  # E-step\n  cov_Z = s2*np.linalg.inv(s2*np.eye(W.shape[1]) + W.T@W)\n  Z = cov_Z*W.T@X_ / s2\n\n  # M-step\n  W = X_@Z.T@np.linalg.inv(Z@Z.T + X_.shape[1]*cov_Z)\n  s2 = ((X_-W@Z)**2).sum()/(n*d) + np.trace(W@cov_Z@W.T)/d\n  artists.append(frame)\n\nplt.close()\nanim = animation.ArtistAnimation(fig, artists, interval=500, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nIn the above animation, the updates made to p(\\mathbf{x}) after each M-step is shown."
  },
  {
    "objectID": "pages/roc.html",
    "href": "pages/roc.html",
    "title": "Exploring the Significance of ROC AUC in Classification Models",
    "section": "",
    "text": "Within the realm of assessing the performance of classification models, the Receiver Operating Characteristic (ROC) curve and its corresponding Area Under the Curve (AUC) metric stand as pivotal tools. This exploration seeks to expound upon their practical significance by employing diverse datasets, offering a nuanced understanding of these fundamental concepts in model evaluation."
  },
  {
    "objectID": "pages/roc.html#understanding-roc-auc-a-brief-overview",
    "href": "pages/roc.html#understanding-roc-auc-a-brief-overview",
    "title": "Exploring the Significance of ROC AUC in Classification Models",
    "section": "Understanding ROC AUC: A Brief Overview",
    "text": "Understanding ROC AUC: A Brief Overview\nA receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values.\n\nDid you know? The Receiver Operating Characteristic (ROC) curve originated from radar engineering during World War II. It’s named for the device that detects signals from enemy objects (the “receiver”) and its ability to distinguish between true signals and background noise (the “operating characteristic”). This curve illustrates the trade-off between true positive and false positive rates at various threshold settings. Its purpose was to assess receiver performance and optimize its settings. For more details, check out the Wikipedia article on the ROC curve or this Cross Validated question on its history.You can read more about the history of the ROC curve from this Wikipedia article or this Cross Validated question.\n\nThe ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting.\nThe ROC can also be thought of as a plot of the statistical power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of false positive rate.\nThe following confusion matrix illustrates the relationship between the true positives and false positives:\n\n\nROC Curve\n\nThe ROC curve displays how well a model separates true positives from false positives at different classification thresholds.\nTrue Positive Rate (TPR) measures how often the model correctly identifies positives.\nFalse Positive Rate (FPR) calculates how often the model wrongly flags negatives.\n\n \\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}} \n \\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives + True Negatives}} \n\n\nAUC\n\nThe Area Under the Curve (AUC) summarizes the ROC curve’s performance.\nAUC values range from 0 to 1, where higher values imply better discrimination ability.\nAUC = 1 signifies a perfect classifier, while AUC = 0.5 suggests random guessing.\n\n \\text{AUC} = \\int_{x=0}^{1} \\text{TPR}(\\text{FPR}^-(x)) \\, d(\\text{x}) \nThe ROC curve shows how well a model distinguishes between classes, while AUC quantifies this performance, aiding in understanding a model’s effectiveness.\n\n\nExample Calculation of ROC Curve and AUC\nGiven Information\nActual labels: [1, 0, 1, 1, 0]\nPredicted probabilities: [0.8, 0.6, 0.7, 0.4, 0.2]\nNow, let’s sort the predicted probabilities and align them with their corresponding actual labels:\n\n\n\nPredicted Probability\nActual Label\n\n\n\n\n0.8\n1\n\n\n0.7\n1\n\n\n0.6\n0\n\n\n0.4\n1\n\n\n0.2\n0\n\n\n\nCalculating TPR and FPR for Different Thresholds\n\n\n\n\n\n\n\n\n\n\nThreshold\nTrue Positives\nFalse Positives\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n\n\n\n\n0.8\n1\n0\n1/3 (0.33)\n0/2 (0)\n\n\n0.7\n2\n0\n2/3 (0.66)\n0/2 (0)\n\n\n0.6\n2\n1\n2/3 (0.66)\n1/2 (0.5)\n\n\n0.4\n3\n1\n3/3 (1)\n1/2 (0.5)\n\n\n0.2\n3\n2\n3/3 (1)\n2/2 (1)\n\n\n\nPlotting the ROC Curve\n\n\n\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n\n\n\n\n1/3 (0.33)\n0/2 (0)\n\n\n2/3 (0.66)\n0/2 (0)\n\n\n2/3 (0.66)\n1/2 (0.5)\n\n\n3/3 (1)\n1/2 (0.5)\n\n\n3/3 (1)\n2/2 (1)\n\n\n\nCalculating Area Under the ROC Curve (AUC)\nTo compute the AUC using the trapezoidal rule:\n\nCalculate the area of the trapezoids formed by adjacent points:\n\nTrapezoid 1: [(0, 0), (0, 0.67), (0.5, 0), (0.5, 0.67)] with base = 0.5 and height = 0.67.\nArea = base * height = 0.5 * 0.67 = 0.33\nTrapezoid 2: [(0.5, 0), (0.5, 1), (1, 0), (1, 1)] with base = 0.5 and height = 1.\nArea = base * height = 0.5 * 1 = 0.5\n\nSum up the areas of the trapezoids:\nTotal AUC ≈ 0.33 + 0.5 ≈ 0.83\n\nConclusion: The calculated approximate AUC for the ROC curve using the trapezoidal rule is approximately 0.83.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# Actual labels and predicted probabilities\nactual_labels = [1, 0, 1, 1, 0]\npredicted_probabilities = [0.8, 0.6, 0.7, 0.4, 0.2]\n\n# Calculate fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(actual_labels, predicted_probabilities)\n\n# Calculate AUC\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='red', linestyle='--', lw=2, label='Random Guessing')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "pages/roc.html#analyzing-area-under-the-curve-auc-values-across-datasets-and-models",
    "href": "pages/roc.html#analyzing-area-under-the-curve-auc-values-across-datasets-and-models",
    "title": "Exploring the Significance of ROC AUC in Classification Models",
    "section": "Analyzing Area Under the Curve (AUC) Values Across Datasets and Models",
    "text": "Analyzing Area Under the Curve (AUC) Values Across Datasets and Models\n\nAUC Values: Understanding Model Performance\nLet’s delve into the AUC values obtained across diverse datasets using different classification models.\n\nCode for AUC Analysis\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom ucimlrepo import fetch_ucirepo\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# Suppress convergence warnings\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\ndef calculate_auc(dataset, dataset_name, ax):\n    # Split dataset into features and target\n    X = dataset[\"data\"][\"features\"]\n    y = dataset[\"data\"][\"targets\"]\n    targets = list(y[y.columns[0]].unique())\n    y = (y.replace({targets[0]: 0, targets[1]: 1})).to_numpy().reshape(-1)  # Adjust target labels if needed\n    \n    # Identify categorical columns for encoding\n    categorical_cols = [col for col in X.columns if X[col].dtype == 'object']\n    \n    # Split dataset into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Preprocessing: One-hot encode categorical columns\n    preprocessor = ColumnTransformer(\n        transformers=[('encoder', OneHotEncoder(), categorical_cols)],\n        remainder='passthrough'\n    )\n    \n    # Define classification models\n    models = [\n        ('Logistic Regression', LogisticRegression()),\n        ('Random Forest', RandomForestClassifier()),\n        ('Gradient Boosting', GradientBoostingClassifier()),\n        ('SVM', SVC(probability=True)),\n        ('KNN', KNeighborsClassifier(n_neighbors=5)),\n    ]\n    \n    # Calculate AUC for each model\n    auc_results = {}\n    for name, model in models:\n        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('scaling', StandardScaler(with_mean=False)), ('classifier', model)])\n        pipeline.fit(X_train, y_train)\n        y_prob = pipeline.predict_proba(X_test)[:, 1]\n        auc_value = roc_auc_score(y_test, y_prob)\n        auc_results[name] = auc_value\n        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n        roc_auc = auc(fpr, tpr)\n        ax.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n    \n    auc_results['Class_Ratio 1:0'] = len(y[y == 1]) / len(y[y == 0])\n\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title(f\"ROC AUC Curves for Different Models on {dataset_name} Dataset\")\n    ax.legend(loc=\"lower right\")\n\n    return auc_results\n\n\nhtru = pd.read_csv(\"data/HTRU_2.csv\", header=None)\n\nhtru_2 = {\n   \"data\": {\n      \"features\": htru.drop(8, axis=1),\n      \"targets\": htru[8].to_frame()\n   }\n}\n\ndatasets = [\n   (htru_2, \"htru_2\"),\n   (fetch_ucirepo(id=222), 'bank_marketing'),  # bank_marketing\n   (fetch_ucirepo(id=144), 'statlog_german_credit_data'),  # statlog_german_credit_data\n   (fetch_ucirepo(id=52), 'ionosphere'),  # ionosphere\n   (fetch_ucirepo(id=73), 'mushroom'),  # mushroom\n   (fetch_ucirepo(id=545), 'rice_cammeo_and_osmancik'),  # rice_cammeo_and_osmancik\n   (fetch_ucirepo(id=94), 'spambase'),  # spambase\n   (fetch_ucirepo(id=105), 'congressional_voting_records'),  # congressional_voting_records\n]\n\n\n# Create subplots in a 4x2 grid (4 rows, 2 columns)\nfig, axs = plt.subplots(4, 2, figsize=(12, 20))\naxs = axs.flatten()\n\nauc_results_dict = {}\nfor i, (dataset, dataset_name) in enumerate(datasets):\n    auc_results_dict[dataset_name] = calculate_auc(dataset, dataset_name, axs[i])\n\n# Hide empty subplots if there are fewer than 8 datasets\nfor i in range(len(datasets), len(axs)):\n    axs[i].axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Convert the dictionary to a DataFrame for easy visualization\nauc_results_df = pd.DataFrame(auc_results_dict).T\n\n# Ensure columns are of numeric data type (float)\nauc_results_df = auc_results_df.astype(float)\n\n# Sort columns by name (if needed)\nauc_results_df = auc_results_df.reindex(sorted(auc_results_df.columns), axis=1)\n\nauc_results_df\n\n\n\n\n\n\n\n\nClass_Ratio 1:0\nGradient Boosting\nKNN\nLogistic Regression\nRandom Forest\nSVM\n\n\n\n\nhtru_2\n0.100806\n0.973845\n0.964026\n0.974464\n0.970874\n0.971288\n\n\nbank_marketing\n0.132483\n0.921774\n0.822823\n0.904550\n0.924950\n0.917207\n\n\nstatlog_german_credit_data\n0.428571\n0.812237\n0.729475\n0.817887\n0.808090\n0.794687\n\n\nionosphere\n0.560000\n0.974252\n0.924834\n0.875415\n0.986296\n0.996678\n\n\nmushroom\n1.074566\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nrice_cammeo_and_osmancik\n1.337423\n0.979414\n0.959702\n0.981969\n0.977521\n0.982153\n\n\nspambase\n1.537783\n0.984449\n0.945009\n0.971433\n0.985448\n0.978449\n\n\ncongressional_voting_records\n1.589286\n0.987327\n0.931164\n0.979263\n0.994816\n0.993088\n\n\n\n\n\n\n\n\n\n\nInterpretation of AUC Values\n\nConsistently High AUC:\n\nMost models exhibit consistently high AUC values across various datasets.\nHowever, relying solely on AUC for model comparison might not distinguish between models effectively due to consistently high scores.\n\nChallenge of Class Imbalance:\n\nImbalanced datasets, highlighted in the ‘Class_Ratio 1:0’ column, pose challenges.\nModels with high AUC might struggle to accurately predict minority classes, impacting real-world applicability.\n\nBeyond AUC Evaluation:\n\nSupplementary metrics like precision, recall, and F1-score, along with techniques like precision-recall curves, become crucial, particularly in imbalanced datasets.\nConsideration of domain-specific knowledge guides model selection beyond AUC reliance.\n\nComprehensive Model Evaluation:\n\nEmphasis on a comprehensive evaluation strategy considering class imbalance impact and multiple relevant metrics beyond AUC for informed model selection and performance assessment."
  },
  {
    "objectID": "pages/roc.html#precision-recall-and-f1-score-complementary-metrics-to-roc-auc",
    "href": "pages/roc.html#precision-recall-and-f1-score-complementary-metrics-to-roc-auc",
    "title": "Exploring the Significance of ROC AUC in Classification Models",
    "section": "Precision, Recall, and F1-score: Complementary Metrics to ROC AUC",
    "text": "Precision, Recall, and F1-score: Complementary Metrics to ROC AUC\nIn addition to ROC AUC, evaluating classification models involves considering precision, recall, and the F1-score, providing nuanced insights into a model’s performance in specific scenarios.\n\nPrecision\nPrecision gauges the accuracy of positive predictions made by the model, measuring the ratio of true positives to the total predicted positives. Higher precision implies fewer false positives, making it crucial when minimizing false identifications is a priority.\n \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} \n\n\nRecall (Sensitivity)\nRecall evaluates the model’s ability to correctly identify all positive instances, calculating the ratio of true positives to the actual positives. Higher recall indicates capturing a larger proportion of actual positive instances, essential in scenarios where missing positives is a greater concern.\n \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} \n\n\nF1-score\nThe F1-score, a harmonic mean of precision and recall, offers a balanced assessment by considering false positives and false negatives. It becomes valuable when both precision and recall are equally important and provides a single metric summarizing their trade-offs.\n \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \nWhen to Prioritize Precision, Recall, and F1-score over ROC AUC:\n\nPrecision: When minimizing false positives is critical, e.g., medical diagnoses where avoiding misdiagnoses is crucial.\nRecall: In scenarios prioritizing capturing all positive instances, like identifying fraud in financial transactions.\nF1-score: When seeking a balance between precision and recall is essential, offering a consolidated metric.\n\nThese metrics serve as complementary tools to ROC AUC, offering specific insights into different aspects of a model’s performance, allowing for informed decision-making based on specific priorities and constraints in various real-world applications."
  },
  {
    "objectID": "pages/roc.html#conclusion",
    "href": "pages/roc.html#conclusion",
    "title": "Exploring the Significance of ROC AUC in Classification Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, the Receiver Operating Characteristic (ROC) curve and its corresponding Area Under the Curve (AUC) metric serve as pivotal elements in assessing the performance of classification models. These tools offer valuable insights into a model’s ability to distinguish between classes, aiding in informed decision-making across diverse domains.\nTheir significance transcends various applications, enabling stakeholders to make informed choices aligned with specific objectives and contextual needs. ROC AUC facilitates model comparison, guiding the selection of appropriate algorithms for distinct real-world scenarios. Ultimately, its practical significance lies in enhancing the understanding and evaluation of classification model performance, enabling better-informed decisions in various domains and applications."
  },
  {
    "objectID": "pages/cirrhosis.html",
    "href": "pages/cirrhosis.html",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "",
    "text": "Micrograph of PBC showing bile duct inflammation and injury, H&E stain\n\n\n\n\nCirrhosis stands as a severe consequence of liver diseases, where extensive scarring compromises liver function. The dataset at hand originates from the Mayo Clinic’s trial addressing primary biliary cirrhosis (PBC) during 1974-1984. This collection of data encompasses crucial insights gleaned from 424 PBC patients, marking a pivotal endeavor in understanding the clinical landscape of this condition.\nThe dataset, meticulously gathered as part of a randomized placebo-controlled trial involving D-penicillamine, presents a comprehensive array of covariates. Each entry comprises fundamental patient attributes, ranging from clinical measurements to key indicators of liver health.\n\n\n\nThe primary aim of this notebook is to harness machine learning methodologies to predict the status of PBC patients based on a diverse set of features. The ‘Status’ variable, delineated as C (censored), CL (censored due to liver tx), or D (death), serves as the focal point for predictive modeling. Leveraging the patient-specific attributes provided, the goal is to construct robust predictive models capable of discerning and forecasting patient outcomes within the scope of the study period.\n\n\n\n\nSize: The dataset contains information on 424 PBC patients, amalgamating both trial participants and supplementary cases with recorded measurements.\nFeatures: Comprising 19 attributes encompassing a spectrum of patient-related variables, including demographic data, clinical measurements, and indicators of liver health.\nContext: The features encapsulate critical facets of patient health, such as age, sex, presence of specific symptoms (ascites, hepatomegaly, spiders, edema), laboratory test results (bilirubin, cholesterol, albumin, copper, alkaline phosphatase, SGOT, triglycerides, platelets, prothrombin), and histologic staging of the disease.\n\nThroughout this notebook, we embark on a journey through data exploration, preprocessing, model selection, and evaluation, culminating in the development of predictive models aimed at discerning and forecasting the status of PBC patients.\nLet’s delve into the intricate realm of predictive modeling in the context of primary biliary cirrhosis, unraveling insights and patterns within this comprehensive dataset."
  },
  {
    "objectID": "pages/cirrhosis.html#introduction-and-dataset-overview",
    "href": "pages/cirrhosis.html#introduction-and-dataset-overview",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "",
    "text": "Micrograph of PBC showing bile duct inflammation and injury, H&E stain\n\n\n\n\nCirrhosis stands as a severe consequence of liver diseases, where extensive scarring compromises liver function. The dataset at hand originates from the Mayo Clinic’s trial addressing primary biliary cirrhosis (PBC) during 1974-1984. This collection of data encompasses crucial insights gleaned from 424 PBC patients, marking a pivotal endeavor in understanding the clinical landscape of this condition.\nThe dataset, meticulously gathered as part of a randomized placebo-controlled trial involving D-penicillamine, presents a comprehensive array of covariates. Each entry comprises fundamental patient attributes, ranging from clinical measurements to key indicators of liver health.\n\n\n\nThe primary aim of this notebook is to harness machine learning methodologies to predict the status of PBC patients based on a diverse set of features. The ‘Status’ variable, delineated as C (censored), CL (censored due to liver tx), or D (death), serves as the focal point for predictive modeling. Leveraging the patient-specific attributes provided, the goal is to construct robust predictive models capable of discerning and forecasting patient outcomes within the scope of the study period.\n\n\n\n\nSize: The dataset contains information on 424 PBC patients, amalgamating both trial participants and supplementary cases with recorded measurements.\nFeatures: Comprising 19 attributes encompassing a spectrum of patient-related variables, including demographic data, clinical measurements, and indicators of liver health.\nContext: The features encapsulate critical facets of patient health, such as age, sex, presence of specific symptoms (ascites, hepatomegaly, spiders, edema), laboratory test results (bilirubin, cholesterol, albumin, copper, alkaline phosphatase, SGOT, triglycerides, platelets, prothrombin), and histologic staging of the disease.\n\nThroughout this notebook, we embark on a journey through data exploration, preprocessing, model selection, and evaluation, culminating in the development of predictive models aimed at discerning and forecasting the status of PBC patients.\nLet’s delve into the intricate realm of predictive modeling in the context of primary biliary cirrhosis, unraveling insights and patterns within this comprehensive dataset."
  },
  {
    "objectID": "pages/cirrhosis.html#data-exploration",
    "href": "pages/cirrhosis.html#data-exploration",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nDataset Snapshot\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv('datasets/dataset.csv', index_col='ID')\ntest_df = pd.read_csv('datasets/test.csv', index_col='id')\ndata.head()\n\n\n\n\n\n\n\n\nN_Days\nStatus\nDrug\nAge\nSex\nAscites\nHepatomegaly\nSpiders\nEdema\nBilirubin\nCholesterol\nAlbumin\nCopper\nAlk_Phos\nSGOT\nTryglicerides\nPlatelets\nProthrombin\nStage\n\n\nID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n400\nD\nD-penicillamine\n21464\nF\nY\nY\nY\nY\n14.5\n261.0\n2.60\n156.0\n1718.0\n137.95\n172.0\n190.0\n12.2\n4.0\n\n\n2\n4500\nC\nD-penicillamine\n20617\nF\nN\nY\nY\nN\n1.1\n302.0\n4.14\n54.0\n7394.8\n113.52\n88.0\n221.0\n10.6\n3.0\n\n\n3\n1012\nD\nD-penicillamine\n25594\nM\nN\nN\nN\nS\n1.4\n176.0\n3.48\n210.0\n516.0\n96.10\n55.0\n151.0\n12.0\n4.0\n\n\n4\n1925\nD\nD-penicillamine\n19994\nF\nN\nY\nY\nS\n1.8\n244.0\n2.54\n64.0\n6121.8\n60.63\n92.0\n183.0\n10.3\n4.0\n\n\n5\n1504\nCL\nPlacebo\n13918\nF\nN\nY\nY\nN\n3.4\n279.0\n3.53\n143.0\n671.0\n113.15\n72.0\n136.0\n10.9\n3.0\n\n\n\n\n\n\n\nThe initial entries of the dataset, as revealed by the train.head() command, offer a glimpse into the fundamental attributes and features characterizing the primary biliary cirrhosis (PBC) dataset. These entries encompass a diverse range of patient-related information, providing crucial insights into the condition and status of the individuals under study.\nAttributes Overview:\n\nID: A unique identifier assigned to each patient within the dataset.\nN_Days: The number of days between registration and the earlier occurrence of death, liver transplantation, or the study analysis time in July 1986.\nStatus: Categorization of the patient’s status denoted as C (censored), CL (censored due to liver tx), or D (death).\nDrug: Indicates the type of drug administered to the patient, specifying either D-penicillamine or Placebo.\nAge: Age of the patient represented in days.\nSex: Gender classification denoted as M (male) or F (female).\nAscites: Presence or absence of ascites, indicated by Y (Yes) or N (No), respectively.\nHepatomegaly: Identification of hepatomegaly, denoted by Y (Yes) or N (No).\nSpiders: Presence or absence of spiders, indicating Y (Yes) or N (No), respectively.\nEdema: Details the presence and management of edema, categorized as S (edema present without diuretics, or edema resolved by diuretics), Y (edema despite diuretic therapy), or N (no edema and no diuretic therapy for edema).\nBilirubin: Serum bilirubin levels measured in mg/dl.\nCholesterol: Serum cholesterol levels measured in mg/dl.\nAlbumin: Albumin levels in gm/dl.\nCopper: Urine copper levels recorded in ug/day.\nAlk_Phos: Alkaline phosphatase levels in U/liter.\nSGOT: SGOT (Serum Glutamic Oxaloacetic Transaminase) levels measured in U/ml.\nTryglicerides: Triglyceride levels measured in mg/dl.\nPlatelets: Platelet count per cubic ml/1000.\nProthrombin: Prothrombin time measured in seconds (s).\nStage: Histologic stage of the disease categorized as 1, 2, 3, or 4.\n\nThis snapshot serves as a foundational view of the dataset, highlighting the diverse array of attributes and their corresponding values that will be further explored, analyzed, and utilized in the predictive modeling and analysis undertaken in this notebook.\n\n\nDataset Information Summary\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 418 entries, 1 to 418\nData columns (total 19 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   N_Days         418 non-null    int64  \n 1   Status         418 non-null    object \n 2   Drug           312 non-null    object \n 3   Age            418 non-null    int64  \n 4   Sex            418 non-null    object \n 5   Ascites        312 non-null    object \n 6   Hepatomegaly   312 non-null    object \n 7   Spiders        312 non-null    object \n 8   Edema          418 non-null    object \n 9   Bilirubin      418 non-null    float64\n 10  Cholesterol    284 non-null    float64\n 11  Albumin        418 non-null    float64\n 12  Copper         310 non-null    float64\n 13  Alk_Phos       312 non-null    float64\n 14  SGOT           312 non-null    float64\n 15  Tryglicerides  282 non-null    float64\n 16  Platelets      407 non-null    float64\n 17  Prothrombin    416 non-null    float64\n 18  Stage          412 non-null    float64\ndtypes: float64(10), int64(2), object(7)\nmemory usage: 65.3+ KB\n\n\nThe output from train.info() furnishes essential insights into the structure and composition of the primary biliary cirrhosis (PBC) dataset, providing a comprehensive understanding of the data types, non-null counts, and overall characteristics of the dataset’s columns.\nKey Observations:\n\nTotal Entries: The dataset comprises 418 entries or records, each representing an individual patient case, spanning a range from 0 to 417.\nColumns and Non-Null Counts: The dataset incorporates 19 columns, each delineating a specific attribute. Notably, several columns showcase discrepancies in non-null counts, indicating missing or null values within the dataset.\n\nCategorical Variables: Columns such as ‘Drug,’ ‘Ascites,’ ‘Hepatomegaly,’ ‘Spiders,’ ‘Edema,’ and ‘Status’ are represented as object data types, signifying categorical information regarding medications, symptoms, and patient status. Among these, ‘Drug,’ ‘Ascites,’ ‘Hepatomegaly,’ and ‘Spiders’ exhibit missing values.\nNumerical Variables: The dataset also contains numerical variables denoted by float64 and int64 data types, encompassing clinical measurements and patient-specific information such as age, laboratory test results, and disease stage. Notably, columns like ‘Cholesterol,’ ‘Copper,’ ‘Alk_Phos,’ ‘SGOT,’ ‘Tryglicerides,’ ‘Platelets,’ ‘Prothrombin,’ and ‘Stage’ display missing values.\n\nMemory Usage: The dataset’s memory usage is estimated to be approximately 65.4 KB, reflecting the computational footprint of the dataset in memory.\n\nData Completeness and Challenges:\n\nThe presence of missing values across various columns warrants attention and necessitates strategies for data imputation or handling missing data during the preprocessing phase.\n\nThis information overview lays the groundwork for comprehensive data exploration, cleaning, and preprocessing steps essential for building robust predictive models in the subsequent sections of this analysis.\n\n\nStatistical Summary of the Dataset\n\ndata.describe()\n\n\n\n\n\n\n\n\nN_Days\nAge\nBilirubin\nCholesterol\nAlbumin\nCopper\nAlk_Phos\nSGOT\nTryglicerides\nPlatelets\nProthrombin\nStage\n\n\n\n\ncount\n418.000000\n418.000000\n418.000000\n284.000000\n418.000000\n310.000000\n312.000000\n312.000000\n282.000000\n407.000000\n416.000000\n412.000000\n\n\nmean\n1917.782297\n18533.351675\n3.220813\n369.510563\n3.497440\n97.648387\n1982.655769\n122.556346\n124.702128\n257.024570\n10.731731\n3.024272\n\n\nstd\n1104.672992\n3815.845055\n4.407506\n231.944545\n0.424972\n85.613920\n2140.388824\n56.699525\n65.148639\n98.325585\n1.022000\n0.882042\n\n\nmin\n41.000000\n9598.000000\n0.300000\n120.000000\n1.960000\n4.000000\n289.000000\n26.350000\n33.000000\n62.000000\n9.000000\n1.000000\n\n\n25%\n1092.750000\n15644.500000\n0.800000\n249.500000\n3.242500\n41.250000\n871.500000\n80.600000\n84.250000\n188.500000\n10.000000\n2.000000\n\n\n50%\n1730.000000\n18628.000000\n1.400000\n309.500000\n3.530000\n73.000000\n1259.000000\n114.700000\n108.000000\n251.000000\n10.600000\n3.000000\n\n\n75%\n2613.500000\n21272.500000\n3.400000\n400.000000\n3.770000\n123.000000\n1980.000000\n151.900000\n151.000000\n318.000000\n11.100000\n4.000000\n\n\nmax\n4795.000000\n28650.000000\n28.000000\n1775.000000\n4.640000\n588.000000\n13862.400000\n457.250000\n598.000000\n721.000000\n18.000000\n4.000000\n\n\n\n\n\n\n\nThe train.describe() output provides a comprehensive statistical overview of the numerical columns present in the PBC dataset, offering insights into the central tendencies, dispersion, and distribution of various clinical measurements and patient-specific attributes.\nKey Observations:\n\nCount: The count for each column signifies the number of non-null entries available for the corresponding attribute. Discrepancies in counts indicate the presence of missing or null values across different features.\nCentral Tendencies:\n\nN_Days (Number of Days): The average time between registration and events (death, transplantation, or study analysis) is approximately 1917.78 days, with a wide range from 41 days to 4795 days.\nAge: The average age in days stands at approximately 18533.35, with a minimum of 9598 days and a maximum of 28650 days, showcasing the wide age range of patients included in the dataset.\nClinical Measurements: Various clinical measurements like bilirubin, cholesterol, albumin, copper, alkaline phosphatase (Alk_Phos), SGOT, triglycerides, platelets, prothrombin, and disease stage (Stage) display different mean values and ranges, indicating diversity in patient conditions and responses to treatment.\n\nDispersion:\n\nStandard Deviation: The standard deviation illustrates the dispersion or spread of values around the mean for each numerical attribute. Higher standard deviations, as observed in bilirubin, cholesterol, Alk_Phos, SGOT, and others, suggest greater variability in these measurements across patients.\n\nMinimum, Maximum, and Quartiles:\n\nThe minimum and maximum values signify the range of values observed within each attribute.\nQuartiles (25%, 50%, and 75%) offer insights into the distribution of values, providing information on the spread of data and potential presence of outliers within each variable.\n\n\nData Interpretation:\n\nThe wide range of values across different clinical measurements and patient-specific attributes underlines the heterogeneity of patient conditions within the PBC dataset.\nThe presence of missing values, particularly notable in columns like ‘Cholesterol,’ ‘Copper,’ ‘Tryglicerides,’ ‘Platelets,’ and ‘Stage,’ indicates the necessity for appropriate handling of missing data during data preprocessing and analysis stages.\n\nThis statistical summary lays the foundation for a deeper exploration and analysis of the dataset, guiding further preprocessing, feature engineering, and model development in subsequent sections of the analysis.\n\n\nPie Plot Analysis\n\ncat_cols = data.drop([\"Status\"], axis=1).select_dtypes(include='object').columns.tolist()\nnum_cols = data.select_dtypes(exclude='object').columns.tolist()\n\n\ndef plot_target(data: pd.DataFrame, col: str, title: str, pie_colors: list, test_df: pd.DataFrame = pd.DataFrame()) -&gt; None:\n    if not test_df.empty:\n        fig, axes = plt.subplots(1, 4, figsize=(20, 6), gridspec_kw={'width_ratios': [2, 1, 2, 1]})\n        \n        for i, data in enumerate([data, test_df]):\n            textprops={'fontsize': 12, 'weight': 'bold', 'color': 'black'}\n            ax = axes[i * 2]\n            \n            ax.pie(data[col].value_counts().to_list(),\n                colors=pie_colors,\n                labels=data[col].value_counts().index.to_list(),\n                autopct='%1.f%%',\n                explode=([.05] * data[col].nunique()),\n                pctdistance=0.5,\n                wedgeprops={'linewidth': 1, 'edgecolor': 'black'},\n                textprops=textprops)\n\n            sns.countplot(x=col, data=data, palette='pastel6', hue=col, order=data[col].value_counts().to_dict().keys(), ax=axes[i * 2 + 1])\n\n            for p, count in enumerate(data[col].value_counts().to_dict().values(), 0):\n                axes[i * 2 + 1].text(p - 0.1, count + (np.sqrt(count)), count, color='black', fontsize=13)\n\n            plt.setp(axes[i * 2 + 1].get_xticklabels(), fontweight='bold')\n            plt.yticks([], ax=axes[i * 2 + 1])\n            axes[i * 2 + 1].set_ylabel('')\n            axes[i * 2 + 1].set_xlabel('')\n            # axes[i * 2 + 1].get_legend().remove()\n            # plt.box(False, ax=axes[i * 2 + 1])\n\n            axes[i * 2].set_title(f'Distribution in {\"Train\" if i == 0 else \"Test\"} Set', fontsize=16, fontweight='bold')\n\n        fig.suptitle(x=0.5, y=1.05, t=f'► {title} Distribution ◄', fontsize=18, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(15, 6), width_ratios=[2,1])\n\n        textprops={'fontsize': 12, 'weight': 'bold',\"color\": \"black\"}\n        ax[0].pie(data[col].value_counts().to_list(),\n                colors=pie_colors,\n                labels=data[col].value_counts().index.to_list(),\n                autopct='%1.f%%', \n                explode=([.05]*data[col].nunique()),\n                pctdistance=0.5,\n                wedgeprops={'linewidth' : 1, 'edgecolor' : 'black'}, \n                textprops=textprops)\n\n        sns.countplot(x = col, data=data, palette = \"pastel6\", hue=col, order=data[col].value_counts().to_dict().keys())\n        for p, count in enumerate(data[col].value_counts().to_dict().values(),0):\n            ax[1].text(p-0.1, count+(np.sqrt(count)), count, color='black', fontsize=13)\n        plt.setp(ax[1].get_xticklabels(), fontweight=\"bold\")\n        plt.yticks([])\n        plt.box()\n        fig.suptitle(x=0.56, t=f'► {title} Distribution ◄', fontsize=18, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n\nStatus Distribution\n\nplot_target(data,\n            col='Status', \n            title='Status', \n            pie_colors=['lightblue', 'lightgreen', 'orange'])\n\n\n\n\nThe pie chart representing the distribution of patient statuses (‘C’, ‘D’, ‘CL’) revealed the following ratios:\n\nC (censored): 56%\nD (death): 39%\nCL (censored due to liver tx): 6%\n\nThis distribution indicates that a majority of the patients (56%) had a status of ‘C’ (censored), followed by 39% classified as ‘D’ (death) and a smaller portion, 6%, categorized as ‘CL’ (censored due to liver tx). This insight provides an initial understanding of the imbalance in patient statuses within the dataset.\n\n\nCategorical Variable Distributions (Train and Test Sets)\n\nfor cat_col in cat_cols:\n    plot_target(data=data,\n                test_df=test_df, \n                col=cat_col, \n                title=cat_col, \n                pie_colors=['lightblue', 'lightgreen', 'orange'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of categorical variable distributions across both the train and test datasets demonstrated highly similar distributions for all categorical columns. These columns, encompassing attributes such as ‘Drug,’ ‘Ascites,’ ‘Hepatomegaly,’ ‘Spiders,’ ‘Edema,’ and others, showcased consistent patterns across both datasets.\nThe similarity in distributions suggests that the categorical variables exhibit consistent trends and distributions between the train and test datasets. This consistency is vital when deploying machine learning models trained on the train dataset to make predictions on unseen test data, ensuring that the model generalizes well to new, unseen observations.\nOverall, the plots depicting categorical variable distributions across train and test datasets reveal remarkable coherence, implying a uniform representation of categorical attributes between the datasets."
  },
  {
    "objectID": "pages/cirrhosis.html#data-preprocessing",
    "href": "pages/cirrhosis.html#data-preprocessing",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nOne-Hot Encoding and Data Preparation\n\nOne-Hot Encoding Categorical Variables\nTo facilitate the utilization of categorical variables in machine learning models, a process known as one-hot encoding was applied to the categorical columns present in the primary biliary cirrhosis (PBC) dataset.\n\ndata_cat = pd.get_dummies(data, columns=cat_cols, drop_first=True, dtype=int)\n\nThe pd.get_dummies() function from the Pandas library was used to convert categorical variables into numerical representations suitable for model training. This process expanded the categorical columns into binary columns, creating new binary features for each category within the original categorical variables. The drop_first=True parameter was employed to drop the first level of each categorical variable to prevent multicollinearity in the dataset, reducing the risk of introducing redundant information.\n\n\nFeature-Target Splitting\nFollowing the one-hot encoding, the dataset was prepared for machine learning modeling by splitting it into feature variables (X) and the target variable (y).\n\nX = data_cat.drop([\"Status\"], axis=1)\ny = data_cat[\"Status\"].map({\"C\": 0, \"CL\": 1, \"D\": 2}).to_numpy().reshape((-1,))\n\n\nFeature Variables (X): The feature variables (X) were derived by excluding the ‘Status’ column, which serves as the target variable for prediction. These feature variables encompass the encoded categorical attributes and numerical features, ready to be used for training predictive models.\nTarget Variable (y): The ‘Status’ column was transformed into numerical labels representing the classes ‘C’, ‘CL’, and ‘D’. This transformation was performed using the map() function to assign numerical values (0, 1, 2) to the respective classes. The resulting ‘y’ variable constitutes the target labels for training the machine learning models, enabling the models to learn and predict the patient statuses based on the provided features.\n\nThis process of encoding categorical variables and preparing the feature-target split lays the groundwork for subsequent model training and evaluation tasks within the predictive modeling pipeline.\n\n\n\nScaling using RobustScaler\nIn the realm of data preprocessing, scaling plays a pivotal role in standardizing numerical features, ensuring a level playing field for different attributes that might have varying scales and distributions. For this dataset concerning primary biliary cirrhosis (PBC), employing the RobustScaler technique proves advantageous due to its robustness against outliers.\n\nRobustScaler Explanation:\nRobustScaler is a scaling technique that utilizes robust statistics to scale features. It operates by centering and scaling data based on the interquartile range (IQR). Unlike standard scaling methods (e.g., MinMaxScaler, StandardScaler), RobustScaler relies on the median and the IQR, making it less sensitive to outliers. It scales features by subtracting the median and then dividing by the IQR, effectively reducing the influence of outliers on the scaling process.\n\n\nApplication of RobustScaler:\nIn the context of the PBC dataset, several numerical attributes such as bilirubin, cholesterol, copper, alkaline phosphatase, SGOT, triglycerides, platelets, prothrombin, and others might exhibit potential outliers due to variations in patient conditions or laboratory measurements. Applying RobustScaler to these features can help normalize their scales, mitigating the impact of outliers and ensuring that the machine learning models are less influenced by extreme values.\n\nfrom sklearn.preprocessing import RobustScaler\n\n# Extracting numerical columns\nnum_cols = X.select_dtypes(exclude='object').columns.tolist()\n\n# Initializing RobustScaler\nscaler = RobustScaler()\n\n# Scaling numerical features in the train dataset\nX[num_cols] = scaler.fit_transform(X[num_cols])\n\nThe RobustScaler instance is instantiated and applied to the numerical columns in both the train and test datasets. This process ensures that numerical features are transformed and standardized using robust statistics, contributing to improved model performance and stability.\n\n\n\nHandling Missing Values: KNNImputer\nMissing values are a common occurrence in datasets and necessitate careful handling to ensure the integrity and effectiveness of machine learning models. In the context of the primary biliary cirrhosis (PBC) dataset, several columns exhibit missing values across different attributes, requiring an effective strategy for imputation.\n\nKNNImputer Explanation:\nKNNImputer is an imputation technique that leverages the concept of k-nearest neighbors to estimate and fill missing values in a dataset. This imputer calculates the missing values based on the values of neighboring data points (samples) in the dataset. It identifies the k nearest neighbors of each sample with missing features and imputes the missing values by taking the average (or another user-defined function) of those neighbors’ values for the specific feature.\n\n\nApplication of KNNImputer:\nGiven the complexity and diversity of patient-related attributes in the PBC dataset, utilizing KNNImputer proves beneficial. It accounts for the relationships and similarities between samples, allowing for more informed imputation of missing values. By considering the nearest neighbors, KNNImputer provides a robust strategy for imputing missing values in a dataset with complex patterns and diverse patient profiles.\n\nfrom sklearn.impute import KNNImputer\n\n# Initializing KNNImputer with the desired number of neighbors (n_neighbors)\nimputer = KNNImputer(n_neighbors=5)\n\n# Imputing missing values in the train dataset\nX_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n\nThe KNNImputer instance is initialized with the desired number of neighbors (e.g., 5) and applied to impute missing values in both the train and test datasets. This method leverages the similarities between samples to estimate and fill missing values in the dataset effectively."
  },
  {
    "objectID": "pages/cirrhosis.html#exploratory-data-analysis-eda",
    "href": "pages/cirrhosis.html#exploratory-data-analysis-eda",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nCorrelation Analysis\n\n# Calculating correlation matrix\ncorrelation_matrix = pd.concat([X_imputed, pd.DataFrame(y, columns=[\"Status\"])], axis=1).corr()\n\n# Plotting a heatmap to visualize correlations\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation Heatmap of Features')\nplt.xlabel('Features')\nplt.ylabel('Features')\nplt.show()\n\n\n\n\nThe correlation heatmap above displays the pairwise correlations between the features present in the Primary Biliary Cirrhosis (PBC) dataset after preprocessing and imputation. Each cell in the heatmap represents the correlation coefficient between two variables, ranging from -1 to 1.\nInterpreting the Heatmap:\n\nPositive Correlation: Values closer to 1 indicate a strong positive relationship, implying that as one variable increases, the other tends to increase as well.\nNegative Correlation: Values closer to -1 signify a strong negative relationship, indicating that as one variable increases, the other tends to decrease.\nCorrelation Close to 0: Values close to 0 suggest a weak or no linear relationship between variables.\n\nInsights from Correlation Analysis:\nThe correlation matrix reveals valuable insights into the relationships between various features and the target variable (‘Status’) in the Primary Biliary Cirrhosis (PBC) dataset. Here are key observations:\n\nStrong Correlations with ‘Status’:\n\nBilirubin (0.43): Exhibits a moderate positive correlation with ‘Status’, indicating a considerable association between higher levels of bilirubin and the patient’s status.\nAlbumin (-0.26): Shows a moderate negative correlation, suggesting that lower levels of albumin might be associated with a deteriorating patient status.\nCopper (0.41): Demonstrates a significant positive correlation, implying a potential relationship between higher copper levels and a positive patient status.\n\nOther Significant Correlations:\n\nSGOT (0.31): Shows a moderate positive correlation with ‘Status’, indicating a potential influence on patient status.\nProthrombin (0.34): Displays a moderate positive correlation, suggesting a possible impact on the patient’s status.\nEdema_Y (0.25): Indicates a moderate positive correlation, suggesting a correlation between the presence of edema and patient status.\n\nWeak Correlations:\n\nAge (0.19): Shows a weak positive correlation with ‘Status’, suggesting a mild association between age and patient status.\nCholesterol (0.20): Displays a weak positive correlation with ‘Status’, indicating a subtle relationship.\n\nInverse Correlation:\n\nPlatelets (-0.09): Exhibits a weak negative correlation, implying a minor association between platelet count and patient status.\n\n\nThese correlation insights provide a preliminary understanding of potential influential factors in predicting the status of PBC patients. However, correlation doesn’t imply causation. Further analysis, such as feature importance determination using machine learning models or domain-specific investigations, is crucial for accurate predictive modeling and clinical interpretations."
  },
  {
    "objectID": "pages/cirrhosis.html#model-building",
    "href": "pages/cirrhosis.html#model-building",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Model Building",
    "text": "Model Building\n\nCross-validation and Train-Test Split\n\nStratified K-Fold Cross-Validation\nCross-validation is a crucial technique used to assess the performance and generalizability of machine learning models. Stratified K-Fold cross-validation, implemented through StratifiedKFold, is particularly advantageous when working with classification tasks, maintaining the distribution of the target variable’s classes across folds.\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n# Initializing Stratified K-Fold with 5 folds\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nIn this code snippet, the StratifiedKFold object is created with parameters: - n_splits=5: Divides the dataset into 5 folds for cross-validation. - shuffle=True: Shuffles the data before splitting to ensure randomness. - random_state=42: Sets a random seed for reproducibility.\n\n\nTrain-Test Split\nThe train_test_split function partitions the dataset into training and testing sets, facilitating model training and evaluation.\n\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the dataset into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=3935, stratify=y)\n\nHere, train_test_split:\n\nX_imputed and y are the feature and target variables, respectively.\ntest_size=0.2: Allocates 20% of the data for testing, leaving 80% for training.\nrandom_state=3935: Sets a specific seed for reproducibility in random sampling.\nstratify=y: Ensures that the splitting preserves the proportion of classes in the target variable ‘y’.\n\nCombining StratifiedKFold for cross-validation and train_test_split for initial training and testing partitions ensures robust model validation and evaluation, contributing to more reliable model performance estimation.\nThis approach facilitates both cross-validation to assess model performance across multiple folds and the creation of distinct training and testing sets for initial model training and evaluation.\n\n\n\nTraining Multiple LightGBM Models with Cross-Validation\nThe code snippet demonstrates the training of multiple LightGBM models using StratifiedKFold for cross-validation and evaluating their performance.\n\n%%capture\n\nimport lightgbm as lgb\nfrom sklearn.metrics import log_loss\n\n# List to store trained LightGBM models\nlg_models = []\n\n# Parameters for the LightGBM model\nparams = {\n    'objective': 'multiclass',\n    'metric': 'multi_logloss', \n    'max_depth': 15, \n    'min_child_samples': 13, \n    'learning_rate': 0.05285597081335651, \n    'n_estimators': 284, \n    'min_child_weight': 5, \n    'subsample': 0.7717873512945741,\n    'colsample_bytree': 0.10012816493265511, \n    'reg_alpha': 0.8767668608061822, \n    'reg_lambda': 0.8705834466355764,\n    'random_state': 42,\n    'verbose': -1\n}\n\n# Training multiple LightGBM models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    LGBModel = lgb.LGBMClassifier(**params)\n    LGBModel.fit(X_train.iloc[x_idx], y_train[x_idx], eval_set=[(X_train.iloc[val_idx], y_train[val_idx])])\n    lg_models.append(LGBModel)\n\nExplanation:\n\nlg_models: This list stores trained LightGBM models.\nparams: Represents the hyperparameters configuration for the LightGBM model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nLGBModel.fit(): Trains the LightGBM model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\nlg_score: Initializes a variable to store the cumulative log loss across all models.\nfor i, LGBModel in enumerate(lg_models): Loops through the trained models and evaluates each on the test set using log_loss() function. It prints the log loss for each model.\n\n\n\n# Evaluating the models on the test set\nfor i, LGBModel in enumerate(lg_models):\n    y_pred = LGBModel.predict_proba(X_test)\n    print(f'Model {i+1} Log Loss: ', log_loss(y_test, y_pred))\n\nModel 1 Log Loss:  0.7048019339936583\nModel 2 Log Loss:  0.6668449298711905\nModel 3 Log Loss:  0.6558914179839223\nModel 4 Log Loss:  0.67101782116581\nModel 5 Log Loss:  0.7453997236011517\n\n\nThe log loss values obtained from the five LightGBM models showcase varying predictive performance. Models 3 and 2 exhibit relatively lower log loss values of 0.656 and 0.667, respectively, suggesting higher predictive accuracy compared to other models. Conversely, Model 5 presents the highest log loss of 0.745, indicating comparatively weaker predictive performance. These divergent log loss values underline differing levels of accuracy and precision across the models, suggesting a need for deeper investigation into the features and parameters influencing their respective performances.\n\n\nTraining Multiple XGBoost Models with Cross-Validation\nThe following code trains multiple XGBoost models using StratifiedKFold for cross-validation and evaluates their performance using log loss.\n\n%%capture\n\nimport xgboost as xgb\n\n# List to store trained XGBoost models\nxgb_models = []\n\n# Parameters for the XGBoost model\nparams ={\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'n_estimators': 397,\n    'max_depth': 44,\n    'min_child_weight': 4.8419409783368215,\n    'learning_rate': 0.049792103525168455,\n    'subsample': 0.7847543051746505,\n    'gamma': 0.4377096783729759,\n    'colsample_bytree': 0.22414960640035653,\n    'colsample_bylevel': 0.8173336142032213,\n    'colsample_bynode': 0.9468109886478254,\n    'random_state': 42,\n    'verbose': -1\n}\n\n# Training multiple XGBoost models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    xgb_model = xgb.XGBClassifier(**params)\n    xgb_model.fit(X_train.iloc[x_idx], y_train[x_idx], eval_set=[(X_train.iloc[val_idx], y_train[val_idx])], verbose=0)\n    xgb_models.append(xgb_model)\n\nExplanation:\n\nxgb_models: This list stores trained XGBoost models.\nparams: Represents the hyperparameters configuration for the XGBoost model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nxgb_model.fit(): Trains the XGBoost model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\nxgb_score: Initializes a variable to store the cumulative log loss across all models.\nfor i, xgb_model in enumerate(xgb_models): Loops through the trained models and evaluates each on the test set using log_loss() function. It prints the log loss for each model.\n\n\n# Evaluating the models on the test set\nfor i, xgb_model in enumerate(xgb_models):\n    y_pred = xgb_model.predict_proba(X_test)\n    print(f'Model {i+1} Log Loss: ', log_loss(y_test, y_pred))\n\nModel 1 Log Loss:  0.6815692319610926\nModel 2 Log Loss:  0.6508326753005506\nModel 3 Log Loss:  0.6568453212687483\nModel 4 Log Loss:  0.6553995987447889\nModel 5 Log Loss:  0.6723285733325526\n\n\nThe obtained log loss values from the five XGBoost models reveal varying performance levels. Models 2 and 3 exhibit relatively lower log loss values of approximately 0.651 and 0.657, respectively, indicating higher predictive accuracy compared to the other models. Conversely, Models 1, 4, and 5 present slightly higher log loss values, suggesting relatively weaker predictive performance. These diverse log loss scores indicate differing levels of predictive accuracy across the trained XGBoost models, highlighting potential variations in their learned patterns and the need for further investigation into their individual characteristics to improve overall model performance.\n\n\nTraining CatBoost Models and Evaluating Performance\nThe following code trains multiple CatBoost models using Stratified K-Fold cross-validation and assesses their performance using log loss:\n\nfrom catboost import CatBoostClassifier\n\n# List to store trained CatBoost models\ncat_models = []\n\n# Parameters for the CatBoost model\nparams = {\n    'logging_level': 'Silent', \n    'random_seed': 42, \n    'iterations': 593,\n    'depth': 43,\n    'min_data_in_leaf': 42,\n    'learning_rate': 0.023456006693305914,\n    'subsample': 0.8018560299887264,\n    'random_strength': 0.04176274518438195,\n    'grow_policy': 'Lossguide',\n    'bootstrap_type' : 'Bernoulli',\n    # 'bootstrap_type': 'Poisson'\n}\n\n# Training multiple CatBoost models using Stratified K-Fold\nfor x_idx, val_idx in skf.split(X_train, y_train):\n    cat_model = CatBoostClassifier(**params)\n    cat_model.fit(X=X_train.iloc[x_idx], y=y_train[x_idx], eval_set=[(X_train.iloc[val_idx], y_train[val_idx])])\n    cat_models.append(cat_model)\n\nExplanation:\n\ncat_models: This list stores trained CatBoost models.\nparams: Represents the hyperparameters configuration for the CatBoost model.\nfor x_idx, val_idx in skf.split(X_train, y_train): Iterates over the folds generated by Stratified K-Fold.\nCatBoostClassifier.fit(): Trains the CatBoost model on the training data using fit() method. The eval_set parameter enables tracking model performance on the validation set during training.\ncat_score: Initializes a variable to store the cumulative log loss across all CatBoost models.\nfor i, cat_model in enumerate(cat_models): Loops through the trained models and evaluates each on the test set using log_loss() function. It prints the log loss for each model.\n\nThis section demonstrates the training of multiple CatBoost models using cross-validation and subsequent evaluation of their performance using log loss on the test set. The log loss metric measures the accuracy of the model’s predicted probabilities compared to the true labels. Adjustments to hyperparameters or model evaluation strategies can further refine model performance.\n\n\n# Evaluating the models on the test set\nfor i, cat_model in enumerate(cat_models):\n    y_pred = cat_model.predict_proba(X_test)\n    print(f'Model {i+1} Log Loss: ', log_loss(y_test, y_pred))\n\nModel 1 Log Loss:  0.6411249325455535\nModel 2 Log Loss:  0.6347449226539893\nModel 3 Log Loss:  0.6667271272394244\nModel 4 Log Loss:  0.6372961395542406\nModel 5 Log Loss:  0.6731041546602471\n\n\nThe log loss values obtained from the five CatBoost models suggest varying degrees of predictive performance. Model 2 showcases the lowest log loss of 0.635, indicating higher predictive accuracy compared to the other models. Conversely, Model 5 exhibits the highest log loss of 0.673, signifying relatively weaker predictive performance. These divergent log loss values signify differing levels of precision across the models, emphasizing the need for deeper exploration into the models’ attributes, such as hyperparameters or feature importance, to ascertain the factors influencing their respective performances.\n\n\nElevating Predictive Power with Stacked Ensemble Model\nNow, let’s explore the construction of a more robust predictive model through a technique called Stacking. Stacking involves combining multiple machine learning models, leveraging their diverse strengths to enhance overall predictive performance. In this section, we’ll build a Stacked Ensemble Model using an MLPClassifier as the final estimator.\n\nModel Configuration:\n\nMLPClassifier: A Multi-layer Perceptron (MLP) neural network with 64 and 32 neurons in its hidden layers, employing the ‘relu’ activation function, ‘adam’ solver, and various hyperparameters for optimization.\n\n\n\nStackingClassifier Configuration:\n\nEstimators: The StackingClassifier utilizes predictions from previously trained models, including LGBM, XGBoost, and CatBoost.\nFinal Estimator: The final estimator, an MLPClassifier, aggregates predictions from the base models.\nCross-Validation (cv): Employing Stratified K-Fold cross-validation ensures robustness in model evaluation and performance estimation.\n\n\n\nImplementation:\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Initializing an MLPClassifier\nmlp = MLPClassifier(\n    hidden_layer_sizes=(64, 32),\n    max_iter=1000,\n    random_state=42,\n    activation='relu',\n    learning_rate_init=0.001,\n    solver='adam',\n    validation_fraction=0.1,\n    momentum=0.9,\n    nesterovs_momentum=True,\n    batch_size=32,\n    beta_1=0.9,\n    beta_2=0.999\n)\n\n# Creating a StackingClassifier\nstacking_model = StackingClassifier(\n    estimators=[\n        ('LGBM', LGBModel),\n        ('XGB', xgb_model),\n        ('CAT', cat_model)\n    ],\n    final_estimator=mlp,\n    cv=skf\n)\n\n\n%%capture\n# Fitting the StackingClassifier on the training data\nstacking_model.fit(X_train, y_train)\n\n\n\nExplanation:\nThe StackingClassifier combines predictions from diverse base models (LGBM, XGBoost, CatBoost) and utilizes an MLPClassifier as the final layer to learn and make predictions based on the diverse outputs. This stacking technique aims to improve predictive accuracy by leveraging the collective knowledge of multiple models, potentially capturing a more nuanced understanding of the data and enhancing overall performance on unseen test data. The model fitting is conducted using the training data, and subsequent predictions are generated for evaluation and assessment of the ensemble model’s effectiveness."
  },
  {
    "objectID": "pages/cirrhosis.html#model-evaluation",
    "href": "pages/cirrhosis.html#model-evaluation",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nEvaluating Stacked Ensemble Model Performance\n\nModel Evaluation Metrics:\nTo assess the performance of the Stacked Ensemble Model, several evaluation metrics are computed using the model’s predictions on the test dataset.\n\n\nEvaluation Metrics Computed:\n\nLog Loss: A measure of uncertainty in the model’s predictions.\nAccuracy: Proportion of correctly predicted outcomes.\nPrecision: Measure of the model’s exactness in predicting each class.\nRecall: Measure of the model’s completeness in capturing each class.\nF1 Score: Harmonic mean of precision and recall, providing a balanced assessment.\n\n\n\nConfusion Matrix and Classification Report:\n\nConfusion Matrix: Tabulation of actual vs. predicted class counts, aiding in understanding misclassifications.\nClassification Report: Detailed summary showcasing precision, recall, F1 score, and support for each class.\n\n\n\nEvaluation Process and Metrics Computation:\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n\n# Evaluate the model on the test data\ny_pred = stacking_model.predict_proba(X_test)\nlloss = log_loss(y_test, y_pred)\nprint(f\"Log loss on test data: {lloss}\")\n\n# Round probabilities to get hard predictions\ny_pred_hard = np.argmax(y_pred, axis=1)\ny_test_hard = y_test\n\n# Calculate and print evaluation metrics\naccuracy = accuracy_score(y_test_hard, y_pred_hard)\nprecision = precision_score(y_test_hard, y_pred_hard, average='weighted')\nrecall = recall_score(y_test_hard, y_pred_hard, average='weighted')\nf1 = f1_score(y_test_hard, y_pred_hard, average='weighted')\nconf_matrix = confusion_matrix(y_test_hard, y_pred_hard)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Print classification report and confusion matrix\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_hard, y_pred_hard))\nprint(\"\\nConfusion Matrix:\")\nprint(conf_matrix)\n\nLog loss on test data: 0.6094441313988819\nAccuracy: 0.7500\nPrecision: 0.7648\nRecall: 0.7500\nF1 Score: 0.7345\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.74      0.89      0.81        47\n           1       1.00      0.20      0.33         5\n           2       0.77      0.62      0.69        32\n\n    accuracy                           0.75        84\n   macro avg       0.84      0.57      0.61        84\nweighted avg       0.76      0.75      0.73        84\n\n\nConfusion Matrix:\n[[42  0  5]\n [ 3  1  1]\n [12  0 20]]\n\n\n\n\n\nAnalysis of Stacked Ensemble Model Performance\nThe evaluation metrics obtained from the Stacked Ensemble Model’s predictions on the test dataset are instrumental in understanding its performance.\n\nInsights from Evaluation Metrics:\n\nLog Loss: The model’s log loss of 0.6094 indicates moderate uncertainty in its predictions, with lower values being desirable.\nAccuracy (0.7500): The model accurately predicted 75% of instances in the test dataset.\nPrecision (0.7648): The precision score of 0.7648 suggests that when the model predicts a certain class, it is correct approximately 76.48% of the time on average across all classes.\nRecall (0.7500): The recall score of 0.7500 indicates that the model identified 75% of all actual instances for each class.\nF1 Score (0.7345): The F1 score, at 0.7345, demonstrates the harmonic mean of precision and recall, reflecting the model’s balance between these metrics.\n\n\n\nDetailed Class-wise Performance:\nThe classification report and confusion matrix offer insights into the model’s performance for each class:\n\nClass 0:\n\nPrecision (0.74): The model achieved 74% precision in correctly identifying instances of Class 0.\nRecall (0.89): It captured 89% of all actual instances of Class 0.\nF1 Score (0.81): The harmonic mean of precision and recall for Class 0 is 0.81.\n\n\n\nClass 1:\n\nPrecision (1.00): Perfect precision was observed for Class 1, albeit from a relatively small number of instances.\nRecall (0.20): The recall score for Class 1 is lower, capturing only 20% of all actual instances.\n\n\n\nClass 2:\n\nPrecision (0.77): The model exhibited 77% precision in predicting instances of Class 2.\nRecall (0.62): It captured 62% of all actual instances of Class 2.\nF1 Score (0.69): The F1 score for Class 2, at 0.69, signifies a balance between precision and recall.\n\n\n\n\nOverall Summary:\nThe model performed relatively well in identifying instances of Class 0, showcasing strong precision and recall. However, for Classes 1 and 2, while precision was moderate, recall varied, indicating room for improvement, especially in identifying Class 1 instances.\nThe confusion matrix illustrates specific misclassifications between classes, aiding in understanding where the model struggles and excels. This analysis helps identify the model’s strengths and areas that require further refinement for more robust predictions."
  },
  {
    "objectID": "pages/cirrhosis.html#conclusion-and-future-steps",
    "href": "pages/cirrhosis.html#conclusion-and-future-steps",
    "title": "Predictive Modeling of Patient Status in Primary Biliary Cirrhosis",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nConclusion\nThe Stacked Ensemble Model exhibited promising performance in predicting classes across the dataset. It achieved an overall accuracy of 75%, demonstrating a fair ability to classify instances into their respective classes. However, a closer examination reveals areas for enhancement.\n\n\nKey Findings\n\nPerformance Variability: The model showcased strong precision and recall for Class 0, while Classes 1 and 2 demonstrated varying results, particularly lower recall in Class 1.\nLog Loss: The moderate log loss of 0.6094 suggests room for improvement in reducing prediction uncertainty.\n\n\n\nInsights for Improvement\n\nClass Imbalance Handling: Addressing class imbalances, especially for Class 1, could enhance the model’s ability to recognize these instances more accurately.\nModel Tuning: Further optimization of hyperparameters, especially related to individual base learners and the final stacked model, may refine its predictive capabilities.\nFeature Engineering: Exploring additional features or engineering existing ones might offer a deeper understanding of the data, potentially leading to improved model performance.\n\n\n\nFuture Steps\n\nFine-Tuning Hyperparameters: Conduct more extensive hyperparameter tuning to seek better combinations that enhance the model’s performance across all classes.\nFeature Enhancement: Engage in thorough feature analysis and engineering to uncover more informative features or transformations that can contribute positively to the model’s predictive power.\nEnsemble Diversification: Experiment with diverse base models or ensemble techniques to enhance the model’s diversity and, consequently, its overall predictive ability.\nRobust Validation: Consider employing additional cross-validation strategies or validation techniques to validate the model’s consistency and generalization.\nModel Interpretation: Explore methods to interpret the model’s decisions, aiding in understanding its behavior and potentially identifying areas for improvement.\n\n\n\nFinal Note\nContinued refinement and exploration of advanced techniques are vital for enhancing the model’s predictive performance. By addressing the identified areas for improvement and leveraging sophisticated methodologies, the model can evolve into a more robust and accurate predictor, proving invaluable in various real-world applications."
  },
  {
    "objectID": "pages/accuracy.html",
    "href": "pages/accuracy.html",
    "title": "Accuracy",
    "section": "",
    "text": "Accuracy is a metric for evaluating the performance of a classification algorithm. It can be understood as the fraction of predictions the model performed correctly.\n\\displaystyle \\text{Accuracy} \\ =\\ \\frac{\\text{No of correct predictions}}{\\text{Total number of predictions}}"
  },
  {
    "objectID": "pages/accuracy.html#what-is-accuracy",
    "href": "pages/accuracy.html#what-is-accuracy",
    "title": "Accuracy",
    "section": "",
    "text": "Accuracy is a metric for evaluating the performance of a classification algorithm. It can be understood as the fraction of predictions the model performed correctly.\n\\displaystyle \\text{Accuracy} \\ =\\ \\frac{\\text{No of correct predictions}}{\\text{Total number of predictions}}"
  },
  {
    "objectID": "pages/accuracy.html#calculating-accuracy-for-binary-classification-problems",
    "href": "pages/accuracy.html#calculating-accuracy-for-binary-classification-problems",
    "title": "Accuracy",
    "section": "Calculating accuracy for Binary Classification problems",
    "text": "Calculating accuracy for Binary Classification problems\n \\displaystyle \\text{Accuracy} \\ =\\ \\frac{\\text{TP + TN}}{\\text{TP + FP +TN + FN}}\n\nWhere, TP = True Positives FP = False Positives TN = True Negatives FN = False Negatives"
  },
  {
    "objectID": "pages/accuracy.html#issues-with-using-accuracy",
    "href": "pages/accuracy.html#issues-with-using-accuracy",
    "title": "Accuracy",
    "section": "Issues with using Accuracy",
    "text": "Issues with using Accuracy\n\nIn an imbalaced dataset i.e when one class significantly outnumbers the other, if the model predicts the majority class all the time, we get a high accuracy but we know that the model can not be used"
  },
  {
    "objectID": "pages/accuracy.html#accuracy-in-imbalanced-dataset",
    "href": "pages/accuracy.html#accuracy-in-imbalanced-dataset",
    "title": "Accuracy",
    "section": "Accuracy in imbalanced dataset",
    "text": "Accuracy in imbalanced dataset\nConsider a dataset on 100 individuals where we would like to predict if an individual is suffering from cancer.\nThe dataset contains 90 individuals who do not suffer from cancer (class 0) whereas 10 of them who do (class 1). If we build a model that always predicts class 0, then our model will never be able to predict that an individual might be suffering from cancer. So, we establish that this is a bad model.\nHowever based on the definition of accuracy, \\displaystyle \\text{Accuracy} \\ =\\ \\frac{90}{100} = 0.9\nFrom this, we can see that we would have to use other metrics when we have imbalanced datasets"
  },
  {
    "objectID": "pages/accuracy.html#accuracy-in-sklearn",
    "href": "pages/accuracy.html#accuracy-in-sklearn",
    "title": "Accuracy",
    "section": "Accuracy in sklearn",
    "text": "Accuracy in sklearn\nlink: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\nTo calculate accuracy using sklearn, we use the accuracy_score function from the module sklearn.metrics\nThe parameters are as follows: \n\ny_true : Ground truth (correct) labels.\ny_pred : Predicted labels, as returned by a classifier.\nnormalize (True) : If False, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples.\nsample_weight (None) : Sample weights.\n\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\n\naccuracy_score(y_true, y_pred)\n\n0.5\n\n\n\n#normalize=False gives the count of correctly classified samples\n\naccuracy_score(y_true, y_pred, normalize=False)\n\n2\n\n\n\n#Sample weights can be varied for each point\n\naccuracy_score(y_true, y_pred, sample_weight=[0,0,0,1])\n\n1.0\n\n\n\n#To view a simplified version of the documentation in colab environment\naccuracy_score?"
  },
  {
    "objectID": "pages/accuracy.html#accuracy-on-a-real-word-dataset",
    "href": "pages/accuracy.html#accuracy-on-a-real-word-dataset",
    "title": "Accuracy",
    "section": "Accuracy on a real word dataset",
    "text": "Accuracy on a real word dataset\nWe are going to calculate the accuracy of a KNN classifier model on the breast_cancer dataset using sklearn.\nThe dataset does not have any categorical variables and we will just split the dataset into train and test. We shall fit the model on the training dataset and find the accuracy on the test dataset.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nX,y = load_breast_cancer(return_X_y=True,as_frame=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier()\nmodel.fit(X_train,y_train)\n\nmodel.score(X_test,y_test)\n\n0.965034965034965\n\n\n\ny_pred = model.predict(X_test)\naccuracy_score(y_test,y_pred)\n\n0.965034965034965\n\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7d7a6fb661a0&gt;\n\n\n\n\n\nIn the above matrix, we can see that: TP = 88, TN = 50, FP = 4, FN = 1\n\\begin{aligned}\n{\\displaystyle \\text{Accuracy}} & ={\\displaystyle \\frac{\\text{TP + TN}}{\\text{TP + FP +TN + FN}}}\\\\\n& \\\\\n& {\\displaystyle =\\frac{\\text{88 + 50}}{\\text{88 + 4 + 50 + 1}}}\\\\\n& \\\\\n& {\\displaystyle =\\ \\frac{\\text{138}}{\\text{143}}}\\\\\n& \\\\\n& {\\displaystyle =\\ 0.965}\n\\end{aligned}"
  },
  {
    "objectID": "pages/accuracy.html#tuning-for-accuracy",
    "href": "pages/accuracy.html#tuning-for-accuracy",
    "title": "Accuracy",
    "section": "Tuning for accuracy",
    "text": "Tuning for accuracy\nWe use GridSearchCV for hyperparameter tuning and cross validation. We can change the scoring metric to tune the model as per our requirement.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'n_neighbors':[3,5,10,20]}\n\nmodel_tuned = GridSearchCV(estimator=KNeighborsClassifier(),param_grid=param_grid,scoring='accuracy')\nmodel_tuned.fit(X_train,y_train)\n\nmodel_tuned.score(X_test,y_test)\n\n0.972027972027972"
  },
  {
    "objectID": "pages/accuracy.html#notes",
    "href": "pages/accuracy.html#notes",
    "title": "Accuracy",
    "section": "Notes",
    "text": "Notes\n\nThe value of accuracy required by a model varies by use case\nAccuracy can often be a misleading metric"
  },
  {
    "objectID": "pages/precision.html",
    "href": "pages/precision.html",
    "title": "Precision in Classification Metrics",
    "section": "",
    "text": "Precision is a classification metric that measures the accuracy of the positive predictions made by a model. It is defined as the ratio of true positives to the sum of true positives and false positives.\n\\displaystyle \\text{Precision} \\ =\\ \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\nPrecision is particularly useful in situations where the cost of false positives is high. For example, in medical diagnoses, where a false positive might lead to unnecessary treatments, precision becomes a crucial metric."
  },
  {
    "objectID": "pages/precision.html#calculating-precision-for-binary-classification-problems",
    "href": "pages/precision.html#calculating-precision-for-binary-classification-problems",
    "title": "Precision in Classification Metrics",
    "section": "Calculating Precision for Binary Classification Problems",
    "text": "Calculating Precision for Binary Classification Problems\n \\displaystyle \\text{Precision} \\ =\\ \\frac{\\text{TP}}{\\text{TP + FP}}\n\nWhere, TP = True Positives FP = False Positives"
  },
  {
    "objectID": "pages/precision.html#issues-with-using-precision",
    "href": "pages/precision.html#issues-with-using-precision",
    "title": "Precision in Classification Metrics",
    "section": "Issues with using Precision",
    "text": "Issues with using Precision\n\nA model with high precision may still have low recall, and vice versa. There is often a trade-off between precision and recall, and the choice between them depends on the specific problem and its requirements."
  },
  {
    "objectID": "pages/precision.html#where-can-precision-be-used",
    "href": "pages/precision.html#where-can-precision-be-used",
    "title": "Precision in Classification Metrics",
    "section": "Where can Precision be used?",
    "text": "Where can Precision be used?\n\nIn Fraud detection, if the primary objective is customer service, we do not want to flag a genuine transaction as fraud leading to inconvenience to the customer."
  },
  {
    "objectID": "pages/precision.html#precision-in-sklearn",
    "href": "pages/precision.html#precision-in-sklearn",
    "title": "Precision in Classification Metrics",
    "section": "Precision in sklearn",
    "text": "Precision in sklearn\nprecision_score is the function in scikit-learn used to calculate precision.\nThe parameters are as follows: - y_true: Ground truth (correct) labels. - y_pred: Predicted lab as returned by a classifier. - labels (None): The set of labels to include when average is not None. - pos_label (1): The label of the positive class. - average (‘binary’): The averaging strategy for multiclass settings. - sample_weight (None): Sample weights.\n\nfrom sklearn.metrics import precision_score\n\ny_true = [0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 1, 0, 1]\n\nprecision_score(y_true, y_pred)\n\n1.0"
  },
  {
    "objectID": "pages/precision.html#precision-on-a-real-world-dataset",
    "href": "pages/precision.html#precision-on-a-real-world-dataset",
    "title": "Precision in Classification Metrics",
    "section": "Precision on a real-world dataset",
    "text": "Precision on a real-world dataset\nWe are going to calculate the precision of a logistic regression model on the breast_cancer dataset using sklearn.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprecision_score(y_test, y_pred)\n\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.9666666666666667\n\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7e3d61cdd180&gt;\n\n\n\n\n\nIn the above matrix, we can see that: TP = 87, TN = 51, FP = 3, FN = 2\n\\begin{aligned}\n{\\displaystyle \\text{Precision}} & ={\\displaystyle \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}}\\\\\n& \\\\\n& {\\displaystyle =\\frac{\\text{87}}{\\text{87 + 3}}}\\\\\n& \\\\\n& {\\displaystyle =\\ \\frac{\\text{87}}{\\text{90}}}\\\\\n& \\\\\n& {\\displaystyle =\\ 0.9666}\n\\end{aligned}"
  },
  {
    "objectID": "pages/agg.html",
    "href": "pages/agg.html",
    "title": "Agglomerative Clustering",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/agg.html#toy-dataset",
    "href": "pages/agg.html#toy-dataset",
    "title": "Agglomerative Clustering",
    "section": "Toy Dataset",
    "text": "Toy Dataset\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram\n\nX = np.array([\n    [5, 5], [5, 6],\n    [7, 5], [8, 6],\n    [6, 10], [5, 12], [8, 10]\n])\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.axis('equal');"
  },
  {
    "objectID": "pages/agg.html#agglomerative-clustering",
    "href": "pages/agg.html#agglomerative-clustering",
    "title": "Agglomerative Clustering",
    "section": "Agglomerative Clustering",
    "text": "Agglomerative Clustering\nThe function agglomerative constructs a linkage matrix that encodes the hierarchical clustering, given a linkage function.\nMimics the behaviour of scipy.cluster.hierarchy.linkage\nReturns a (n-1) \\times 4 matrix where: - 1st and 2nd elements denote the index of the merged clusters - 3rd element denotes the linkage value - 4th element denotes the number of values in the merged cluster\nThe above scaffolding is what is implemented in the scipy library, and we attempt to replicate it here.\nThis allows the usage of scipy.cluster.hierarchy.dendrogram to visualize the clustering.\n\nfrom itertools import combinations\n\ndef agglomerative(X, linkage):\n\n  clusters = [[tuple(i)] for i in X]\n  n = len(X)\n\n  X_i = dict(zip([tuple(i) for i in X], range(n)))\n  merged = []\n  linkage_matrix = []\n  Zs = []\n\n  for _ in range(n-1):\n\n    min_dist = np.inf\n    current_clusters = [i for i in range(len(clusters)) if i not in merged]\n\n    # Compute linkage for all clusters pairwise to find best cluster-pair\n    for c1, c2 in combinations(current_clusters, 2):\n\n      linkage_val = linkage(clusters[c1], clusters[c2])\n\n      # Find cluster-pair with smallest linkage\n      if linkage_val &lt; min_dist:\n        min_dist = linkage_val\n        best_pair = sorted([c1, c2])\n\n    # Merge the best pair and append to clusters\n    clusters.append(clusters[best_pair[0]] + clusters[best_pair[1]])\n\n    # Add best pair clusters to merged\n    merged += best_pair\n\n    linkage_matrix.append(best_pair + [min_dist, len(clusters[-1])])\n\n    # Append cluster indicator array Z to Zs\n    Z = np.zeros(n)\n    for c in current_clusters:\n      for i in clusters[c]:\n        Z[X_i[i]] = c\n\n    Zs.append(Z)\n\n  Zs.append([len(clusters)-1]*n)\n\n  return np.array(linkage_matrix), np.array(Zs)\n\n\nLinkage 1: Single Linkage\nConsiders the intergroup dissimilarity to be that of the closest (least dissimilar) pair. Also called the nearest-neighbour technique.\n d_{SL}(C_1, C_2) = \\min _ {i \\in C_1 \\\\ j \\in C_2} d_{ij}  \n\ndef single(cluster_1, cluster_2):\n  single_linkage_val = np.inf\n\n  for p1 in cluster_1:\n    for p2 in cluster_2:\n\n      p1_p2_dist = np.linalg.norm(np.array(p1)-np.array(p2))\n\n      if single_linkage_val &gt; p1_p2_dist:\n        single_linkage_val = p1_p2_dist\n\n  return single_linkage_val\n\n\nl_mat, Zs = agglomerative(X, single)\ndendrogram(l_mat, leaf_label_func=lambda i: str(tuple(X[i])), leaf_rotation=-45, color_threshold=3);"
  },
  {
    "objectID": "pages/agg.html#dendrograms---how-to-read-them",
    "href": "pages/agg.html#dendrograms---how-to-read-them",
    "title": "Agglomerative Clustering",
    "section": "Dendrograms - How to read them",
    "text": "Dendrograms - How to read them\nDendrograms provides a highly interpretable complete description of the hierarchical clustering in a graphical format.\nThe y-axis indicates the value of the inter-group dissimilarity. Cutting the dendrogram horizontally at a particular height partitions the data into disjoint clusters represented by the vertical lines that intersect it. These are the clusters that would be produced by terminating the procedure when the optimal intergroup dissimilarity exceeds that threshold cut value.\n\ndef cluster_rename(Z):\n  renamed_Z = []\n  mapping = {}\n  x = len(Z)\n\n  for i in Z:\n    try:\n      renamed_Z.append(mapping[i])\n    except:\n      mapping[i] = x\n      x -= 1\n      renamed_Z.append(mapping[i])\n\n  return renamed_Z\n\ndef plot_ellipse(X, ax):\n  cov = np.cov(X[:, 0], X[:, 1])\n  val, rot = np.linalg.eig(cov)\n  val = np.sqrt(val)\n  if min(val)&lt;=0.01:\n    val += 0.2 * max(val)\n  center = np.mean([X[:, 0], X[:, 1]], axis=1)[:, None]\n\n  t = np.linspace(0, 2.0 * np.pi, 1000)\n  xy = np.stack((np.cos(t), np.sin(t)), axis=-1)\n\n  return ax.plot(*(2 * rot @ (val * xy).T + center))\n\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))\ndendrogram(l_mat, ax=ax2, leaf_label_func=lambda i: str(tuple(X[i])), leaf_rotation=-45);\nartists = []\nsplit_vals = list(l_mat[:, 2])\nsplit_vals = np.array([0] + split_vals + [split_vals[-1]*1.05])\n\nfor i in range(len(Zs)):\n  frame = []\n  frame.append(ax1.scatter(X[:, 0], X[:, 1], c=cluster_rename(Zs[i]), s=500))\n  for c in set(Zs[i]):\n    if sum(Zs[i]==c) &gt; 1:\n      frame += plot_ellipse(X[Zs[i]==c], ax1)\n  frame.append(ax2.axhline(y=split_vals[i:i+2].mean(), color='red'))\n  artists.append(frame)\n\nplt.close()\n\nanim = animation.ArtistAnimation(fig, artists, interval=1000, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nLinkage 2: Complete Linkage\nConsiders the intergroup dissimilarity to be that of the furthest (most dissimilar) pair. Also called the furthest-neighbour, Voorhees technique.\n d_{CL}(C_1, C_2) = \\max _ {i \\in C_1 \\\\ j \\in C_2} d_{ij}  \n\ndef complete(cluster_1, cluster_2):\n  complete_linkage_val = 0\n\n  for p1 in cluster_1:\n    for p2 in cluster_2:\n\n      p1_p2_dist = np.linalg.norm(np.array(p1)-np.array(p2))\n\n      if complete_linkage_val &lt; p1_p2_dist:\n        complete_linkage_val = p1_p2_dist\n\n  return complete_linkage_val\n\n\nl_mat, Zs = agglomerative(X, complete)\ndendrogram(l_mat, leaf_label_func=lambda i: str(tuple(X[i])), leaf_rotation=-45, color_threshold=3);\n\n\n\n\nSame clustering is achieved. Can you think about cases where the results would differ?\nLet’s try to investigate the difference between these linkage metrics, by constructing the following toy dataset:\n\nX = np.array([\n    [1, 1], [2, 1],\n    [4, 1], [5, 1],\n    [1, 4], [2, 4],\n    [4, 4], [5, 4],\n])\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.axis('equal');\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\nl_mat_1, Zs_1 = agglomerative(X, single)\ndendrogram(l_mat_1, leaf_label_func=lambda i: str(tuple(X[i])), leaf_rotation=-45, color_threshold=3, ax=ax1);\nl_mat_2, Zs_2 = agglomerative(X, complete)\ndendrogram(l_mat_2, leaf_label_func=lambda i: str(tuple(X[i])), leaf_rotation=-45, color_threshold=3, ax=ax2);\n\n\n\n\nNote that the above hierarchies are not equal (even though they seem to be, visually). The ordering of the singletons (X-axis) is not the same.\nLet’s take a look at how the clusters are formed at each time step:\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))\nartists = []\n\nfor i in range(len(Zs_1)):\n  frame = []\n  frame.append(ax1.scatter(X[:, 0], X[:, 1], c=cluster_rename(Zs_1[i]), s=500))\n  frame.append(ax2.scatter(X[:, 0], X[:, 1], c=cluster_rename(Zs_2[i]), s=500))\n\n  for c in set(Zs_1[i]):\n    if sum(Zs_1[i]==c) &gt; 1:\n      frame += plot_ellipse(X[Zs_1[i]==c], ax1)\n\n  for c in set(Zs_2[i]):\n    if sum(Zs_2[i]==c) &gt; 1:\n      frame += plot_ellipse(X[Zs_2[i]==c], ax2)\n\n  artists.append(frame)\n\nplt.close()\n\nanim = animation.ArtistAnimation(fig, artists, interval=1000, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nObservations: - Single linkage has a tendence to combine observations linked by a series of close intermediate observations - chaining. - Complete linkage tend to produce compact clusters with small diameters. (Can result in elements being close to members of other clusters than to their own)\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=99, noise=0.05, random_state=170)\n\nl_mat_1, Zs_1 = agglomerative(X, single)\nl_mat_2, Zs_2 = agglomerative(X, complete)\n\n# Plot (n-1)th timestep - 2 clusters\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))\nax1.scatter(X[:, 0], X[:, 1], c=cluster_rename(Zs_1[-2]), s=50)\nax2.scatter(X[:, 0], X[:, 1], c=cluster_rename(Zs_2[-2]), s=50)\n\n&lt;matplotlib.collections.PathCollection at 0x78c0a0e12290&gt;\n\n\n\n\n\n\n\nLinkage 3: Average Linkage\nConsiders the average dissimilarity between the groups\n d_{GA}(C_1, C_2) = \\frac{1}{|C_1|*|C_2|}\\sum _ {i \\in C_1} \\sum _ {j \\in C_2} d_{ij} \n\n\n\nimage.png\n\n\n\ndef average(cluster_1, cluster_2):\n  average_linkage_val = 0\n\n  for p1 in cluster_1:\n    for p2 in cluster_2:\n\n      p1_p2_dist = np.linalg.norm(np.array(p1)-np.array(p2))\n      average_linkage_val += p1_p2_dist\n\n  return average_linkage_val/(len(cluster_1)*len(cluster_2))"
  },
  {
    "objectID": "pages/agg.html#cluster-maps",
    "href": "pages/agg.html#cluster-maps",
    "title": "Agglomerative Clustering",
    "section": "Cluster-Maps",
    "text": "Cluster-Maps\nHierarchical clustering also results in a partial ordering of our samples. This ordering can be investigated, when it is coupled along with a heat-map.\n\nimport seaborn as sns\n\niris = sns.load_dataset(\"iris\", cache=False)\nspecies = iris.pop(\"species\")\n\nlut = dict(zip(species.unique(), \"rbg\"))\nrow_colors = species.map(lut)\n\nsns.clustermap(iris, cmap=\"mako\", vmin=0, vmax=10, row_colors=row_colors, method='average')"
  }
]
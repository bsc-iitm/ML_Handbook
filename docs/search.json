[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About Our Project\nHey there, fellow learners! üöÄ We‚Äôre a bunch of ML enthusiasts on a mission to make your learning journey more exciting and practical.\nüîç What We Do We‚Äôre all about breaking down those complex ML models and datasets, helping you understand the ‚Äòwhys‚Äô and ‚Äòhows‚Äô in a fun way.\nüìö Why We‚Äôre Here Traditional courses teach the theory, but we‚Äôre here to fill in the gaps, one Colab notebook at a time.\nüéÅ What You Get Explore our user-friendly notebooks, detailed reports, and a platform designed just for you.\nJoin us in unraveling the magic of machine learning, one experiment at a time. Happy learning! ü§ñüìà"
  },
  {
    "objectID": "pages/demo.html#why",
    "href": "pages/demo.html#why",
    "title": "ML Handbook",
    "section": "Why?",
    "text": "Why?\n\nDecision trees make the following presumption about the structure of data:\n\n\n\nCan figure class out based on a series of binary questions (yes/no) on individual features\n\n\n\n\nInductive Bias:  Anything which makes an algorithm learn one pattern over another\n\n\n\n\nInductive bias of decision trees entails the use of axis-parallel splits to construct the decision boundary\nSensitive to rotations\nAlgorithm invariant to rotation?"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html",
    "href": "pages/Scaling multi-dataset multi-algo.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "href": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "href": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "href": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Datasets",
    "text": "Loading the Datasets\nIn this section, we will start by loading four distinct datasets, each with its unique characteristics. These datasets are commonly used for various classification tasks and will serve as the foundation for our comprehensive study on the impact of feature scaling on classification models.\n\n# Load the datasets\niris = load_iris()\ndigits = load_digits()\nwine = load_wine()\nbreast_cancer = load_breast_cancer()\n\n# Create DataFrames for the datasets\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\ndigits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\nbreast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n\n\n1. Iris Dataset\nDescription: The Iris dataset is a classic dataset in the field of machine learning and consists of 150 samples of iris flowers, each from one of three species: Iris setosa, Iris virginica, and Iris versicolor. There are four features‚Äîsepal length, sepal width, petal length, and petal width‚Äîmeasured in centimeters.\nUse Case: This dataset is often used for practicing classification techniques, especially for building models to distinguish between the three iris species based on their feature measurements.\n\n# Display the first few rows of iris dataset\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\n\n2. Digits Dataset\nDescription: The Digits dataset is a collection of 8x8 pixel images of handwritten digits (0 through 9). There are 1,797 samples, and each sample is an 8x8 image, resulting in 64 features. The goal is to correctly classify the digits based on these pixel values.\nUse Case: This dataset is a fundamental resource for pattern recognition and is frequently used for exploring image classification and digit recognition algorithms.\n\n\n3. Wine Dataset\nDescription: The Wine dataset comprises 178 samples of wine classified into three classes based on their cultivar. The dataset contains 13 feature variables, including measurements related to chemical composition, making it a valuable resource for wine classification tasks.\nUse Case: Wine quality prediction and classification are common applications for this dataset, as it allows for distinguishing between different wine types.\n\n# Display the first few rows of wine dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\n\n\n4. Breast Cancer Dataset\nDescription: The Breast Cancer dataset is used for breast cancer diagnosis. It includes 569 samples with 30 feature variables, primarily related to characteristics of cell nuclei present in breast cancer biopsies. The dataset is labeled to indicate whether a sample is benign or malignant.\nUse Case: This dataset is often employed for building classification models to assist in the early detection of breast cancer, aiding in medical diagnosis.\n\n# Display the first few rows of breast cancer dataset\nbreast_cancer_df.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows √ó 31 columns\n\n\n\nThe datasets contain various features related to their respective domains, with a ‚Äòtarget‚Äô column indicating the class labels."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "href": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Define a function to create noisy datasets\ndef create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n    X = dataset.data\n    y = dataset.target\n\n    rng = np.random.default_rng(seed=random_state)\n    noise = rng.normal(0, noise_std, size=X.shape)\n    X_noisy = X + noise\n\n    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n\n    return X_train_noisy, X_test_noisy, y_train, y_test\n\n# Create noisy datasets for all four datasets\nX_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\nX_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\nX_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\nX_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "href": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Define a function to apply Standard Scaler to a dataset\ndef apply_standard_scaler(X_train, X_test):\n    standard_scaler = StandardScaler()\n    X_train_scaled = standard_scaler.fit_transform(X_train)\n    X_test_scaled = standard_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Standard Scaler to all four datasets\nX_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Define a function to apply Min-max Scaler to a dataset\ndef apply_min_max_scaler(X_train, X_test):\n    min_max_scaler = MinMaxScaler()\n    X_train_scaled = min_max_scaler.fit_transform(X_train)\n    X_test_scaled = min_max_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Min-max Scaler to all four datasets\nX_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Define a function to apply Maximum Absolute Scaler to a dataset\ndef apply_max_abs_scaler(X_train, X_test):\n    max_abs_scaler = MaxAbsScaler()\n    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n    X_test_scaled = max_abs_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Maximum Absolute Scaler to all four datasets\nX_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Define a function to apply Robust Scaler to a dataset\ndef apply_robust_scaler(X_train, X_test):\n    robust_scaler = RobustScaler()\n    X_train_scaled = robust_scaler.fit_transform(X_train)\n    X_test_scaled = robust_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Robust Scaler to all four datasets\nX_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Define a function to apply Quantile Transformer to a dataset\ndef apply_quantile_transformer(X_train, X_test):\n    quantile_transformer = QuantileTransformer(output_distribution='normal')\n    X_train_scaled = quantile_transformer.fit_transform(X_train)\n    X_test_scaled = quantile_transformer.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Quantile Transformer to all four datasets\nX_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "href": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of six classification models on the different scaled datasets. The models we will use are:\n\nRandom Forest\nSupport Vector Machine (SVM)\nDecision Tree\nNaive Bayes (GaussianNB)\nK-Nearest Neighbors (KNN)\nLogistic Regression\n\nFor each scaling method, we will train and evaluate all six models for all four datasets.\n\n# Initialize the classifiers\nrf_classifier = RandomForestClassifier(random_state=42)\nsvm_classifier = SVC(random_state=42)\ndt_classifier = DecisionTreeClassifier(random_state=42)\nnb_classifier = GaussianNB()\nknn_classifier = KNeighborsClassifier()\nlr_classifier = LogisticRegression()\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each dataset and scaling method, and evaluate the models\ndatasets = [\n    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n]\n\nscaling_methods = {\n    \"No Scaling\": {\n        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n    },\n    \"Standard Scaler\": {\n        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n    },\n    \"Min-max Scaler\": {\n        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n    },\n    \"Maximum Absolute Scaler\": {\n        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n    },\n    \"Robust Scaler\": {\n        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n    },\n    \"Quantile Transformer\": {\n        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n    }\n}\n\n# Loop through datasets and scaling methods\nfor dataset_name, X_train, X_test, y_train, y_test in datasets:\n    for scaler_name, scaled_data in scaling_methods.items():\n        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n\n        # Train and evaluate all six models\n        for classifier, classifier_name in zip([rf_classifier, svm_classifier, dt_classifier, nb_classifier, knn_classifier, lr_classifier], [\"Random Forest\", \"SVM\", \"Decision Tree\", \"Naive Bayes\", \"K-Nearest Neighbors\", \"Logistic Regression\"]):\n            classifier.fit(X_train_scaled, y_train)\n            predictions = classifier.predict(X_test_scaled)\n            accuracy = accuracy_score(y_test, predictions)\n            accuracy_scores.append([dataset_name, scaler_name, classifier_name, accuracy])"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "href": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet‚Äôs analyze the results of our experiment and discuss the impact of different scaling methods on multiple classification models for each dataset.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Classifier', 'Accuracy'])\nresults_df\n\n# Create a new DataFrame to display the results\nresults_pivoted_df = results_df.pivot(index=['Dataset', 'Scaling Method'], columns='Classifier', values='Accuracy')\nresults_pivoted_df.reset_index(inplace=True)\nresults_pivoted_df.columns.name = None  # Remove the column name\n\n# Fill any missing values with NaN (for clarity)\nresults_pivoted_df.fillna(value=np.nan, inplace=True)\n\nresults_pivoted_df\n\n\n\n\n\n\n\n\nDataset\nScaling Method\nDecision Tree\nK-Nearest Neighbors\nLogistic Regression\nNaive Bayes\nRandom Forest\nSVM\n\n\n\n\n0\nBreast Cancer\nMaximum Absolute Scaler\n0.938596\n0.903509\n0.929825\n0.956140\n0.964912\n0.929825\n\n\n1\nBreast Cancer\nMin-max Scaler\n0.938596\n0.912281\n0.964912\n0.956140\n0.964912\n0.956140\n\n\n2\nBreast Cancer\nNo Scaling\n0.938596\n0.956140\n0.973684\n0.956140\n0.964912\n0.947368\n\n\n3\nBreast Cancer\nQuantile Transformer\n0.938596\n0.956140\n0.964912\n0.947368\n0.964912\n0.956140\n\n\n4\nBreast Cancer\nRobust Scaler\n0.938596\n0.964912\n0.964912\n0.956140\n0.964912\n0.973684\n\n\n5\nBreast Cancer\nStandard Scaler\n0.938596\n0.938596\n0.964912\n0.956140\n0.964912\n0.964912\n\n\n6\nDigits\nMaximum Absolute Scaler\n0.858333\n0.983333\n0.961111\n0.905556\n0.972222\n0.983333\n\n\n7\nDigits\nMin-max Scaler\n0.858333\n0.988889\n0.963889\n0.905556\n0.972222\n0.983333\n\n\n8\nDigits\nNo Scaling\n0.858333\n0.986111\n0.966667\n0.905556\n0.972222\n0.991667\n\n\n9\nDigits\nQuantile Transformer\n0.858333\n0.966667\n0.941667\n0.900000\n0.969444\n0.975000\n\n\n10\nDigits\nRobust Scaler\n0.858333\n0.819444\n0.963889\n0.905556\n0.972222\n0.913889\n\n\n11\nDigits\nStandard Scaler\n0.858333\n0.975000\n0.977778\n0.905556\n0.972222\n0.986111\n\n\n12\nIris\nMaximum Absolute Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n13\nIris\nMin-max Scaler\n0.933333\n0.966667\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n14\nIris\nNo Scaling\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n15\nIris\nQuantile Transformer\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n16\nIris\nRobust Scaler\n0.933333\n0.933333\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n17\nIris\nStandard Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.966667\n\n\n18\nWine\nMaximum Absolute Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n19\nWine\nMin-max Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n20\nWine\nNo Scaling\n0.916667\n0.722222\n0.972222\n1.000000\n1.000000\n0.805556\n\n\n21\nWine\nQuantile Transformer\n0.916667\n0.944444\n1.000000\n0.972222\n1.000000\n0.972222\n\n\n22\nWine\nRobust Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n23\nWine\nStandard Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# Define a list of scaling methods and their Material Design colors\nscaling_methods = results_pivoted_df['Scaling Method'].unique()\nmaterial_colors = sns.color_palette(\"Set2\")\ndatasets = results_pivoted_df['Dataset'].unique()\n\n# Set Seaborn style for a modern, beautiful look\nsns.set_style(\"whitegrid\")\n\n# Create grouped bar charts for each dataset\nfor dataset in datasets:\n    plt.figure(figsize=(12, 8))\n    \n    # Filter the data for the current dataset\n    dataset_data = results_pivoted_df[results_pivoted_df['Dataset'] == dataset]\n    \n    # Define the x-axis labels (ML algorithms)\n    ml_algorithms = dataset_data.columns[2:].tolist()\n    \n    bar_width = 0.12  # Width of each bar\n    index = range(len(ml_algorithms))\n\n    # Create a bar for each scaling method\n    for i, method in enumerate(scaling_methods):\n        try:\n            accuracies = dataset_data[dataset_data['Scaling Method'] == method].values[0][2:]\n            plt.bar([x + i * bar_width for x in index], accuracies, bar_width, label=method, color=material_colors[i])\n        except IndexError:\n            print(f\"No data found for {method} in {dataset}\")\n    \n    # Set the x-axis labels, title, and legend\n    plt.xlabel('ML Algorithms')\n    plt.ylabel('Accuracy')\n    plt.title(f'Accuracy Comparison for {dataset}')\n    plt.xticks([x + bar_width * (len(scaling_methods) / 2) for x in index], ml_algorithms)\n    plt.legend(scaling_methods, title='Scaling Method')\n\n    # Adjust layout to prevent overlapping x-axis labels\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    # Show the plot or save it as an image\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of Results\nThe evaluation of results is based on the performance of six classification algorithms across different datasets and scaling methods. The accuracy scores are presented for Decision Tree, K-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Random Forest, and Support Vector Machine (SVM). Here‚Äôs an analysis of the findings:\n\nBreast Cancer Dataset\n\nMaximum Absolute Scaler: This scaling method produced competitive accuracy scores for all algorithms. Naive Bayes and Logistic Regression achieved the highest accuracy of approximately 95.6%, while other algorithms also performed well.\nMin-max Scaler: Similar to the Maximum Absolute Scaler, this method yielded strong accuracy results across all algorithms, with Logistic Regression and SVM reaching the highest scores of 96.4% and 95.6%, respectively.\nNo Scaling: Surprisingly, this dataset demonstrated that some algorithms, particularly Naive Bayes and Logistic Regression, do not benefit from feature scaling. They achieved high accuracy without any scaling, indicating that the original feature values were suitable for these models.\nQuantile Transformer: The Quantile Transformer showed consistent accuracy, with Logistic Regression and SVM achieving the highest scores of 96.4% and 95.6%, respectively.\nRobust Scaler: Robust scaling led to competitive accuracy for most algorithms, with SVM achieving the highest accuracy of 97.4%.\nStandard Scaler: Standard scaling demonstrated similar results to other scaling methods, with Logistic Regression and SVM achieving the highest accuracy of 96.4%.\n\n\n\nDigits Dataset\n\nMaximum Absolute Scaler: This scaling method had a positive impact on KNN, which achieved a high accuracy of approximately 98.3%. However, Decision Tree and Logistic Regression showed lower performance.\nMin-max Scaler: Min-max scaling improved the accuracy of KNN to nearly 98.9%. Other algorithms also benefited from this scaling.\nNo Scaling: Surprisingly, the Digits dataset, particularly for KNN, demonstrated that feature scaling is not necessary for achieving high accuracy. KNN reached 99.2% accuracy without any scaling.\nQuantile Transformer: While other algorithms performed well with this scaling method, Decision Tree and KNN showed slightly reduced accuracy.\nRobust Scaler: Robust scaling did not benefit KNN, with its accuracy dropping to 81.9%. Other algorithms showed consistent performance.\nStandard Scaler: Standard scaling improved the accuracy of KNN to 98.6%, making it one of the best-performing algorithms for this dataset.\n\n\n\nIris Dataset\n\nMaximum Absolute Scaler: Scaling had minimal impact on the accuracy of algorithms for the Iris dataset. SVM achieved the highest accuracy of 96.7%, regardless of scaling.\nMin-max Scaler: Similar to Maximum Absolute Scaling, min-max scaling had a limited effect on the accuracy of algorithms. SVM consistently achieved the highest accuracy of 96.7%.\nNo Scaling: The Iris dataset was naturally well-scaled, and most algorithms, particularly SVM, achieved high accuracy without any scaling.\nQuantile Transformer: Scaling had little influence on accuracy. SVM remained the best-performing algorithm, with an accuracy of 96.7%.\nRobust Scaler: Robust scaling slightly improved the accuracy of Decision Tree and SVM but had limited impact overall.\nStandard Scaler: Standard scaling resulted in consistent accuracy for all algorithms, with SVM maintaining the highest accuracy of 96.7%.\n\n\n\nWine Dataset\n\nMaximum Absolute Scaler: This scaling method had a substantial impact on SVM, boosting its accuracy to 100%. Other algorithms also reached high accuracy levels.\nMin-max Scaler: Min-max scaling had a similar effect on SVM, resulting in perfect accuracy. Decision Tree, KNN, and Logistic Regression also reached maximum accuracy.\nNo Scaling: The Wine dataset revealed the significance of scaling, particularly for SVM. Without scaling, SVM‚Äôs accuracy was relatively low at 80.6%, highlighting the sensitivity of SVM to feature values.\nQuantile Transformer: Quantile transformation improved the accuracy of Decision Tree and Logistic Regression. However, SVM remained sensitive to scaling.\nRobust Scaler: Robust scaling had a positive impact, with SVM reaching perfect accuracy. Other algorithms also performed well.\nStandard Scaler: Standard scaling had a similar effect to other scaling methods, with SVM achieving perfect accuracy.\n\n\n\n\nConclusion\nIn summary, the impact of feature scaling on machine learning algorithms varies depending on the dataset and the algorithm used:\n\nSome datasets, like the Iris dataset, are naturally well-scaled, and most algorithms perform consistently well without any scaling.\nFeature scaling, particularly min-max and maximum absolute scaling, has a positive impact on algorithms in datasets like Breast Cancer and Digits, resulting in improved accuracy.\nThe Wine dataset demonstrated that certain algorithms, notably SVM, are highly sensitive to feature scaling. Without proper scaling, SVM‚Äôs performance can be significantly compromised.\nSurprisingly, some algorithms, such as Naive Bayes and Logistic Regression, performed well without any scaling in the Breast Cancer dataset, indicating that the original feature values were suitable for these models.\n\nIn practice, it‚Äôs essential to consider the characteristics of the dataset and the algorithm‚Äôs sensitivity to feature values when deciding whether to apply feature scaling. While scaling can improve the performance of many machine learning algorithms, there are cases where it may not be necessary and could even have a negligible or detrimental effect on model accuracy."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "href": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling",
    "text": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling\nIn the context of our machine learning analysis, it‚Äôs fascinating to observe that Naive Bayes and tree-based algorithms, such as Decision Trees and Random Forests, exhibit remarkable resilience to feature scaling. This resilience stems from the inherent characteristics of these algorithms and their method of decision-making.\n\nNaive Bayes Classifier\nNaive Bayes is a probabilistic algorithm that‚Äôs based on the Bayes‚Äô theorem. It operates under the ‚Äúnaive‚Äù assumption that features are conditionally independent, given the class label. This fundamental assumption simplifies the calculations and often leads to surprisingly good classification results, especially in text and categorical data analysis.\nThe reason why Naive Bayes remains largely unaffected by feature scaling is twofold:\n\nProbabilistic Nature: Naive Bayes calculates probabilities based on the distribution of features within each class. The relative scaling of individual features does not impact the probability ratios significantly. In other words, as long as the relationships between features and classes remain consistent, the algorithm can adapt to different feature scales.\nNormalization in Probability Calculation: When computing probabilities, Naive Bayes often involves normalizing terms. This means that even if feature values are on different scales, the normalization process effectively scales them down to a common scale during probability calculations.\n\n\n\nDecision Trees and Random Forests\nDecision Trees and their ensemble counterpart, Random Forests, are non-parametric algorithms that make decisions by recursively splitting data based on feature values. They are highly interpretable and capable of capturing complex relationships within the data.\nThe key reasons why Decision Trees and Random Forests are generally insensitive to feature scaling include:\n\nSplitting Criteria: Decision Trees make decisions based on feature values relative to certain thresholds. The order or magnitude of these thresholds doesn‚Äôt affect the decision-making process. The algorithm focuses on finding the most discriminative features and their optimal split points.\nEnsemble Nature (Random Forests): Random Forests combine multiple Decision Trees. The ensemble nature of Random Forests further reduces sensitivity to feature scaling. When individual trees make errors due to scaling, the ensemble tends to compensate for them.\nImpurity Measures: Decision Trees use impurity measures like Gini impurity and entropy to determine the quality of a split. These measures are based on class proportions within a split and are independent of feature scales."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we‚Äôll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let‚Äôs explore the world of data together!"
  },
  {
    "objectID": "pages/Inductive_Bias.html",
    "href": "pages/Inductive_Bias.html",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "",
    "text": "Slides: Click here!"
  },
  {
    "objectID": "pages/Inductive_Bias.html#decision-trees",
    "href": "pages/Inductive_Bias.html#decision-trees",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "Decision Trees",
    "text": "Decision Trees\nPopular representation for interpretable classifiers; even among humans!\nExample: I‚Äôve just arrived at a restaurant. Should I stay (wait for a table) or go elsewhere?\nOne may choose to use the following set of rules to make their decision: \nsource: ai.berkeley.edu\nDecision trees: - Have a simple Design - Interpretable - Easy to implement - Good performance in practice\nNote that splits happen individually at the feature level - corresponds to splits parallel to a feature axis - Inductive bias\nInductive bias: anything which makes the algorithm learn one pattern instead of another pattern.\nDecision trees use a step-function collection for classification; but these step functions utilize one feature/variable only. Is this phenomenon sensitive to the nature of the dataset?\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\n\nDTree = DecisionTreeClassifier()\nDTree.fit(X, y)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax = ax1)\ndisp.ax_.scatter(X[:, 0][y==0], X[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Tree Depth: {DTree.get_depth()}');\n\nplot_tree(DTree, label='none', filled=True, feature_names=['F1', 'F2'], class_names=['Red', 'Blue'], node_ids=False, rounded=True, impurity=False, ax=ax2);\n\n\n\n\nThe 4x4 checkerboard dataset with alternating classes requires a tree of depth=7 to capture its structure respectively.\nBut what will happen if we try to train a tree on the rotated variant of this dataset?\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nDTree_rotated = DecisionTreeClassifier()\nDTree_rotated.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Overfit) Tree Depth: {DTree_rotated.get_depth()}')\n\nDTree_rotated_constrained = DecisionTreeClassifier(max_depth=7)\nDTree_rotated_constrained.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated_constrained, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax2)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Constrained) Tree Depth: {DTree_rotated_constrained.get_depth()}');\n\n\n\n\nThe model fails to understand the generation rationale of the dataset as it suffers an inductive bias of axis-parallel splitting.\n\nKNN\nExamine the performance of KNN (with neighbors=3) on both variants of the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(knn, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X[:, 0][y==0], X[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\n\nknn_rotated = KNeighborsClassifier(n_neighbors=3)\nknn_rotated.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(knn_rotated, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax2)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x79d1e140c640&gt;\n\n\n\n\n\nThe rotation performed does not impact the performance of KNN.\nWhat is the inductive bias in KNN then? To investigate, we construct the following dataset\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (14, 6.3)\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams['lines.markersize'] = 4.2\n\nnp.random.seed(0)\nclass1_mean = [0, 0.25]\nclass1_cov = [[100000, 0], [0, 0.01]]\nclass1_data = np.random.multivariate_normal(class1_mean, class1_cov, 100)\n\nclass2_mean = [0, -0.25]\nclass2_cov = [[100000, 0], [0, 0.01]]\nclass2_data = np.random.multivariate_normal(class2_mean, class2_cov, 100)\n\nplt.scatter(class1_data[:, 0], class1_data[:, 1], c='b', label='Class 1', edgecolor='k', alpha=0.5)\nplt.scatter(class2_data[:, 0], class2_data[:, 1], c='r', label='Class 2', edgecolor='k', alpha=0.5)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = np.concatenate([class1_data, class2_data])\ny = np.array([0 for _ in range(100)] + [1 for _ in range(100)])\n\nX_scaled = StandardScaler().fit_transform(X)\ny_scaled = y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_scaled_train, X_scaled_test, y_scaled_train, y_scaled_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3).fit(X, y)\nknn2 = KNeighborsClassifier(n_neighbors=3).fit(X_scaled, y)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\ndisp = DecisionBoundaryDisplay.from_estimator(knn, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X[:, 0][y==0], X[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'KNN | Score: {knn.score(X, y)}')\n\ndisp = DecisionBoundaryDisplay.from_estimator(knn2, X_scaled, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax2)\ndisp.ax_.scatter(X_scaled[:, 0][y_scaled==0], X_scaled[:, 1][y_scaled==0], color='red', label='y_scaled==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_scaled[:, 0][y_scaled==1], X_scaled[:, 1][y_scaled==1], color='blue', label='y_scaled==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'KNN-Scaled | Score: {knn2.score(X_scaled, y_scaled)}');\n\n\n\n\nWe observe that scaling impacts the performance on the dataset. This reveals the inductive bias for KNN:\nThe algorithm assumes that entities belonging to a particular category should appear near each other, and those that are part of different groups should be distant.\nHere even though the seperation is evident, the scaling makes this phenomenon invisible to the knn classifier; hence the model does not capture this structure in the dataset.\nIf through context we are confident that our dataset has an underlying linear seperation, we could use the Perceptron algorithm\n\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron(alpha=0, max_iter=int(1e6), tol=None).fit(X, y)\nperceptron_scaled = Perceptron(alpha=0, max_iter=int(1e6), tol=None).fit(X_scaled, y_scaled)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\ndisp = DecisionBoundaryDisplay.from_estimator(perceptron, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X[:, 0][y==0], X[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Perceptron | Score: {perceptron.score(X, y)}')\n\ndisp = DecisionBoundaryDisplay.from_estimator(perceptron_scaled, X_scaled, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax2)\ndisp.ax_.scatter(X_scaled[:, 0][y_scaled==0], X_scaled[:, 1][y_scaled==0], color='red', label='y_scaled==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_scaled[:, 0][y_scaled==1], X_scaled[:, 1][y_scaled==1], color='blue', label='y_scaled==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Perceptron-Scaled | Score: {perceptron_scaled.score(X_scaled, y_scaled)}');\n\n\n\n\nPerceptron algorithm: - Is a linear Classifier - Simple update rule: on mistake; add/subtract datapoint - Shown to converge only on linearly seperable datasets with non-zero margin (radius-margin bound) - Inductive Bias due to assumption of underlying structure of data\nWhat about non-linear, say, quadratic seperability? Consider the following dataset:\n\nnp.random.seed(1)\n\nX_ = np.random.rand(100, 2)\nX_ = (X_-np.mean(X_))*10\n\nX, y = [], []\n\nperp = lambda i: -1*i[0]/i[1]\nsign = lambda i: 2*int(i &gt;= 0)-1\n\nsep = lambda x: x[1]**2-8*x[1]*x[0]+2*x[0]**2\n\nw = np.array([1, 1])/np.sqrt(2)\ngamma = 0.5\n\nfor p in X_:\n  d = sep(p)\n  if abs(d) &gt;= gamma:\n    X.append(p)\n    y.append(sign(d))\n\nX = np.array(X)\ny = np.array(y)\n\nfig, (ax2) = plt.subplots(1, 1)\nfig.suptitle(f'A non-linearly seperable dataset with Œ≥={gamma} margin')\n\nax2.scatter(X[:, 0][y==1], X[:, 1][y==1], color='green', label='Positive')\nax2.scatter(X[:, 0][y!=1], X[:, 1][y!=1], color='red', label='Negative')\nax2.legend(loc='lower right')\n\nax2.axvline(x=0, c='black')\nax2.axhline(y=0, c='black')\n\nplt.show()\n\n\n\n\nThe above dataset has a seperator corresponding to a second order function of the features.\nTransform the dataset and apply perceptron! Alter inductive bias to our advantage\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Perceptron\n\npoly_perceptron = make_pipeline(PolynomialFeatures(2), Perceptron(alpha=0, max_iter=int(1e6), tol=None))\npoly_perceptron.fit(X, y)\n\nfig, (ax1) = plt.subplots(1, 1)\ndisp = DecisionBoundaryDisplay.from_estimator(poly_perceptron, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X[:, 0][y==-1], X[:, 1][y==-1], color='red', label='y==-1', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Poly-Perceptron | Score: {poly_perceptron.score(X, y)}');"
  },
  {
    "objectID": "pages/Deliverable_1.html",
    "href": "pages/Deliverable_1.html",
    "title": "Dealing with missing values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#introduction",
    "href": "pages/Deliverable_1.html#introduction",
    "title": "Dealing with missing values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#bias",
    "href": "pages/Deliverable_1.html#bias",
    "title": "Dealing with missing values",
    "section": "Bias",
    "text": "Bias\nWhen data is missing, the remaining data may not always accurately reflect the true population. The type of missingness and the way we deal with it can dictate bias in our results.\nFor example, in a survey recording income of individuals, those with low income may choose not to respond. The use of average income to impute the missing values can lead to bias as the average of the available data may not represent that of the population."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "href": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "title": "Dealing with missing values",
    "section": "Missing Completely At Random (MCAR)",
    "text": "Missing Completely At Random (MCAR)\nIn this case, the data is missing randomly and is not related to any variable in the dataset or to the missing value themselves. The probability of missingness is same for all observations. There exists no underlying pattern to the missingness. The missing values are completely independent of other data.\nFor example, during data collection, if some responses were not collected due to a technical error, then the missing data is completely at random.\nAll statistical analysis performed on the dataset will remain unbiased in this case."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-at-random-mar",
    "href": "pages/Deliverable_1.html#missing-at-random-mar",
    "title": "Dealing with missing values",
    "section": "Missing At Random (MAR)",
    "text": "Missing At Random (MAR)\nIn this case, the missingness of the data can be fully accounted for by the other known data values. Here there exists some form of pattern in the missing values.\nFor example, In a survey, women might be unwilling to disclose their age. Here the missingness of the variable age can be explained by another observed variable ‚Äúgender‚Äù."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "href": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "title": "Dealing with missing values",
    "section": "Missing Not At Random (MNAR)",
    "text": "Missing Not At Random (MNAR)\nIn this case, the missingness is neither MCAR nor MAR. The fact that a datapoint is missing is dependent on the value of the data point. In order to correct for the bias we would have to make modelling assumptions about the nature of the bias.\nFor example, in a social survey where individuals are asked about their income, respondnets may not disclose it if it is too high or too low. Thus the missingness in the feature income is directly related to the values of that feature."
  },
  {
    "objectID": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "href": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "title": "Dealing with missing values",
    "section": "Identifying the type of missingness",
    "text": "Identifying the type of missingness\n\nUnderstanding the data collection process, the features involved and the research domain can help identify possible patterns as to why data is missing\nGraphical representation of the missing data and heatmaps can help identify relationships between features that can be utilized to make better imputation of the missing data\nWe can impute the data using different techniques while also making assumptions on the nature of missingness. Subsequently, we analyze the results to observe the assumptions that have led to consistent results."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-value-imputation",
    "href": "pages/Deliverable_1.html#missing-value-imputation",
    "title": "Dealing with missing values",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\nIn many cases it might not be feasible to discard observations that contain missing values. This could be due to the availability of lesser number of samples or due to the importance of each observation. In such cases we can employ imputation which deals with replacing the missing values with some predicted value. We will have a look at two simple imputation techniques that can be implemented using the sklearn package.\n\nSimple Imputer\nK Nearest neighbours Imputer"
  },
  {
    "objectID": "pages/Deliverable_1.html#simple-imputer",
    "href": "pages/Deliverable_1.html#simple-imputer",
    "title": "Dealing with missing values",
    "section": "Simple Imputer",
    "text": "Simple Imputer\nThe Simple Imputation technique offers a basic approach to filling missing data wherein we replace the missing data with a constant value or utilize statistics such as ‚Äúmean‚Äù, ‚Äúmode‚Äù, or ‚Äúmedian‚Äù of the available values. The technique is useful for its simplicity and can serve as a reference technique. It is also worth noting that this can lead to biased or unrealistic results\n\nSimple Imputer - Mean\n\nimport numpy as np\nimport pandas as pd\n\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mean strategy. The mean value of that feature will be \\frac{( 1+1+2+4)}{4} = 2\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 2 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\ndata = {\"feature_1\":[1,2,3,4,5],\n        \"feature_2\":[1,1,np.nan,2,4],\n        \"feature_3\":[3,3,3,4,5]}\n\n\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1\n1.0\n3\n\n\n1\n2\n1.0\n3\n\n\n2\n3\nNaN\n3\n\n\n3\n4\n2.0\n4\n\n\n4\n5\n4.0\n5\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom sklearn.impute import SimpleImputer\n\n\nimputer = SimpleImputer(strategy='mean')\nimputed_data = imputer.fit_transform(df)\n\n\nimputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nimputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n2.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSimple Imputer - Mode\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mode strategy. The value that appears most often is 1.\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nmode_imputer = SimpleImputer(strategy='most_frequent')\nimputed_data = mode_imputer.fit_transform(df)\n\n\nmode_imputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nmode_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/Deliverable_1.html#k-nearest-neighbours",
    "href": "pages/Deliverable_1.html#k-nearest-neighbours",
    "title": "Dealing with missing values",
    "section": "K Nearest Neighbours",
    "text": "K Nearest Neighbours\nThis technique is an extension of the KNN classifier we have seen in MLT to perform imputation. In this technique we identify the points that are similar to the observation we wish to impute based on the available features. We can then use the values of these neighboring points fill in the missing values\n\nfrom sklearn.impute import KNNImputer\n\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nknn_imputed_data = knn_imputer.fit_transform(df)\n\n\nknn_imputed_df = pd.DataFrame(knn_imputed_data, columns=['feature_1','feature_2','feature_3'])\nknn_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.5\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  }
]
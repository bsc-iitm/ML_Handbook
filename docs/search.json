[
  {
    "objectID": "pages/Scaling story.html",
    "href": "pages/Scaling story.html",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "",
    "text": "In the mystical realm of machine learning, feature scaling is a vital ritual performed by data scientists. This ritual, often considered as a preprocessing step, involves transforming the data to a common scale to ensure that no single feature dominates the learning process. In the absence of this ritual, models with distance-based algorithms can get skewed by features with larger scales.\nIn this tale, we shall journey through the impact of various feature scaling methods on classification models, focusing on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nOur adventure will revolve around the Wine dataset from scikit-learn, a commonly employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/Scaling story.html#prologue",
    "href": "pages/Scaling story.html#prologue",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "",
    "text": "In the mystical realm of machine learning, feature scaling is a vital ritual performed by data scientists. This ritual, often considered as a preprocessing step, involves transforming the data to a common scale to ensure that no single feature dominates the learning process. In the absence of this ritual, models with distance-based algorithms can get skewed by features with larger scales.\nIn this tale, we shall journey through the impact of various feature scaling methods on classification models, focusing on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nOur adventure will revolve around the Wine dataset from scikit-learn, a commonly employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/Scaling story.html#chapter-1-gathering-the-tools",
    "href": "pages/Scaling story.html#chapter-1-gathering-the-tools",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 1: Gathering the Tools",
    "text": "Chapter 1: Gathering the Tools\nTo embark on our journey, we first need to gather the necessary tools. In our case, these tools are various libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling story.html#chapter-2-unveiling-the-wine-dataset",
    "href": "pages/Scaling story.html#chapter-2-unveiling-the-wine-dataset",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 2: Unveiling the Wine Dataset",
    "text": "Chapter 2: Unveiling the Wine Dataset\nOur adventure begins with the Wine dataset. Let’s load the dataset and inspect its structure.\n\n# Load the Wine dataset\nwine = load_wine()\n\n# Create a DataFrame for the dataset\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\n\n# Display the first few rows of the dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\nThe Wine dataset consists of various features related to wine properties and a ‘target’ column indicating the class of the wine."
  },
  {
    "objectID": "pages/Scaling story.html#chapter-3-preparing-for-the-journey-data-preprocessing",
    "href": "pages/Scaling story.html#chapter-3-preparing-for-the-journey-data-preprocessing",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 3: Preparing for the Journey (Data Preprocessing)",
    "text": "Chapter 3: Preparing for the Journey (Data Preprocessing)\nBefore we proceed with feature scaling, we need to split the data into training and testing sets. To make our journey more challenging and interesting, we will create a noisy version of the Wine dataset by adding random noise to the feature values. This noisy dataset will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Split the data into features (X) and target (y)\nX = wine.data\ny = wine.target\n\n# Adding random noise to the dataset\nnp.random.seed(42)\nnoise = np.random.normal(0, 0.2, size=X.shape)\nX_noisy = X + noise\n\n# Split the noisy data into training and testing sets\nX_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "pages/Scaling story.html#chapter-4-the-many-faces-of-feature-scaling",
    "href": "pages/Scaling story.html#chapter-4-the-many-faces-of-feature-scaling",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 4: The Many Faces of Feature Scaling",
    "text": "Chapter 4: The Many Faces of Feature Scaling\nIn this chapter, we will explore five different feature scaling methods and their mathematical foundations.\n\n1. Standard Scaler\nThe Standard Scaler, also known as Z-score normalization, transforms the data so that it has a mean (\\(\\mu\\)) of 0 and a standard deviation (\\(\\sigma\\)) of 1. The transformation is defined by the equation:\n\\[z = \\frac{x - \\mu}{\\sigma}\\]\nThis method assumes that the data is normally distributed. If the data is not normally distributed, this scaler could distort the data distribution, leading to suboptimal results.\n\n# Initialize the Standard Scaler\nstandard_scaler = StandardScaler()\n\n# Fit and transform the training data\nX_train_standard = standard_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_standard = standard_scaler.transform(X_test_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler, also known as normalization, scales the data to a specific range, typically between 0 and 1. The transformation is defined by the equation:\n\\[x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}\\]\nIt is suitable for data that does not follow a normal distribution. However, it is sensitive to outliers.\n\n# Initialize the Min-max Scaler\nmin_max_scaler = MinMaxScaler()\n\n# Fit and transform the training data\nX_train_minmax = min_max_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_minmax = min_max_scaler.transform(X_test_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. The transformation is defined by the equation:\n\\[x_{scaled} = \\frac{x}{|x_{max}|}\\]\nIt does not distort the data and keeps zero values at zero, making it suitable for sparse data.\n\n# Initialize the Maximum Absolute Scaler\nmax_abs_scaler = MaxAbsScaler()\n\n# Fit and transform the training data\nX_train_maxabs = max_abs_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_maxabs = max_abs_scaler.transform(X_test_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler scales the data using the median and the interquartile range (IQR), making it robust to outliers. The transformation is defined by the equation:\n\\[x_{scaled} = \\frac{x - Q1}{Q3 - Q1}\\]\nWhere \\(Q1\\) and \\(Q3\\) are the first and third quartiles, respectively.\n\n# Initialize the Robust Scaler\nrobust_scaler = RobustScaler()\n\n# Fit and transform the training data\nX_train_robust = robust_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_robust = robust_scaler.transform(X_test_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (\\(QT\\)) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\\[\nQT(x) = F^{-1}(F(x))\n\\]\nwhere \\(F(x)\\) is the cumulative distribution function of the data, and \\(F^{-1}\\) is the inverse function of \\(F\\).\n\n# Initialize the Quantile Transformer\nquantile_transformer = QuantileTransformer(output_distribution='normal')\n\n# Fit and transform the training data\nX_train_quantile = quantile_transformer.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_quantile = quantile_transformer.transform(X_test_noisy)"
  },
  {
    "objectID": "pages/Scaling story.html#chapter-5-the-battle-of-classification-models",
    "href": "pages/Scaling story.html#chapter-5-the-battle-of-classification-models",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 5: The Battle of Classification Models",
    "text": "Chapter 5: The Battle of Classification Models\nIn this chapter, we will witness the battle of two classification models, Random Forest and Support Vector Machine (SVM), as they compete on the different scaled datasets. For each scaling method, we will train and evaluate both models.\n\n# Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Initialize the SVM classifier\nsvm_classifier = SVC(random_state=42)\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each scaled dataset and evaluate the models\nfor X_train_scaled, X_test_scaled, scaler_name in zip(\n    [X_train_noisy, X_train_standard, X_train_minmax, X_train_maxabs, X_train_robust, X_train_quantile],\n    [X_test_noisy, X_test_standard, X_test_minmax, X_test_maxabs, X_test_robust, X_test_quantile],\n    [\"No Scaling\", \"Standard Scaler\", \"Min-max Scaler\", \"Maximum Absolute Scaler\", \"Robust Scaler\", \"Quantile Transformer\"]\n):\n    # Train the Random Forest model\n    rf_classifier.fit(X_train_scaled, y_train)\n    rf_predictions = rf_classifier.predict(X_test_scaled)\n    \n    # Train the SVM model\n    svm_classifier.fit(X_train_scaled, y_train)\n    svm_predictions = svm_classifier.predict(X_test_scaled)\n    \n    # Calculate accuracy scores for both models\n    rf_accuracy = accuracy_score(y_test, rf_predictions)\n    svm_accuracy = accuracy_score(y_test, svm_predictions)\n    \n    # Store the accuracy scores for comparison\n    accuracy_scores.append([scaler_name, rf_accuracy, svm_accuracy])"
  },
  {
    "objectID": "pages/Scaling story.html#chapter-6-the-aftermath-results-and-discussion",
    "href": "pages/Scaling story.html#chapter-6-the-aftermath-results-and-discussion",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 6: The Aftermath (Results and Discussion)",
    "text": "Chapter 6: The Aftermath (Results and Discussion)\nThe battle is over. Let’s analyze the aftermath of our experiment and discuss the impact of different scaling methods on the classification models.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Scaling Method', 'Random Forest Accuracy', 'SVM Accuracy'])\nresults_df\n\n\n\n\n\n\n\n\nScaling Method\nRandom Forest Accuracy\nSVM Accuracy\n\n\n\n\n0\nNo Scaling\n1.0\n0.805556\n\n\n1\nStandard Scaler\n1.0\n1.000000\n\n\n2\nMin-max Scaler\n1.0\n1.000000\n\n\n3\nMaximum Absolute Scaler\n1.0\n1.000000\n\n\n4\nRobust Scaler\n1.0\n1.000000\n\n\n5\nQuantile Transformer\n1.0\n1.000000"
  },
  {
    "objectID": "pages/Scaling story.html#chapter-7-interpreting-the-aftermath-evaluation-of-results",
    "href": "pages/Scaling story.html#chapter-7-interpreting-the-aftermath-evaluation-of-results",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 7: Interpreting the Aftermath (Evaluation of Results)",
    "text": "Chapter 7: Interpreting the Aftermath (Evaluation of Results)\nThe results of our grand experiment have been revealed. It’s time to interpret what they mean for our journey.\nThe output from the notebook provides accuracy scores for two classification models, Random Forest and Support Vector Machine (SVM), using different feature scaling methods. Here’s a summary of the results:\n\nNo Scaling: With no scaling applied, Random Forest achieved perfect accuracy (1.0), while SVM achieved an accuracy of approximately 0.8056. This shows that even without scaling, some models like Random Forest can perform well. However, SVM, being a distance-based algorithm, suffered due to the lack of scaling.\nStandard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer: For all these scaling methods, both Random Forest and SVM achieved perfect accuracy (1.0). This indicates that these scaling methods worked exceptionally well for the Wine dataset, improving the SVM’s performance significantly compared to when no scaling was applied."
  },
  {
    "objectID": "pages/Scaling story.html#chapter-8-pondering-the-implications-discussion",
    "href": "pages/Scaling story.html#chapter-8-pondering-the-implications-discussion",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 8: Pondering the Implications (Discussion)",
    "text": "Chapter 8: Pondering the Implications (Discussion)\nThe results of our experiment have provided us with some valuable insights into the impact of different feature scaling methods on classification models. Here are some key observations:\n\nNo Scaling: Some models, like Random Forest, can handle unscaled data well. However, for distance-based algorithms like SVM, scaling is crucial to achieve optimal performance.\nStandard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer: All these scaling methods led to perfect accuracy for both Random Forest and SVM. This demonstrates their effectiveness in ensuring all features contribute equally to the model’s learning process.\n\nThe results also underscore the importance of understanding the mathematical foundations of each scaling method. Each method has its strengths, weaknesses, and assumptions, which can influence its effectiveness on different datasets and models."
  },
  {
    "objectID": "pages/Scaling story.html#chapter-9-the-moral-of-the-story-conclusion",
    "href": "pages/Scaling story.html#chapter-9-the-moral-of-the-story-conclusion",
    "title": "The Tale of Feature Scaling and Its Influence on Classification Models",
    "section": "Chapter 9: The Moral of the Story (Conclusion)",
    "text": "Chapter 9: The Moral of the Story (Conclusion)\nAs our tale draws to a close, the moral of the story becomes clear: the choice of feature scaling method is a crucial decision in the context of classification models. The impact of scaling methods on performance can vary significantly, as seen in our experiment.\nWhen working with real-world datasets, it’s essential to experiment with different scaling techniques and select the one that aligns with the data’s distribution and the requirements of the machine learning model. This decision should be guided by a thorough understanding of the data’s characteristics and the mathematical foundations of the scaling methods.\nOur experiment also highlights the importance of feature scaling as a preprocessing step and the need to consider the specific scaling method in the broader context of machine learning tasks. It reminds us that in the realm of machine learning, every decision, no matter how small, can have far-reaching implications.\nAnd so, our tale ends here, but the lessons we learned will guide us in our future adventures in the vast and mysterious realm of machine learning. Until next time!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we’ll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let’s explore the world of data together!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we’ll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let’s explore the world of data together!"
  },
  {
    "objectID": "pages/Scaling.html",
    "href": "pages/Scaling.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use the Wine dataset from scikit-learn, a frequently employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/Scaling.html#introduction",
    "href": "pages/Scaling.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use the Wine dataset from scikit-learn, a frequently employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/Scaling.html#importing-necessary-libraries",
    "href": "pages/Scaling.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling.html#loading-the-wine-dataset",
    "href": "pages/Scaling.html#loading-the-wine-dataset",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Wine Dataset",
    "text": "Loading the Wine Dataset\nWe start by loading the Wine dataset and inspecting its structure.\n\n# Load the Wine dataset\nwine = load_wine()\n\n# Create a DataFrame for the dataset\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\n\n# Display the first few rows of the dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\nThe Wine dataset consists of various features related to wine properties and a ‘target’ column indicating the class of the wine."
  },
  {
    "objectID": "pages/Scaling.html#data-preprocessing",
    "href": "pages/Scaling.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data into training and testing sets. Additionally, to make our study more robust and thorough, we will create a noisy version of the Wine dataset by adding random noise to the feature values. This noisy dataset will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Split the data into features (X) and target (y)\nX = wine.data\ny = wine.target\n\n# Adding random noise to the dataset\nnp.random.seed(42)\nnoise = np.random.normal(0, 0.2, size=X.shape)\nX_noisy = X + noise\n\n# Split the noisy data into training and testing sets\nX_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "pages/Scaling.html#feature-scaling-methods",
    "href": "pages/Scaling.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Initialize the Standard Scaler\nstandard_scaler = StandardScaler()\n\n# Fit and transform the training data\nX_train_standard = standard_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_standard = standard_scaler.transform(X_test_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Initialize the Min-max Scaler\nmin_max_scaler = MinMaxScaler()\n\n# Fit and transform the training data\nX_train_minmax = min_max_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_minmax = min_max_scaler.transform(X_test_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Initialize the Maximum Absolute Scaler\nmax_abs_scaler = MaxAbsScaler()\n\n# Fit and transform the training data\nX_train_maxabs = max_abs_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_maxabs = max_abs_scaler.transform(X_test_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Initialize the Robust Scaler\nrobust_scaler = RobustScaler()\n\n# Fit and transform the training data\nX_train_robust = robust_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_robust = robust_scaler.transform(X_test_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Initialize the Quantile Transformer\nquantile_transformer = QuantileTransformer(output_distribution='normal')\n\n# Fit and transform the training data\nX_train_quantile = quantile_transformer.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_quantile = quantile_transformer.transform(X_test_noisy)"
  },
  {
    "objectID": "pages/Scaling.html#classification-models",
    "href": "pages/Scaling.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of two classification models, Random Forest and Support Vector Machine (SVM), on the different scaled datasets. For each scaling method, we will train and evaluate both models.\n\n# Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Initialize the SVM classifier\nsvm_classifier = SVC(random_state=42)\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each scaled dataset and evaluate the models\nfor X_train_scaled, X_test_scaled, scaler_name in zip(\n    [X_train_noisy, X_train_standard, X_train_minmax, X_train_maxabs, X_train_robust, X_train_quantile],\n    [X_test_noisy, X_test_standard, X_test_minmax, X_test_maxabs, X_test_robust, X_test_quantile],\n    [\"No Scaling\", \"Standard Scaler\", \"Min-max Scaler\", \"Maximum Absolute Scaler\", \"Robust Scaler\", \"Quantile Transformer\"]\n):\n    # Train the Random Forest model\n    rf_classifier.fit(X_train_scaled, y_train)\n    rf_predictions = rf_classifier.predict(X_test_scaled)\n    \n    # Train the SVM model\n    svm_classifier.fit(X_train_scaled, y_train)\n    svm_predictions = svm_classifier.predict(X_test_scaled)\n    \n    # Calculate accuracy scores for both models\n    rf_accuracy = accuracy_score(y_test, rf_predictions)\n    svm_accuracy = accuracy_score(y_test, svm_predictions)\n    \n    # Store the accuracy scores for comparison\n    accuracy_scores.append([scaler_name, rf_accuracy, svm_accuracy])"
  },
  {
    "objectID": "pages/Scaling.html#results-and-discussion",
    "href": "pages/Scaling.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet’s analyze the results of our experiment and discuss the impact of different scaling methods on classification models.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Scaling Method', 'Random Forest Accuracy', 'SVM Accuracy'])\nresults_df\n\n\n\n\n\n\n\n\nScaling Method\nRandom Forest Accuracy\nSVM Accuracy\n\n\n\n\n0\nNo Scaling\n1.0\n0.805556\n\n\n1\nStandard Scaler\n1.0\n1.000000\n\n\n2\nMin-max Scaler\n1.0\n1.000000\n\n\n3\nMaximum Absolute Scaler\n1.0\n1.000000\n\n\n4\nRobust Scaler\n1.0\n1.000000\n\n\n5\nQuantile Transformer\n1.0\n1.000000"
  },
  {
    "objectID": "pages/Scaling.html#evaluation-of-results",
    "href": "pages/Scaling.html#evaluation-of-results",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Evaluation of Results",
    "text": "Evaluation of Results\nThe output from the notebook provides accuracy scores for two classification models, Random Forest and Support Vector Machine (SVM), using different feature scaling methods. Here’s a summary of the results:\n\nNo Scaling: Without any scaling, the Random Forest model achieved perfect accuracy (1.0), while the SVM model’s accuracy was significantly lower (approximately 0.8056). This disparity demonstrates the influence of feature scaling on SVM, which is sensitive to the range of feature values.\nStandard Scaler: The Standard Scaler, which assumes a normal distribution of data, yielded perfect accuracy (1.0) for both models. This indicates that the features in the Wine dataset are likely normally distributed, and the scaling effectively standardized the data, leading to improved SVM performance.\nMin-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer: These methods also resulted in perfect accuracy (1.0) for both models. These results demonstrate that scaling the data to a specific range (Min-max Scaler and Maximum Absolute Scaler), making the scaling robust to outliers (Robust Scaler), or applying a non-linear transformation to map data to a uniform or normal distribution (Quantile Transformer) can significantly improve the performance of SVM. It’s worth noting that the Random Forest’s performance remained consistently high regardless of the scaling method, which is consistent with its insensitivity to the scale of features."
  },
  {
    "objectID": "pages/Scaling.html#conclusion",
    "href": "pages/Scaling.html#conclusion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, this notebook provides a comprehensive study on the impact of feature scaling on classification models. It demonstrates that the choice of feature scaling method can significantly influence the performance of a model, especially for models like SVM that are sensitive to the range of feature values.\nWithout scaling, SVM’s performance was significantly lower compared to other methods. However, with the application of different scaling methods, SVM’s performance improved drastically, achieving perfect accuracy. This highlights the importance of feature scaling in preprocessing, particularly when using models sensitive to the scale of input features.\nThe Standard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer all proved to be effective for the noisy Wine dataset. However, the effectiveness of a scaling method can vary based on the characteristics of the dataset and the specific machine learning model being used.\nWhen working with real-world datasets, it’s essential to experiment with different scaling techniques and select the one that best fits the data’s distribution and the requirements of the machine learning model. This decision should be data-driven and based on a thorough understanding of the data’s characteristics.\nThis experiment underscores the importance of feature scaling as a preprocessing step and the need to consider the specific scaling method in the broader context of machine learning tasks."
  },
  {
    "objectID": "pages/Scaling.html#reference",
    "href": "pages/Scaling.html#reference",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Reference",
    "text": "Reference\n\n“The choice of scaling technique matters for classification performance” by Lucas B.V. de Amorima, b, George D.C. Cavalcantia, Rafael M.O. Cruz"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About Our Project\nHey there, fellow learners! üöÄ We‚Äôre a bunch of ML enthusiasts on a mission to make your learning journey more exciting and practical.\nüîç What We Do We‚Äôre all about breaking down those complex ML models and datasets, helping you understand the ‚Äòwhys‚Äô and ‚Äòhows‚Äô in a fun way.\nüìö Why We‚Äôre Here Traditional courses teach the theory, but we‚Äôre here to fill in the gaps, one Colab notebook at a time.\nüéÅ What You Get Explore our user-friendly notebooks, detailed reports, and a platform designed just for you.\nJoin us in unraveling the magic of machine learning, one experiment at a time. Happy learning! ü§ñüìà"
  },
  {
    "objectID": "pages/EM_Algorithm.html",
    "href": "pages/EM_Algorithm.html",
    "title": "EM Algorithm",
    "section": "",
    "text": "Colab: Click here!"
  },
  {
    "objectID": "pages/EM_Algorithm.html#introduction-of-latent-variable",
    "href": "pages/EM_Algorithm.html#introduction-of-latent-variable",
    "title": "EM Algorithm",
    "section": "Introduction of Latent Variable",
    "text": "Introduction of Latent Variable\nWe introduce a K-dimensional latent variable \\mathbf{z}; only one of the elements of \\mathbf{z} will be 1 (1-of-K encoding) (also a standard basis vector).\n\\mathbf{z} = \\mathbf{e}_k\n\nReformulating problem in terms of latent variable \\mathbf{z}\nWe model the marginal distribuon over \\mathbf{z} using our mixing coefficients \\pi_k:\np(\\mathbf{z}=\\mathbf{e}_k) = \\pi_k\nThe conditional distribution of \\mathbf{x} over \\mathbf{z} can be similarly modelled:\np(\\mathbf{x} | \\mathbf{z}=\\mathbf{e}_k) = \\mathcal{N}(\\mathbf{x}|\\mu_k, Œ£_k)\nWe then obtain p(\\mathbf{x}) by summing the joint distribution over all possible \\mathbf{z} states:\np(x) = \\sum_\\mathbf{z}p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) = \\sum_{k=1}^K\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Œ£_k)\nAnd done! We have formulated our goal using this latent variable \\mathbf{z}. One more quantity of use is the conditional probablity of \\mathbf{z}=\\mathbf{e}_k given \\mathbf{x}_i (which we will call \\lambda_ik):\n\\lambda_{ik} \\equiv p(\\mathbf{z}=\\mathbf{e}_k) = \\frac{p(\\mathbf{z}=\\mathbf{e}_k)p(\\mathbf{x}|\\mathbf{z}_k=\\mathbf{e}_k)}{\\sum_{j=1}^K p(\\mathbf{z}=\\mathbf{e}_j)p(\\mathbf{x}|\\mathbf{z}_j=\\mathbf{e}_j)} = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Œ£_k)}{\\sum_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j, Œ£_j)}\nBy the reformulation above, we have enabled the application of EM on our problem.\n$$$$ ### Let‚Äôs begin!\nTo get started, we perform the following initialization: - Pick 2 points at random as cluster centres - Hard-assign points based on proximity to centres\n\nK = 2\n\nrng = np.random.default_rng(seed=12)\ncentres = X[rng.integers(low=0, high=len(X), size=K)]\nl = np.array([(lambda i: [int(i==j) for j in range(K)])(np.argmin([np.linalg.norm(p-centre) for centre in centres])) for p in X])\npi = l.sum(axis=0)/l.sum()\n\ncov = []\nfor i in range(K):\n  cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\ncov = np.array(cov)\n\ndef plot():\n\n  plt.scatter(X[:, 0], X[:, 1], color='black', marker='+')\n  plt.scatter(centres[:, 0], centres[:, 1], color='red', marker='x')\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  plt.contour(M, N, probability_grid)\n  plt.show()\n\nplot()"
  },
  {
    "objectID": "pages/EM_Algorithm.html#posing-a-maximum-likelihood-problem",
    "href": "pages/EM_Algorithm.html#posing-a-maximum-likelihood-problem",
    "title": "EM Algorithm",
    "section": "Posing a Maximum Likelihood problem:",
    "text": "Posing a Maximum Likelihood problem:\nThe log of the likelihood is the following:\n\\ln p(\\mathbf{X}|\\pi, \\mu, Œ£) = \\sum_{n=1}^N\\ln \\left\\{{\\sum_{k=1}^K\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mu_k, \\Sigma_k)}\\right\\}\n\nMaximization step\nTaking derivate of above equation with \\mu_k and setting to zero yields the following:\n\n0 = -\\sum_{n=1}^N \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^K\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mu_j, \\Sigma_j)} Œ£_k(\\mathbf{x}_n-\\mu_k) ‚â° -\\sum_{n=1}^N \\lambda_{nk}Œ£_k(\\mathbf{x}_n-\\mu_k)\n\nMultiplying both sides by Œ£_k^{-1} (non-singular; invertible) and rearranging, we get:\n\\mathbf{\\mu}_k = \\frac{1}{N_k}\\sum_{n=1}^{N}\\lambda_{nk}\\mathbf{x}_n\nWhere we define N_k to be the effective number of points assigned to cluster k: N_k = \\sum_{n=1}^N \\lambda_{nk}\nWe follow a similar approach and derive the following ML estimates for \\Sigma_k and \\pi_k:\nŒ£_k = \\frac{1}{N_k}\\sum_{n=1}^N\\lambda_{nk}(\\mathbf{x}_n-\\mathbf{\\mu}_k)(\\mathbf{x}_n-\\mathbf{\\mu}_k)^T\n\\\\\\pi_k = \\frac{N_k}{N}\n\n\n\nExpectation step\nRecompute \\lambda using the parameter values. Forumula mentioned again for completion sake:\n\\lambda_{ik} = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Œ£_k)}{\\sum_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j, Œ£_j)}\n\n# Expectation Step\ndef Max(l):\n  pi = l.sum(axis=0)/l.sum()\n  centres, cov = [], []\n  for i in range(K):\n    centres.append(np.dot(l[:, i], X)/l[:, i].sum())\n    cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\n\n  return pi, np.array(centres), np.array(cov)\n\n# Maximization step\ndef Exp(pi, centres, cov):\n  l = []\n  for i in X:\n    p = np.array([pi[k] * multivariate_normal.pdf(i, mean=centres[k], cov=cov[k]) for k in range(K)])\n    p = p/p.sum()\n    l.append(p)\n\n  return np.array(l)\n\n# Convergence criterion\nnorm_theta = lambda pi, centres, cov: np.linalg.norm(np.r_[pi, centres.reshape(-1), cov.reshape(-1)])\n\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\nprev_norm = norm_theta(pi, centres, cov)\n\nfig, ax = plt.subplots()\nartists = []\n\nwhile True:\n  frame = []\n  frame.append(ax.scatter(X[:, 0], X[:, 1], color='black', marker='+'))\n  frame.append(ax.scatter(centres[:, 0], centres[:, 1], color='red', marker='x'))\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  frame += list(ax.contour(M, N, probability_grid).collections)\n  artists.append(frame)\n\n  l = Exp(pi, centres, cov)\n  pi, centres, cov = Max(l)\n\n  curr_norm = norm_theta(pi, centres, cov)\n\n  if abs(curr_norm-prev_norm) &lt; 0.00001:\n    break\n  else:\n    prev_norm = curr_norm\n\nplt.close()\nanim = animation.ArtistAnimation(fig, artists, interval=200, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nRelation to K-Means?\nThere is a very close similarity. In fact K-Means is a restricted GMM clustering (initalize all Œ£_k‚Äôs to œµ\\mathbf{I}, with œµ ‚Üí 0, and do not update in maximization step)\nHow does the above make it K-Means? We investigate \\lambda_{ik}:\n\\lambda_{ik} = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k, Œ£_k)}{\\sum_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j, Œ£_j)} = \\frac{\\pi_k\\exp \\{-||\\mathbf{x}_i-\\mathbf{\\mu}_k||^2/2œµ\\}}{\\sum_{j=1}^K \\pi_j\\exp \\{-||\\mathbf{x}_i-\\mathbf{\\mu}_j||^2/2œµ\\}}\nLet \\phi = \\arg \\min f(j) = ||\\mathbf{x}_i-\\mathbf{\\mu}_j||^2. Setting œµ ‚Üí 0, we see that in the denominator the Œ¶‚Äôth term goes to zero the slowest. Hence \\lambda_{n\\phi} ‚Üí 1 while the others ‚Üí 0 (note that this results in a hard-clustering).\nThe above is equivalent to the K-means clustering paradigm; assign clusters based on proximity from cluster centers.\n\n# (Restricted) Maximization Step\ndef KM_Max(l):\n  pi = l.sum(axis=0)/l.sum()\n  centres, cov = [], []\n  for i in range(K):\n    centres.append(np.dot(l[:, i], X)/l[:, i].sum())\n    cov.append(eps*np.eye(l.shape[1]))\n\n  return pi, np.array(centres), np.array(cov)\n\nK = 2\neps = 0.005\n\nrng = np.random.default_rng(seed=72)\ncentres = X[rng.integers(low=0, high=len(X), size=K)]\nl = np.array([(lambda i: [int(i==j) for j in range(K)])(np.argmin([np.linalg.norm(p-centre) for centre in centres])) for p in X])\npi = l.sum(axis=0)/l.sum()\n\ncov = []\nfor i in range(K):\n  cov.append(eps*np.eye(l.shape[1]))\ncov = np.array(cov)\n\ndef plot():\n\n  plt.scatter(X[:, 0], X[:, 1], color='black', marker='+')\n  plt.scatter(centres[:, 0], centres[:, 1], color='red', marker='x')\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  plt.contour(M, N, probability_grid)\n  plt.show()\n\nplot()\n\n\n\n\n\na, b, c = KM_Max(l)\n[[round(i, 5) for i in j] for j in Exp(a, b, c)[:5]]\n\n[[0.99911, 0.00089], [0.0, 1.0], [0.00082, 0.99918], [0.0, 1.0], [1.0, 0.0]]\n\n\nWe see that the assignments are close to hard-clustering. Setting epsilon to a much smaller value will ensure this better.\nFor visual presentability, epsilon has been set as small as possible.\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\nprev_norm = norm_theta(pi, centres, cov)\n\nfig, ax = plt.subplots()\nartists = []\n\nwhile True:\n\n  frame = []\n  frame.append(ax.scatter(X[:, 0], X[:, 1], color='black', marker='+'))\n  frame.append(ax.scatter(centres[:, 0], centres[:, 1], color='red', marker='x'))\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  frame += list(ax.contour(M, N, probability_grid).collections)\n  artists.append(frame)\n\n  l = Exp(pi, centres, cov)\n  pi, centres, cov = KM_Max(l)\n\n  curr_norm = norm_theta(pi, centres, cov)\n\n  if abs(curr_norm-prev_norm) &lt; 0.0001:\n    print(curr_norm-prev_norm)\n    break\n  else:\n    prev_norm = curr_norm\n\nplt.close()\nanim = animation.ArtistAnimation(fig, artists, interval=1000, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n-7.535850650119968e-05\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "pages/vis2.html",
    "href": "pages/vis2.html",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/vis2.html#introduction",
    "href": "pages/vis2.html#introduction",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Introduction",
    "text": "Introduction\nData visualization is an essential tool in the field of data analysis and interpretation. It allows us to gain insights from complex data by representing it in a visual format. In this Jupyter notebook, we will explore various data visualization techniques using Matplotlib and Seaborn, two popular Python libraries. These techniques cater to the needs of Computer Science and Data Science students, helping them understand and utilize visualization methods effectively."
  },
  {
    "objectID": "pages/vis2.html#table-of-contents",
    "href": "pages/vis2.html#table-of-contents",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIntroduction\nBasic Plots\n\nLine Plot\nScatter Plot\nBar Plot\nHistogram\n\nStatistical Plots\n\nBox Plot\nViolin Plot\nSwarm Plot\n\nMatrix Plots\n\nHeatmap\nClustermap\n\nDistribution Plots\n\nKDE Plot\nPair Plot\n\nTime Series Plots\n\nTime Series Plot\nAutoCorrelation Plot\n\nGeospatial Data Visualization\n\nScatter Geo Plot\nChoropleth Map\n\n3D Plots\n\n3D Scatter Plot\n3D Line Plot\n\nSpecialized Plots\n\nPolar Plot\nNetwork Plot\nWord Cloud\n\nAdvanced Data Visualization\n\nROC Curves and AUC\nt-SNE Plots\n\nConclusion\nAdditional Notes"
  },
  {
    "objectID": "pages/vis2.html#basic-plots",
    "href": "pages/vis2.html#basic-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "1: Basic Plots",
    "text": "1: Basic Plots\nIn this section, we will delve into a comprehensive exploration of basic data visualization techniques, collectively known as ‚ÄúBasic Plots.‚Äù These fundamental visualizations are crucial for understanding data trends, relationships, and distributions. We will cover Line Plots, Scatter Plots, Bar Plots, and Histograms, each offering a unique perspective on data representation.\n\n1.1: Line Plot (Visualizing Trends Over Time)\nLine plots are a fundamental tool for visualizing data trends, particularly those that evolve over time. In this subsection, we will use a synthetic time-series dataset, such as stock market data, to illustrate the significance of line plots.\nCreating a Line Plot:\nWe will begin by generating synthetic time-series data, including time points and corresponding stock prices. Then, we will use Matplotlib to craft an informative line plot.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic time-series data\ntime = np.arange(0, 10, 0.1)\nstock_prices = np.sin(time) + np.random.normal(0, 0.2, len(time))\n\n# Creating a line plot\nplt.figure(figsize=(10, 6))\nplt.plot(time, stock_prices, label='Stock Prices', color='b', linestyle='-', marker='o')\nplt.xlabel('Time')\nplt.ylabel('Stock Prices')\nplt.title('Stock Price Trends Over Time')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe resulting line plot provides a visual representation of stock price trends over time. It offers customization options such as line style, color, and labels to enhance clarity.\nInterpreting Line Plots:\nInterpreting a line plot involves assessing various aspects:\n\nTrends: Observe the direction of the line to identify upward, downward, or stable trends in the data.\nAmplitude: The vertical distance of the line from the baseline signifies the magnitude of changes in the variable being measured.\nCyclic Patterns: Some time-series data exhibit cyclic patterns or seasonality, which can be spotted in the plot.\nVariability: Variations in the data are reflected in the fluctuations of the line.\n\nLine plots are essential for detecting temporal patterns, understanding data evolution, and making informed decisions based on historical data.\n\n\n1.2: Scatter Plot (Visualizing Relationships Between Variables)\nScatter plots are valuable for visualizing the relationships between two numeric variables. In this subsection, we will use synthetic data representing height vs.¬†weight to demonstrate the utility of scatter plots.\nCreating a Scatter Plot:\nWe will generate synthetic height and weight data and then employ Matplotlib to create a comprehensive scatter plot.\n\n# Generating synthetic height vs. weight data\nheight = np.random.normal(170, 10, 100)\nweight = height * 0.6 + np.random.normal(0, 5, 100)\n\n# Creating a scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(height, weight, label='Height vs. Weight', color='r', marker='o')\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')\nplt.title('Relationship between Height and Weight')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe scatter plot visually illustrates the relationship between height and weight, allowing for the identification of patterns and correlations.\nInterpreting Scatter Plots:\nInterpreting a scatter plot involves considering several key aspects:\n\nTrend Direction: Determine if the points exhibit an upward, downward, or random trend.\nScatter Density: The density of points in different areas of the plot indicates data concentration.\nOutliers: Identify any data points that deviate significantly from the general pattern, which might be outliers.\nCorrelation: Assess the overall direction and strength of the relationship between the variables.\n\nScatter plots are essential for understanding the correlation between two variables and identifying potential outliers or trends.\n\n\n1.3: Bar Plot (Visualizing Categorical Data)\nBar plots are instrumental for representing categorical data. In this subsection, we will use synthetic sales data by product category to demonstrate the effectiveness of bar plots.\nCreating a Bar Plot:\nWe will generate synthetic sales data categorized by product type and then create a bar plot using Matplotlib.\n\n# Generating synthetic sales data by product category\ncategories = ['Electronics', 'Clothing', 'Books', 'Home Decor']\nsales = [1200, 800, 1500, 900]\n\n# Creating a bar plot\nplt.figure(figsize=(10, 6))\nplt.bar(categories, sales, color='g', alpha=0.7)\nplt.xlabel('Product Categories')\nplt.ylabel('Sales')\nplt.title('Sales by Product Category')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe bar plot visually represents the sales data by product category, offering insights into categorical data representation.\nInterpreting Bar Plots:\nInterpreting a bar plot involves considering the following aspects:\n\nCategory Comparison: Compare the heights of bars to understand variations in sales among different categories.\nCategorical Representation: Observe how categories are represented on the x-axis.\nColor Usage: Color can be utilized to highlight specific categories or add visual appeal to the plot.\nStacked vs.¬†Grouped Bars: Depending on the data representation, bars can be stacked or grouped for better comprehension.\n\nBar plots are essential for comparing categorical data and understanding the distribution of values within categories.\n\n\n1.4: Histogram (Visualizing Data Distribution)\nHistograms are powerful tools for visualizing the distribution of a single variable. In this subsection, we will use synthetic exam score data to create a histogram.\nCreating a Histogram:\nWe will generate synthetic exam scores and then employ Matplotlib to create an informative histogram.\n\n# Generating synthetic exam score data\nexam_scores = np.random.normal(70, 10, 300)\n\n# Creating a histogram\nplt.figure(figsize=(10, 6))\nplt.hist(exam_scores, bins=20, color='purple', edgecolor='black', alpha=0.7)\nplt.xlabel('Exam Scores')\nplt.ylabel('Frequency')\nplt.title('Distribution of Exam Scores')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe histogram visually represents the distribution of exam scores, offering customization options for bin size and normalization.\nInterpreting Histograms:\nInterpreting a histogram involves considering several key aspects:\n\nData Distribution: Assess whether the data is normally distributed, skewed, or exhibits other patterns.\nCentral Tendency: Identify the central tendency of the data, such as the mean or median.\nDispersion: Examine the spread or variability of the data.\nBin Width: The width of histogram bins can affect the visual representation of the distribution.\n\nHistograms are essential for understanding the distribution of a single variable and identifying patterns in the data."
  },
  {
    "objectID": "pages/vis2.html#statistical-plots",
    "href": "pages/vis2.html#statistical-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "2: Statistical Plots",
    "text": "2: Statistical Plots\nIn this section, we will dive into a comprehensive exploration of statistical data visualization techniques, collectively known as ‚ÄúStatistical Plots.‚Äù These visualizations are particularly suited for gaining insights into data distributions, identifying outliers, and understanding the central tendencies and variations within datasets. We will cover Box Plots, Violin Plots, and Swarm Plots, each offering a unique perspective on data distribution and statistical characteristics.\n\n2.1: Box Plot (Visualizing Distribution Characteristics)\nBox plots, often referred to as box-and-whisker plots, are powerful tools for visualizing the distribution and central tendencies of a dataset. They provide valuable information about the quartiles, outliers, and the spread of data. To illustrate the utility of box plots, we will utilize a synthetic dataset representing income distribution.\nCreating a Box Plot:\nWe will commence by generating synthetic income distribution data and then proceed to create an informative box plot using Matplotlib.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic income distribution data\nincome_data = np.random.normal(50000, 10000, 500)\n\n# Creating a box plot\nplt.figure(figsize=(10, 6))\nplt.boxplot(income_data, vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'))\nplt.xlabel('Income')\nplt.title('Income Distribution')\nplt.grid(axis='x')\nplt.show()\n\n\n\n\nThe resulting box plot offers an intuitive representation of income distribution, where the box‚Äôs boundaries denote the interquartile range, the median is indicated by the central line, and whiskers extend to minimum and maximum values. The use of color adds an additional layer of visualization.\nInterpreting Box Plots:\nInterpreting a box plot involves analyzing several key aspects:\n\nMedian (Q2): The central line inside the box represents the median income, providing insight into the dataset‚Äôs central tendency.\nInterquartile Range (IQR): The span of the box represents the IQR, indicating the spread of data between the 25th and 75th percentiles.\nWhiskers: The whiskers extend from the box to the minimum and maximum values within the dataset, highlighting potential outliers.\nOutliers: Any data points beyond the whiskers are considered outliers, which may warrant further investigation.\n\nBox plots are valuable for comparing the distributions of different datasets and identifying variations in data characteristics.\n\n\n2.2: Violin Plot (Combining Box Plot and KDE)\nViolin plots are a hybrid of box plots and Kernel Density Estimation (KDE) plots, offering a more detailed view of data distribution. These plots are especially useful when you need to visualize the shape and density of the dataset. To demonstrate the capabilities of violin plots, we will continue using the synthetic income distribution data.\nCreating a Violin Plot:\nWe will take the income distribution data and craft a violin plot that combines the benefits of box plots and KDE to provide a richer representation.\n\n# Creating a violin plot\nplt.figure(figsize=(10, 6))\nplt.violinplot(income_data, vert=False, showmedians=True, showextrema=True)\nplt.xlabel('Income')\nplt.title('Income Distribution (Violin Plot)')\nplt.grid(axis='x')\nplt.show()\n\n\n\n\nIn the resulting plot, you can observe a combination of the classic box plot and a KDE representation, providing a more comprehensive understanding of data distribution.\nInterpreting Violin Plots:\nWhen interpreting violin plots, consider the following:\n\nWidth of the Violin: The width of the violin at any given value indicates the density of data points at that level. Wider sections represent higher data density.\nBox within the Violin: Just like in a box plot, the central box in the violin plot represents the IQR, and the central line is the median.\nViolin Extrema: The extrema, represented as small lines or points, highlight the minimum and maximum values in the dataset.\n\nViolin plots are effective for capturing both the central tendencies and the variations in data, making them a powerful tool in exploratory data analysis.\n\n\n2.3: Swarm Plot (Visualizing Categorical Data)\nSwarm plots are excellent for visualizing categorical data with multiple categories, showcasing individual data points within these categories. To exemplify the utility of swarm plots, we will employ synthetic survey response data, which is often categorical and offers a prime use case for this type of visualization.\nCreating a Swarm Plot:\nWe will generate synthetic survey response data and construct a swarm plot using the Seaborn library, which excels in creating aesthetically pleasing and informative categorical plots.\n\n# Generating synthetic survey response data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncategories = ['Category A', 'Category B', 'Category C', 'Category D']\nresponses = np.random.choice(categories, size=100)\n\n# Creating a swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x=responses, y=np.random.normal(0, 1, 100), palette='Set2', hue=responses, legend=False)\nplt.xlabel('Survey Categories')\nplt.ylabel('')\nplt.title('Survey Responses')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe resulting swarm plot showcases individual survey responses distributed along the categorical axis, revealing the distribution of data points within each category.\nInterpreting Swarm Plots:\nSwarm plots are particularly useful for:\n\nVisualizing Distribution: The positions of individual data points offer a clear view of how responses are distributed within each category.\nIdentifying Clustering: Patterns or clustering of responses within categories can be observed, aiding in the identification of trends or commonalities among responses.\n\nSwarm plots are an excellent choice when working with categorical data and seeking insights into the distribution and clustering of responses."
  },
  {
    "objectID": "pages/vis2.html#matrix-plots",
    "href": "pages/vis2.html#matrix-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "3: Matrix Plots",
    "text": "3: Matrix Plots\nMatrix plots are essential for visualizing relationships and patterns in data, particularly when dealing with multivariate datasets. This section will provide an in-depth exploration of matrix plots, focusing on Heatmaps and Clustermaps. These visualization techniques offer a comprehensive view of data interactions and similarities, aiding in the discovery of hidden insights within complex datasets.\n\n3.1: Heatmap (Visualizing Correlations)\nHeatmaps are powerful tools for visualizing correlation matrices of variables. These visualizations allow us to gain insights into how variables interact with each other, identify patterns, and assess the strength and direction of these relationships. For our demonstration, we will create a synthetic correlation matrix and generate an informative heatmap.\nCreating a Heatmap:\nWe begin by generating a synthetic correlation matrix and then proceed to create a compelling heatmap using Seaborn.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Generating a synthetic correlation matrix\ncorrelation_matrix = np.corrcoef(np.random.rand(5, 5))\n\n# Creating a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\nThe resulting heatmap visually represents correlations between variables. It uses a color map to accentuate the strength of the relationships. In this example, warmer colors indicate positive correlations, cooler colors represent negative correlations, and the annotation provides precise correlation values.\nInterpreting Heatmaps:\nInterpreting a heatmap involves analyzing the following aspects:\n\nColor Intensity: The intensity of color at the intersection of two variables signifies the strength of their correlation. Darker colors represent stronger correlations.\nColor Direction: Warm colors (e.g., red and orange) indicate positive correlations, while cool colors (e.g., blue and green) denote negative correlations.\nAnnotation: Annotation within the heatmap provides specific correlation values, enabling precise quantitative assessment.\n\nHeatmaps are instrumental in identifying significant relationships in datasets, making them invaluable in fields like finance, biology, and social sciences.\n\n\n3.2: Clustermap (Hierarchical Clustering)\nClustermaps are a specialized form of heatmap that combines data visualization with hierarchical clustering. They are exceptionally useful for grouping and ordering data based on similarity, revealing underlying structures in the dataset. Dendrograms are often employed to illustrate the clustering hierarchy.\nCreating a Clustermap:\nWe will utilize the same synthetic correlation matrix to create a clustermap, which employs hierarchical clustering to group and order data.\n\n# Creating a clustermap without specifying cbar_pos\nsns.clustermap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Clustermap of Correlation Matrix')\nplt.show()\n\n\n\n\nThe clustermap visually presents the clustered relationships among variables. It employs dendrograms to showcase the hierarchical structure within the data. By using dendrograms, the clustermap provides insights into how data points are grouped based on their similarity.\nInterpreting Clustermaps:\nInterpreting a clustermap involves focusing on the following components:\n\nDendrograms: Dendrograms in the row and column margins show the hierarchical structure of clustered data points. The closer data points are on the dendrogram, the more similar they are.\nOrdering: The order of rows and columns reflects the clustering hierarchy, allowing us to identify groups of variables with similar relationships.\n\nClustermaps are a valuable tool for identifying and visualizing patterns within datasets, making them indispensable in fields such as genomics and social network analysis. They help unveil the underlying structure of complex data, enabling informed decision-making and insightful data exploration."
  },
  {
    "objectID": "pages/vis2.html#distribution-plots",
    "href": "pages/vis2.html#distribution-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "4: Distribution Plots",
    "text": "4: Distribution Plots\nIn this section, we will delve into the realm of distribution plots, a set of visualization techniques designed to provide insights into the distribution of data. These plots are invaluable for understanding the underlying structure of datasets, exploring the shape of distributions, and detecting important statistical properties. We will explore two distribution plots: Kernel Density Estimate (KDE) Plot and Pair Plot.\n\n4.1: Kernel Density Estimate (KDE) Plot (Visualizing Probability Density)\nKernel Density Estimate (KDE) plots offer an effective means of visualizing the probability density function of a single variable. They provide a smooth representation of data distribution, allowing us to explore underlying patterns and characteristics. To illustrate the utility of KDE plots, we will use a synthetic dataset of exam scores.\nCreating a Kernel Density Estimate (KDE) Plot:\nLet‚Äôs begin by generating synthetic exam score data and then create a KDE plot using Seaborn.\n\n# Importing necessary libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic exam score data\nexam_scores = np.random.normal(75, 10, 200)\n\n# Creating a KDE plot\nplt.figure(figsize=(10, 6))\nsns.kdeplot(exam_scores, fill=True, color='orange')\nplt.xlabel('Exam Scores')\nplt.ylabel('Probability Density')\nplt.title('Kernel Density Estimate (KDE) of Exam Scores')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe resulting KDE plot provides a smooth representation of the exam scores‚Äô probability density, highlighting potential peaks and trends in the data. The shade area under the curve represents the estimated probability.\nInterpreting KDE Plots:\nInterpreting a KDE plot involves recognizing key elements:\n\nKernel Smoothness: The smoothness of the curve is determined by the choice of the kernel function. Smoother curves indicate a more generalized representation of the data.\nPeaks: Peaks in the KDE plot represent modes or significant clusters within the data. These peaks indicate areas where data points are more concentrated.\nTails: The tails of the KDE plot extend towards the data‚Äôs minimum and maximum values, providing insights into the data‚Äôs spread.\n\nKDE plots are crucial for understanding the underlying data distribution, especially when dealing with single-variable datasets.\n\n\n4.2: Pair Plot (Exploring Multivariate Relationships)\nPair plots are a powerful tool for exploring the relationships between multiple numeric variables within a dataset. They provide a comprehensive overview of variable interactions, including scatterplots, histograms, and correlation coefficients. To demonstrate the utility of pair plots, we will use a synthetic dataset with multiple features.\nCreating a Pair Plot:\nLet‚Äôs generate synthetic data with multiple numeric features and use Seaborn to create a pair plot.\n\n# Generating synthetic dataset with multiple features\nimport pandas as pd\n\ndata = pd.DataFrame({\n  'Feature1': np.random.normal(0, 1, 100),\n  'Feature2': np.random.normal(0, 1, 100),\n  'Feature3': np.random.normal(0, 1, 100),\n  'Feature4': np.random.normal(0, 1, 100)\n})\n\n# Creating a pair plot\nsns.pairplot(data)\nplt.suptitle('Pair Plot of Multiple Features')\nplt.show()\n\n\n\n\nThe resulting pair plot offers a matrix of scatterplots for pairwise variable comparisons, histograms along the diagonal, and correlation coefficients.\nInterpreting Pair Plots:\nInterpreting a pair plot involves examining various components:\n\nScatterplots: The scatterplots in the upper and lower triangles of the matrix illustrate the relationships between pairs of variables. They help identify trends and correlations.\nHistograms: The diagonal of the pair plot consists of histograms for each variable, revealing the distribution of each feature individually.\nCorrelation Coefficients: If desired, correlation coefficients can be displayed within the scatterplots, indicating the strength and direction of linear relationships.\n\nPair plots are instrumental in identifying relationships between variables, detecting outliers, and gaining insights into dataset characteristics."
  },
  {
    "objectID": "pages/vis2.html#time-series-plots",
    "href": "pages/vis2.html#time-series-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "5: Time Series Plots",
    "text": "5: Time Series Plots\nTime series data is a fundamental component of various fields, including finance, economics, and environmental sciences. Visualizing time-dependent trends is crucial for understanding patterns, making predictions, and conducting in-depth analyses. In this section, we will explore a range of time series visualization techniques that empower us to decode and interpret the dynamics of temporal data.\n\n5.1: Time Series Plot (Unveiling Temporal Trends)\nTime series plots are a go-to choice for unveiling temporal trends in data. By tracking changes over time, we can uncover patterns, fluctuations, and anomalies. For this demonstration, we will employ a synthetic time series dataset representing stock prices over time.\nCreating a Time Series Plot:\nLet‚Äôs initiate our exploration by generating a synthetic time series dataset and crafting an informative time series plot using Matplotlib.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic time series data\ntime = np.arange(0, 10, 0.1)\nstock_prices = np.sin(time) + np.random.normal(0, 0.2, len(time))\n\n# Creating a time series plot\nplt.figure(figsize=(10, 6))\nplt.plot(time, stock_prices, label='Stock Prices', color='b', linestyle='-', marker='o')\nplt.xlabel('Time')\nplt.ylabel('Stock Prices')\nplt.title('Stock Price Trends Over Time')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nThe resulting time series plot beautifully illustrates stock price trends over time. This visualization is instrumental for detecting long-term trends, seasonal patterns, and short-term fluctuations in time series data.\nInterpreting Time Series Plots:\nInterpreting time series plots involves analyzing various aspects:\n\nTrends: Examining the overall direction of the time series to identify upward, downward, or stationary trends.\nSeasonality: Detecting recurring patterns or cycles within the data, which may occur daily, weekly, monthly, or seasonally.\nVolatility: Observing the degree of variability in the data, which is crucial for risk assessment and financial analysis.\nAnomalies: Identifying unusual data points that deviate significantly from the expected patterns.\n\nTime series plots are foundational for analyzing historical data and can guide decision-making in areas such as investment and resource allocation.\n\n\n5.2: AutoCorrelation Plot (Unmasking Time-Dependent Dependencies)\nAutoCorrelation plots are essential tools for unveiling time-dependent dependencies in time series data. They help us understand the relationship between a time series and its past observations. In this demonstration, we will utilize a synthetic time series dataset representing monthly sales data.\nCreating an AutoCorrelation Plot:\nTo illustrate the concept of auto-correlation, we will generate synthetic monthly sales data and craft an informative auto-correlation plot using Matplotlib.\n\n# Generating synthetic monthly sales data\nmonths = np.arange(1, 13)\nmonthly_sales = np.sin(months) + np.random.normal(0, 0.2, 12)\n\n# Creating an auto-correlation plot\nplt.figure(figsize=(10, 6))\npd.plotting.autocorrelation_plot(monthly_sales)\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Plot of Monthly Sales')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\nThe resulting auto-correlation plot unveils insights into the temporal dependencies within the monthly sales data. It is instrumental for identifying seasonal patterns, lags, and potential predictive features.\nInterpreting AutoCorrelation Plots:\nInterpreting auto-correlation plots involves examining several key components:\n\nLags: On the x-axis, the lag represents the number of time periods between observations. It helps identify time-dependent relationships.\nAutocorrelation Values: The y-axis displays autocorrelation values, which indicate the strength and direction of the relationship. Peaks and valleys in this plot reveal time-dependent patterns.\nSeasonality: Peaks at regular intervals in the auto-correlation plot suggest the presence of seasonal patterns. The width of these peaks may reveal the season‚Äôs duration.\n\nAuto-correlation plots are indispensable for understanding the time-dependent dynamics of data, identifying seasonality, and guiding the selection of appropriate forecasting models."
  },
  {
    "objectID": "pages/vis2.html#geospatial-data-visualization",
    "href": "pages/vis2.html#geospatial-data-visualization",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "6: Geospatial Data Visualization",
    "text": "6: Geospatial Data Visualization\nIn this section, we will embark on an in-depth exploration of geospatial data visualization, a crucial domain for understanding and interpreting data in geographic contexts. Geospatial data visualization techniques enable us to represent data with latitude and longitude coordinates, visualize patterns in geographical data, and gain insights into the distribution and relationships of spatial data points. We will cover Scatter Geo Plots and Choropleth Maps, each offering a unique perspective on geospatial data representation.\n\n6.1: Scatter Geo Plot (Mapping Data Points)\nScatter Geo Plots are instrumental in mapping data points with latitude and longitude coordinates onto a geographical map. This visualization technique allows us to observe the spatial distribution of data, identify clusters, and understand the geographical patterns within a dataset.\nCreating a Scatter Geo Plot:\nTo exemplify the usage of Scatter Geo Plots, we will generate synthetic data representing earthquake locations worldwide and visualize them using the scatter geo plot.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic earthquake location data\nlatitude = np.random.uniform(-90, 90, 100)\nlongitude = np.random.uniform(-180, 180, 100)\n\n# Creating a scatter geo plot\nplt.figure(figsize=(12, 8))\nplt.scatter(longitude, latitude, color='red', alpha=0.7)\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Earthquake Locations')\nplt.grid()\nplt.show()\n\n\n\n\nThe scatter geo plot provides a visual representation of earthquake locations, with latitude on the y-axis and longitude on the x-axis. Each data point on the map signifies the location of an earthquake event, and the color (red) distinguishes the data points for easier identification.\nInterpreting Scatter Geo Plots:\nInterpreting a scatter geo plot involves:\n\nSpatial Distribution: Observing the distribution of data points across the geographical area. Clusters or patterns may indicate regions with a higher concentration of events.\nOutliers: Identifying isolated data points that deviate significantly from the main cluster, which may denote unique or extreme events.\nGeographic Relationships: Understanding the relationships between data points based on their geographic proximity.\n\nScatter Geo Plots are vital for understanding and analyzing geospatial data, making them invaluable for applications such as seismology, epidemiology, and environmental studies.\n\n\n6.2: Choropleth Map (Color-Coded Data Distribution)\nChoropleth Maps are a powerful visualization tool for representing geographic data in regions or administrative boundaries. They use color gradients to depict variations in data values across different geographic areas. Choropleth Maps are instrumental for understanding regional disparities, population distributions, and data patterns at a macroscopic level.\nCreating a Choropleth Map:\nTo illustrate the application of Choropleth Maps, we will utilize synthetic population data by country and generate a color-coded map to visualize population distribution.\n\n# Importing necessary libraries\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Loading the 'naturalearth_lowres' dataset\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n# Generating synthetic population data by country\ncountries = ['India', 'USA', 'China', 'Russia']\npopulation = [1400, 330, 1440, 145]\n\n# Merging the synthetic population data with the geospatial data\nworld['Population'] = 0  # Initialize the 'Population' column with zeros\n\n# Populate the 'Population' column with synthetic data\nfor country, pop in zip(countries, population):\n    world.loc[world['name'] == country, 'Population'] = pop\n\n# Creating a choropleth map\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nworld.boundary.plot(ax=ax, linewidth=1)\nworld.plot(column='Population', cmap='YlOrRd', ax=ax, legend=True)\nplt.title('Population by Country')\nplt.show()\n\n/tmp/ipykernel_88883/2624646566.py:6: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n  world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\n\n\n\nIn this example, we load a world map with country boundaries and overlay it with color-coded regions based on population. The color intensity reflects population density, allowing us to visualize variations in population across different countries.\nInterpreting Choropleth Maps:\nInterpreting a choropleth map involves:\n\nColor Gradients: Understanding the color spectrum used to represent data values. Darker colors typically denote higher values, while lighter colors indicate lower values.\nRegional Patterns: Observing variations in data distribution across different regions. Darker regions indicate higher population or data values.\nGeographic Trends: Identifying regional trends, disparities, or clusters within the dataset.\n\nChoropleth Maps are indispensable for visualizing data associated with geographic regions, and they find extensive use in fields such as demographics, economics, and public health."
  },
  {
    "objectID": "pages/vis2.html#d-plots-visualizing-three-dimensional-data",
    "href": "pages/vis2.html#d-plots-visualizing-three-dimensional-data",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "7: 3D Plots (Visualizing Three-Dimensional Data)",
    "text": "7: 3D Plots (Visualizing Three-Dimensional Data)\nIn this section, we will embark on an exploration of three-dimensional (3D) data visualization techniques. Visualizing data in three dimensions allows us to understand complex relationships and patterns that cannot be effectively represented in two dimensions. We will cover two fundamental 3D plot types: 3D Scatter Plots and 3D Line Plots.\n\n7.1: 3D Scatter Plot (Visualizing Data Clusters in 3D Space)\n3D scatter plots are a valuable tool for visualizing data with three numeric variables. They enable us to explore data points in a three-dimensional space, making it easier to identify clusters, patterns, and relationships among variables.\nCreating a 3D Scatter Plot:\nTo illustrate the concept, we will generate synthetic 3D data and create an insightful 3D scatter plot using Matplotlib. The generated data includes three numeric variables: X, Y, and Z coordinates.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic 3D data\nx = np.random.normal(0, 1, 100)\ny = np.random.normal(0, 1, 100)\nz = np.random.normal(0, 1, 100)\n\n# Creating a 3D scatter plot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c='b', marker='o')\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Z-axis')\nax.set_title('3D Scatter Plot')\nplt.show()\n\n\n\n\nThe 3D scatter plot portrays the data in a three-dimensional space, offering an intuitive perspective on the distribution of data points. The use of color, markers, and labels enhances the visualization.\nInterpreting 3D Scatter Plots:\nInterpreting a 3D scatter plot involves several key considerations:\n\nData Clusters: Examine the distribution of data points in 3D space to identify clusters or patterns. Data points that are close to each other may represent a cohesive group or relationship.\nOutliers: Look for data points that deviate significantly from the main cluster, as these may indicate outliers or special cases.\nVariable Relationships: Understand how the three numeric variables (X, Y, and Z) interact in the 3D space. Observing their positions can reveal relationships and correlations.\n\n3D scatter plots are valuable for a wide range of applications, including data clustering, spatial analysis, and the visualization of multidimensional data.\n\n\n7.2: 3D Line Plot (Visualizing Data Trajectories in 3D Space)\n3D line plots are instrumental in visualizing data trajectories or data with time-dependent coordinates in three-dimensional space. These plots help us understand how data points evolve in a 3D environment.\nCreating a 3D Line Plot:\nTo illustrate the concept, we will generate synthetic 3D trajectory data and create a 3D line plot using Matplotlib. The data includes time, X, Y, and Z coordinates, which can represent a variety of phenomena, such as particle motion, aircraft paths, or spatial trajectories.\n\n# Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating synthetic trajectory data\nt = np.linspace(0, 10, 100)\nx = np.sin(t)\ny = np.cos(t)\nz = t\n\n# Creating a 3D line plot\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot(x, y, z, c='r')\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Z-axis')\nax.set_title('3D Line Plot')\nplt.show()\n\n\n\n\nThe 3D line plot portrays the trajectory or path of data points in a three-dimensional space. This visualization provides insights into the evolution and spatial characteristics of the data.\nInterpreting 3D Line Plots:\nInterpreting a 3D line plot involves several key considerations:\n\nTrajectories: Observe the path followed by the data points over time or in 3D space. Identify any loops, patterns, or trends within the trajectories.\nSpatial Relationships: Analyze how the data points are distributed in the 3D space. Investigate whether certain regions are densely populated or sparsely populated.\nCustomization: Explore customization options for line style and color to enhance the clarity and visual appeal of the plot.\n\n3D line plots are invaluable for studying phenomena with three-dimensional characteristics, and they offer a unique perspective on the data‚Äôs behavior in space and time."
  },
  {
    "objectID": "pages/vis2.html#specialized-plots",
    "href": "pages/vis2.html#specialized-plots",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "8: Specialized Plots",
    "text": "8: Specialized Plots\nIn this section, we will explore a range of specialized data visualization techniques that cater to specific data types and analysis needs. Specialized plots offer unique insights and enable the visualization of data that may not be adequately represented by standard plot types. We will delve into Polar Plots, Network Plots, and Word Clouds, each serving distinct purposes in data analysis.\n\n8.1: Polar Plot (Visualizing Circular Data)\nPolar plots are a specialized form of data visualization ideal for representing data with angular coordinates, such as wind direction, compass bearings, or circular data. These plots are invaluable for revealing patterns and trends in cyclical datasets.\nCreating a Polar Plot:\nTo illustrate the creation of a polar plot, we will use a synthetic dataset representing wind direction and wind speeds. This plot will provide insights into wind speed distribution in different directions.\n\n# Generating synthetic wind direction data\nangles = np.linspace(0, 2 * np.pi, 8)\nwind_speeds = np.random.uniform(0, 10, 8)\n\n# Creating a polar plot\nplt.figure(figsize=(8, 8))\nplt.polar(angles, wind_speeds, label='Wind Speeds', linestyle='-', marker='o')\nplt.fill(angles, wind_speeds, alpha=0.3)\nplt.thetagrids(angles * 180 / np.pi, labels=['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\nplt.rgrids(np.arange(0, 10, 2), angle=45)\nplt.legend()\nplt.title('Wind Speeds in Different Directions')\nplt.show()\n\n\n\n\nThe polar plot above represents wind speeds in various directions. The circular nature of the plot is well-suited for visualizing angular data.\nInterpreting Polar Plots:\nInterpreting a polar plot involves understanding the following elements:\n\nAngular Coordinates: The angles on the plot‚Äôs perimeter represent the directional data, with labels denoting the corresponding directions.\nRadial Axes: The radial axes extending from the center indicate values, in this case, wind speeds.\nData Representation: Each data point is plotted at its angular position, and the radial distance from the center corresponds to the value being represented.\n\nPolar plots are excellent for visualizing cyclical patterns and identifying trends in circular data, making them valuable in fields such as meteorology and environmental science.\n\n\n8.2: Network Plot (Visualizing Complex Relationships)\nNetwork plots, also known as graph visualizations, are designed to represent complex relationships and connections between entities. They are particularly useful for visualizing social networks, communication structures, and various interconnected data.\nCreating a Network Plot:\nTo showcase the creation of a network plot, we will use a synthetic dataset representing social network relationships. This plot will reveal the connections between individuals within the network.\n\n# Generating synthetic network data\nimport networkx as nx\n\nG = nx.Graph()\nG.add_node(\"Alice\")\nG.add_node(\"Bob\")\nG.add_node(\"Charlie\")\nG.add_edge(\"Alice\", \"Bob\")\nG.add_edge(\"Alice\", \"Charlie\")\n\n# Creating a network plot\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G, pos)\nnx.draw_networkx_edges(G, pos)\nnx.draw_networkx_labels(G, pos)\nplt.title('Social Network Relationships')\nplt.show()\n\n\n\n\nThe resulting network plot visually represents the relationships between individuals within the social network.\nInterpreting Network Plots:\nInterpreting a network plot involves considering the following aspects:\n\nNodes: Nodes represent individual entities, such as people or objects within the network.\nEdges: Edges, often depicted as lines connecting nodes, signify relationships or connections between entities.\nLayout: The arrangement of nodes and edges within the plot reflects the structure of the network. Different layout algorithms can reveal various network properties.\nClustering: Patterns of clustering and connectivity can provide insights into the network‚Äôs structure.\n\nNetwork plots are essential for understanding complex relationships and can be applied in diverse fields, including social sciences, biology, and information technology.\n\n\n8.3: Word Cloud (Visualizing Text Data)\nWord clouds are a specialized form of data visualization used to represent text data, specifically word frequency within a corpus or document. They provide an intuitive way to grasp the most common words and their relative importance.\nCreating a Word Cloud:\nTo demonstrate the creation of a word cloud, we will use a synthetic text data sample. This word cloud will visualize word frequency in the provided text.\n\n# Generating synthetic text data\nfrom wordcloud import WordCloud\n\ntext_data = \"This is a sample text data for creating a word cloud. Word clouds are a fun way to visualize word frequency.\"\n\n# Creating a word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Word Cloud of Text Data')\nplt.show()\n\n\n\n\nThe resulting word cloud visually emphasizes words by size, with more frequent words appearing larger.\nInterpreting Word Clouds:\nInterpreting a word cloud involves considering the following aspects:\n\nWord Size: The size of each word in the cloud corresponds to its frequency within the text. Larger words are more frequently used.\nColor: Word clouds can employ color to further emphasize certain words or categories.\nContext: Understanding the context of the word cloud is crucial to extract meaningful insights.\n\nWord clouds are an engaging way to uncover prominent terms within text data, making them valuable in text analysis, content marketing, and sentiment analysis."
  },
  {
    "objectID": "pages/vis2.html#advanced-data-visualization",
    "href": "pages/vis2.html#advanced-data-visualization",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "9: Advanced Data Visualization",
    "text": "9: Advanced Data Visualization\nIn this section, we will explore specialized data visualization techniques that cater to distinct data analysis needs. These visualizations offer unique insights into specific aspects of data analysis, such as model evaluation and dimensionality reduction.\n\n9.1: ROC Curves and AUC (Model Evaluation)\nROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve) are powerful tools for evaluating the performance of binary classification models. They provide a visual representation of a model‚Äôs ability to discriminate between positive and negative classes over various thresholds.\nCreating ROC Curves and Calculating AUC:\nTo illustrate the use of ROC curves and AUC, we will follow these steps:\n\nGenerate a synthetic dataset for binary classification.\nSplit the dataset into training and testing sets.\nTrain a logistic regression model.\nCalculate the ROC curve and AUC.\n\n\n# Generating synthetic binary classification data and training a model\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Generating synthetic binary classification data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Creating a ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Plotting the ROC curve\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\nThe ROC curve illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate as the classification threshold varies. AUC quantifies the overall model performance, with higher AUC values indicating better classification ability.\nInterpreting ROC Curves and AUC:\n\nTrue Positive Rate (TPR): The TPR represents the proportion of true positive predictions concerning all actual positive instances. It reflects the model‚Äôs ability to correctly classify positive cases.\nFalse Positive Rate (FPR): The FPR represents the proportion of false positive predictions concerning all actual negative instances. A low FPR is desired, as it indicates minimal misclassification of negative cases.\nROC Curve Shape: The shape of the ROC curve and its proximity to the top-left corner indicate the model‚Äôs performance. A curve that approaches the top-left corner indicates a superior model.\nArea Under the Curve (AUC): AUC summarizes the ROC curve‚Äôs performance in a single value. It ranges from 0.5 (random classification) to 1.0 (perfect classification). An AUC value greater than 0.5 suggests that the model outperforms random chance.\n\nROC curves and AUC are invaluable for assessing the quality of binary classification models and selecting the optimal threshold for specific application requirements.\n\n\n9.2: t-SNE Plots (Dimensionality Reduction)\nt-SNE (t-distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique that is particularly useful for visualizing high-dimensional data in lower dimensions while preserving the structure of data clusters. It is an excellent tool for exploring patterns and relationships within complex datasets.\nCreating t-SNE Scatter Plots:\nTo demonstrate the use of t-SNE, we will perform the following steps:\n\nGenerate synthetic high-dimensional data.\nApply t-SNE to reduce data to two dimensions.\nCreate a scatter plot of the reduced data.\n\n\n# Generating synthetic high-dimensional data and reducing it using t-SNE\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Generating synthetic high-dimensional data\nX, y = make_classification(n_samples=100, n_features=50, random_state=42)\n\n# Reducing the data to two dimensions using t-SNE\nX_embedded = TSNE(n_components=2, random_state=42).fit_transform(X)\n\n# Creating a t-SNE scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='viridis')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.title('t-SNE Scatter Plot')\nplt.show()\n\n\n\n\nThe resulting t-SNE scatter plot provides a simplified representation of the original high-dimensional data while preserving data patterns and clusters.\nInterpreting t-SNE Plots:\n\nClusters: Data points that are close together in the t-SNE scatter plot belong to the same clusters in the high-dimensional space, revealing natural groupings within the data.\nDimensionality Reduction: t-SNE effectively reduces the data‚Äôs dimensionality, making it easier to explore and understand complex datasets.\nOutliers: Outliers or anomalies may appear as data points that are isolated from the main clusters in the scatter plot.\n\nt-SNE is a valuable tool for data exploration, visualization, and gaining insights into high-dimensional data structures. It is particularly useful in fields such as machine learning, biology, and text analysis.\nAlthough extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. By exploring how it behaves in simple cases, we can learn to use it more effectively. Refer to this article for more info: How to Use t-SNE Effectively"
  },
  {
    "objectID": "pages/vis2.html#conclusion",
    "href": "pages/vis2.html#conclusion",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Conclusion",
    "text": "Conclusion\nIn this extensive Jupyter notebook, we have explored various data visualization techniques using Matplotlib and Seaborn. We began with basic plots, including line plots, scatter plots, bar plots, and histograms. Then, we delved into statistical plots like box plots, violin plots, and swarm plots. The matrix plots section covered heatmaps and clustermaps. We also explored distribution plots, time series plots, geospatial data visualization, 3D plots, specialized plots, custom visualizations, interactive visualizations, and specialized plots like ROC curves and t-SNE plots.\nData visualization is an integral part of data analysis, helping us gain insights, make informed decisions, and communicate our findings effectively. Choosing the right visualization technique for a given dataset is crucial, and this notebook provides a comprehensive overview to aid Computer Science and Data Science students in their data visualization journey."
  },
  {
    "objectID": "pages/vis2.html#additional-notes",
    "href": "pages/vis2.html#additional-notes",
    "title": "Comprehensive Data Visualization with Matplotlib and Seaborn",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nFor interactive visualizations, consider using libraries like Plotly, Bokeh, or Dash.\nTo enhance your data visualization skills, practice with real-world datasets and explore more advanced techniques and libraries.\nAlways strive for clear and informative visualizations that convey the intended message effectively."
  },
  {
    "objectID": "pages/Deliverable_1.html",
    "href": "pages/Deliverable_1.html",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#introduction",
    "href": "pages/Deliverable_1.html#introduction",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#bias",
    "href": "pages/Deliverable_1.html#bias",
    "title": "Dealing with missing-values",
    "section": "Bias",
    "text": "Bias\nWhen data is missing, the remaining data may not always accurately reflect the true population. The type of missingness and the way we deal with it can dictate bias in our results.\nFor example, in a survey recording income of individuals, those with low income may choose not to respond. The use of average income to impute the missing values can lead to bias as the average of the available data may not represent that of the population."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "href": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "title": "Dealing with missing-values",
    "section": "Missing Completely At Random (MCAR)",
    "text": "Missing Completely At Random (MCAR)\nIn this case, the data is missing randomly and is not related to any variable in the dataset or to the missing value themselves. The probability of missingness is same for all observations. There exists no underlying pattern to the missingness. The missing values are completely independent of other data.\nFor example, during data collection, if some responses were not collected due to a technical error, then the missing data is completely at random.\nAll statistical analysis performed on the dataset will remain unbiased in this case."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-at-random-mar",
    "href": "pages/Deliverable_1.html#missing-at-random-mar",
    "title": "Dealing with missing-values",
    "section": "Missing At Random (MAR)",
    "text": "Missing At Random (MAR)\nIn this case, the missingness of the data can be fully accounted for by the other known data values. Here there exists some form of pattern in the missing values.\nFor example, In a survey, women might be unwilling to disclose their age. Here the missingness of the variable age can be explained by another observed variable ‚Äúgender‚Äù."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "href": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "title": "Dealing with missing-values",
    "section": "Missing Not At Random (MNAR)",
    "text": "Missing Not At Random (MNAR)\nIn this case, the missingness is neither MCAR nor MAR. The fact that a datapoint is missing is dependent on the value of the data point. In order to correct for the bias we would have to make modelling assumptions about the nature of the bias.\nFor example, in a social survey where individuals are asked about their income, respondnets may not disclose it if it is too high or too low. Thus the missingness in the feature income is directly related to the values of that feature."
  },
  {
    "objectID": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "href": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "title": "Dealing with missing-values",
    "section": "Identifying the type of missingness",
    "text": "Identifying the type of missingness\n\nUnderstanding the data collection process, the features involved and the research domain can help identify possible patterns as to why data is missing\nGraphical representation of the missing data and heatmaps can help identify relationships between features that can be utilized to make better imputation of the missing data\nWe can impute the data using different techniques while also making assumptions on the nature of missingness. Subsequently, we analyze the results to observe the assumptions that have led to consistent results."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-value-imputation",
    "href": "pages/Deliverable_1.html#missing-value-imputation",
    "title": "Dealing with missing-values",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\nIn many cases it might not be feasible to discard observations that contain missing values. This could be due to the availability of lesser number of samples or due to the importance of each observation. In such cases we can employ imputation which deals with replacing the missing values with some predicted value. We will have a look at two simple imputation techniques that can be implemented using the sklearn package.\n\nSimple Imputer\nK Nearest neighbours Imputer"
  },
  {
    "objectID": "pages/Deliverable_1.html#simple-imputer",
    "href": "pages/Deliverable_1.html#simple-imputer",
    "title": "Dealing with missing-values",
    "section": "Simple Imputer",
    "text": "Simple Imputer\nThe Simple Imputation technique offers a basic approach to filling missing data wherein we replace the missing data with a constant value or utilize statistics such as ‚Äúmean‚Äù, ‚Äúmode‚Äù, or ‚Äúmedian‚Äù of the available values. The technique is useful for its simplicity and can serve as a reference technique. It is also worth noting that this can lead to biased or unrealistic results\n\nSimple Imputer - Mean\n\nimport numpy as np\nimport pandas as pd\n\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mean strategy. The mean value of that feature will be \\frac{( 1+1+2+4)}{4} = 2\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 2 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\ndata = {\"feature_1\":[1,2,3,4,5],\n        \"feature_2\":[1,1,np.nan,2,4],\n        \"feature_3\":[3,3,3,4,5]}\n\n\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1\n1.0\n3\n\n\n1\n2\n1.0\n3\n\n\n2\n3\nNaN\n3\n\n\n3\n4\n2.0\n4\n\n\n4\n5\n4.0\n5\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom sklearn.impute import SimpleImputer\n\n\nimputer = SimpleImputer(strategy='mean')\nimputed_data = imputer.fit_transform(df)\n\n\nimputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nimputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n2.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSimple Imputer - Mode\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mode strategy. The value that appears most often is 1.\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nmode_imputer = SimpleImputer(strategy='most_frequent')\nimputed_data = mode_imputer.fit_transform(df)\n\n\nmode_imputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nmode_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/Deliverable_1.html#k-nearest-neighbours",
    "href": "pages/Deliverable_1.html#k-nearest-neighbours",
    "title": "Dealing with missing-values",
    "section": "K Nearest Neighbours",
    "text": "K Nearest Neighbours\nThis technique is an extension of the KNN classifier we have seen in MLT to perform imputation. In this technique we identify the points that are similar to the observation we wish to impute based on the available features. We can then use the values of these neighboring points fill in the missing values\n\nfrom sklearn.impute import KNNImputer\n\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nknn_imputed_data = knn_imputer.fit_transform(df)\n\n\nknn_imputed_df = pd.DataFrame(knn_imputed_data, columns=['feature_1','feature_2','feature_3'])\nknn_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.5\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/Inductive_Bias.html",
    "href": "pages/Inductive_Bias.html",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "",
    "text": "Slides: Click here!"
  },
  {
    "objectID": "pages/Inductive_Bias.html#inductive-bias",
    "href": "pages/Inductive_Bias.html#inductive-bias",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "Inductive bias",
    "text": "Inductive bias\n\nAnything which makes the algorithm learn one pattern instead of another pattern.\n\nDecision trees use a step-function collection for classification; but these step functions utilize one feature/variable only. Is this phenomenon sensitive to the nature of the dataset?\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\n\nDTree = DecisionTreeClassifier()\nDTree.fit(X, y)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax = ax1)\ndisp.ax_.scatter(X[:, 0][y==0], X[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Tree Depth: {DTree.get_depth()}');\n\nplot_tree(DTree, label='none', filled=True, feature_names=['F1', 'F2'], class_names=['Red', 'Blue'], node_ids=False, rounded=True, impurity=False, ax=ax2);\n\n\n\n\nThe 4x4 checkerboard dataset with alternating classes requires a tree of depth=7 to capture its structure respectively.\nBut what will happen if we try to train a tree on the rotated variant of this dataset?\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nDTree_rotated = DecisionTreeClassifier()\nDTree_rotated.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Overfit) Tree Depth: {DTree_rotated.get_depth()}')\n\nDTree_rotated_constrained = DecisionTreeClassifier(max_depth=7)\nDTree_rotated_constrained.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated_constrained, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax2)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Constrained) Tree Depth: {DTree_rotated_constrained.get_depth()}');\n\n\n\n\n\nOh!\nThe model fails to understand the generation rationale of the dataset as it suffers an inductive bias of axis-parallel splitting."
  },
  {
    "objectID": "pages/Inductive_Bias.html#being-mindful-of-the-bias",
    "href": "pages/Inductive_Bias.html#being-mindful-of-the-bias",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "Being Mindful of the Bias",
    "text": "Being Mindful of the Bias\nThe above dataset has a seperator corresponding to a second order function of the features.\nTransform the dataset and apply perceptron! Alter inductive bias to our advantage\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Perceptron\n\npoly_perceptron = make_pipeline(PolynomialFeatures(2), Perceptron(alpha=0, max_iter=int(1e6), tol=None))\npoly_perceptron.fit(X, y)\n\nfig, (ax1) = plt.subplots(1, 1)\ndisp = DecisionBoundaryDisplay.from_estimator(poly_perceptron, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X[:, 0][y==-1], X[:, 1][y==-1], color='red', label='y==-1', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Poly-Perceptron | Score: {poly_perceptron.score(X, y)}');"
  },
  {
    "objectID": "pages/demo.html#why",
    "href": "pages/demo.html#why",
    "title": "ML Handbook",
    "section": "Why?",
    "text": "Why?\n\nDecision trees make the following presumption about the structure of data:\n\n\n\nCan figure class out based on a series of binary questions (yes/no) on individual features\n\n\n\n\nInductive Bias:  Anything which makes an algorithm learn one pattern over another\n\n\n\n\nInductive bias of decision trees entails the use of axis-parallel splits to construct the decision boundary\nSensitive to rotations\nAlgorithm invariant to rotation?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we‚Äôll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let‚Äôs explore the world of data together!"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html",
    "href": "pages/Scaling multi-dataset multi-algo.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "href": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "href": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "href": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Datasets",
    "text": "Loading the Datasets\nIn this section, we will start by loading four distinct datasets, each with its unique characteristics. These datasets are commonly used for various classification tasks and will serve as the foundation for our comprehensive study on the impact of feature scaling on classification models.\n\n# Load the datasets\niris = load_iris()\ndigits = load_digits()\nwine = load_wine()\nbreast_cancer = load_breast_cancer()\n\n# Create DataFrames for the datasets\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\ndigits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\nbreast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n\n\n1. Iris Dataset\nDescription: The Iris dataset is a classic dataset in the field of machine learning and consists of 150 samples of iris flowers, each from one of three species: Iris setosa, Iris virginica, and Iris versicolor. There are four features‚Äîsepal length, sepal width, petal length, and petal width‚Äîmeasured in centimeters.\nUse Case: This dataset is often used for practicing classification techniques, especially for building models to distinguish between the three iris species based on their feature measurements.\n\n# Display the first few rows of iris dataset\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\n\n2. Digits Dataset\nDescription: The Digits dataset is a collection of 8x8 pixel images of handwritten digits (0 through 9). There are 1,797 samples, and each sample is an 8x8 image, resulting in 64 features. The goal is to correctly classify the digits based on these pixel values.\nUse Case: This dataset is a fundamental resource for pattern recognition and is frequently used for exploring image classification and digit recognition algorithms.\n\n\n3. Wine Dataset\nDescription: The Wine dataset comprises 178 samples of wine classified into three classes based on their cultivar. The dataset contains 13 feature variables, including measurements related to chemical composition, making it a valuable resource for wine classification tasks.\nUse Case: Wine quality prediction and classification are common applications for this dataset, as it allows for distinguishing between different wine types.\n\n# Display the first few rows of wine dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\n\n\n4. Breast Cancer Dataset\nDescription: The Breast Cancer dataset is used for breast cancer diagnosis. It includes 569 samples with 30 feature variables, primarily related to characteristics of cell nuclei present in breast cancer biopsies. The dataset is labeled to indicate whether a sample is benign or malignant.\nUse Case: This dataset is often employed for building classification models to assist in the early detection of breast cancer, aiding in medical diagnosis.\n\n# Display the first few rows of breast cancer dataset\nbreast_cancer_df.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows √ó 31 columns\n\n\n\nThe datasets contain various features related to their respective domains, with a ‚Äòtarget‚Äô column indicating the class labels."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "href": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Define a function to create noisy datasets\ndef create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n    X = dataset.data\n    y = dataset.target\n\n    rng = np.random.default_rng(seed=random_state)\n    noise = rng.normal(0, noise_std, size=X.shape)\n    X_noisy = X + noise\n\n    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n\n    return X_train_noisy, X_test_noisy, y_train, y_test\n\n# Create noisy datasets for all four datasets\nX_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\nX_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\nX_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\nX_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "href": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Define a function to apply Standard Scaler to a dataset\ndef apply_standard_scaler(X_train, X_test):\n    standard_scaler = StandardScaler()\n    X_train_scaled = standard_scaler.fit_transform(X_train)\n    X_test_scaled = standard_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Standard Scaler to all four datasets\nX_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Define a function to apply Min-max Scaler to a dataset\ndef apply_min_max_scaler(X_train, X_test):\n    min_max_scaler = MinMaxScaler()\n    X_train_scaled = min_max_scaler.fit_transform(X_train)\n    X_test_scaled = min_max_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Min-max Scaler to all four datasets\nX_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Define a function to apply Maximum Absolute Scaler to a dataset\ndef apply_max_abs_scaler(X_train, X_test):\n    max_abs_scaler = MaxAbsScaler()\n    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n    X_test_scaled = max_abs_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Maximum Absolute Scaler to all four datasets\nX_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Define a function to apply Robust Scaler to a dataset\ndef apply_robust_scaler(X_train, X_test):\n    robust_scaler = RobustScaler()\n    X_train_scaled = robust_scaler.fit_transform(X_train)\n    X_test_scaled = robust_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Robust Scaler to all four datasets\nX_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Define a function to apply Quantile Transformer to a dataset\ndef apply_quantile_transformer(X_train, X_test):\n    quantile_transformer = QuantileTransformer(output_distribution='normal')\n    X_train_scaled = quantile_transformer.fit_transform(X_train)\n    X_test_scaled = quantile_transformer.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Quantile Transformer to all four datasets\nX_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "href": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of six classification models on the different scaled datasets. The models we will use are:\n\nRandom Forest\nSupport Vector Machine (SVM)\nDecision Tree\nNaive Bayes (GaussianNB)\nK-Nearest Neighbors (KNN)\nLogistic Regression\n\nFor each scaling method, we will train and evaluate all six models for all four datasets.\n\n# Initialize the classifiers\nrf_classifier = RandomForestClassifier(random_state=42)\nsvm_classifier = SVC(random_state=42)\ndt_classifier = DecisionTreeClassifier(random_state=42)\nnb_classifier = GaussianNB()\nknn_classifier = KNeighborsClassifier()\nlr_classifier = LogisticRegression()\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each dataset and scaling method, and evaluate the models\ndatasets = [\n    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n]\n\nscaling_methods = {\n    \"No Scaling\": {\n        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n    },\n    \"Standard Scaler\": {\n        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n    },\n    \"Min-max Scaler\": {\n        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n    },\n    \"Maximum Absolute Scaler\": {\n        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n    },\n    \"Robust Scaler\": {\n        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n    },\n    \"Quantile Transformer\": {\n        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n    }\n}\n\n# Loop through datasets and scaling methods\nfor dataset_name, X_train, X_test, y_train, y_test in datasets:\n    for scaler_name, scaled_data in scaling_methods.items():\n        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n\n        # Train and evaluate all six models\n        for classifier, classifier_name in zip([rf_classifier, svm_classifier, dt_classifier, nb_classifier, knn_classifier, lr_classifier], [\"Random Forest\", \"SVM\", \"Decision Tree\", \"Naive Bayes\", \"K-Nearest Neighbors\", \"Logistic Regression\"]):\n            classifier.fit(X_train_scaled, y_train)\n            predictions = classifier.predict(X_test_scaled)\n            accuracy = accuracy_score(y_test, predictions)\n            accuracy_scores.append([dataset_name, scaler_name, classifier_name, accuracy])"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "href": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet‚Äôs analyze the results of our experiment and discuss the impact of different scaling methods on multiple classification models for each dataset.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Classifier', 'Accuracy'])\nresults_df\n\n# Create a new DataFrame to display the results\nresults_pivoted_df = results_df.pivot(index=['Dataset', 'Scaling Method'], columns='Classifier', values='Accuracy')\nresults_pivoted_df.reset_index(inplace=True)\nresults_pivoted_df.columns.name = None  # Remove the column name\n\n# Fill any missing values with NaN (for clarity)\nresults_pivoted_df.fillna(value=np.nan, inplace=True)\n\nresults_pivoted_df\n\n\n\n\n\n\n\n\nDataset\nScaling Method\nDecision Tree\nK-Nearest Neighbors\nLogistic Regression\nNaive Bayes\nRandom Forest\nSVM\n\n\n\n\n0\nBreast Cancer\nMaximum Absolute Scaler\n0.938596\n0.903509\n0.929825\n0.956140\n0.964912\n0.929825\n\n\n1\nBreast Cancer\nMin-max Scaler\n0.938596\n0.912281\n0.964912\n0.956140\n0.964912\n0.956140\n\n\n2\nBreast Cancer\nNo Scaling\n0.938596\n0.956140\n0.973684\n0.956140\n0.964912\n0.947368\n\n\n3\nBreast Cancer\nQuantile Transformer\n0.938596\n0.956140\n0.964912\n0.947368\n0.964912\n0.956140\n\n\n4\nBreast Cancer\nRobust Scaler\n0.938596\n0.964912\n0.964912\n0.956140\n0.964912\n0.973684\n\n\n5\nBreast Cancer\nStandard Scaler\n0.938596\n0.938596\n0.964912\n0.956140\n0.964912\n0.964912\n\n\n6\nDigits\nMaximum Absolute Scaler\n0.858333\n0.983333\n0.961111\n0.905556\n0.972222\n0.983333\n\n\n7\nDigits\nMin-max Scaler\n0.858333\n0.988889\n0.963889\n0.905556\n0.972222\n0.983333\n\n\n8\nDigits\nNo Scaling\n0.858333\n0.986111\n0.966667\n0.905556\n0.972222\n0.991667\n\n\n9\nDigits\nQuantile Transformer\n0.858333\n0.966667\n0.941667\n0.900000\n0.969444\n0.975000\n\n\n10\nDigits\nRobust Scaler\n0.858333\n0.819444\n0.963889\n0.905556\n0.972222\n0.913889\n\n\n11\nDigits\nStandard Scaler\n0.858333\n0.975000\n0.977778\n0.905556\n0.972222\n0.986111\n\n\n12\nIris\nMaximum Absolute Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n13\nIris\nMin-max Scaler\n0.933333\n0.966667\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n14\nIris\nNo Scaling\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n15\nIris\nQuantile Transformer\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n16\nIris\nRobust Scaler\n0.933333\n0.933333\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n17\nIris\nStandard Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.966667\n\n\n18\nWine\nMaximum Absolute Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n19\nWine\nMin-max Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n20\nWine\nNo Scaling\n0.916667\n0.722222\n0.972222\n1.000000\n1.000000\n0.805556\n\n\n21\nWine\nQuantile Transformer\n0.916667\n0.944444\n1.000000\n0.972222\n1.000000\n0.972222\n\n\n22\nWine\nRobust Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n23\nWine\nStandard Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# Define a list of scaling methods and their Material Design colors\nscaling_methods = results_pivoted_df['Scaling Method'].unique()\nmaterial_colors = sns.color_palette(\"Set2\")\ndatasets = results_pivoted_df['Dataset'].unique()\n\n# Set Seaborn style for a modern, beautiful look\nsns.set_style(\"whitegrid\")\n\n# Create grouped bar charts for each dataset\nfor dataset in datasets:\n    plt.figure(figsize=(12, 8))\n    \n    # Filter the data for the current dataset\n    dataset_data = results_pivoted_df[results_pivoted_df['Dataset'] == dataset]\n    \n    # Define the x-axis labels (ML algorithms)\n    ml_algorithms = dataset_data.columns[2:].tolist()\n    \n    bar_width = 0.12  # Width of each bar\n    index = range(len(ml_algorithms))\n\n    # Create a bar for each scaling method\n    for i, method in enumerate(scaling_methods):\n        try:\n            accuracies = dataset_data[dataset_data['Scaling Method'] == method].values[0][2:]\n            plt.bar([x + i * bar_width for x in index], accuracies, bar_width, label=method, color=material_colors[i])\n        except IndexError:\n            print(f\"No data found for {method} in {dataset}\")\n    \n    # Set the x-axis labels, title, and legend\n    plt.xlabel('ML Algorithms')\n    plt.ylabel('Accuracy')\n    plt.title(f'Accuracy Comparison for {dataset}')\n    plt.xticks([x + bar_width * (len(scaling_methods) / 2) for x in index], ml_algorithms)\n    plt.legend(scaling_methods, title='Scaling Method')\n\n    # Adjust layout to prevent overlapping x-axis labels\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    # Show the plot or save it as an image\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of Results\nThe evaluation of results is based on the performance of six classification algorithms across different datasets and scaling methods. The accuracy scores are presented for Decision Tree, K-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Random Forest, and Support Vector Machine (SVM). Here‚Äôs an analysis of the findings:\n\nBreast Cancer Dataset\n\nMaximum Absolute Scaler: This scaling method produced competitive accuracy scores for all algorithms. Naive Bayes and Logistic Regression achieved the highest accuracy of approximately 95.6%, while other algorithms also performed well.\nMin-max Scaler: Similar to the Maximum Absolute Scaler, this method yielded strong accuracy results across all algorithms, with Logistic Regression and SVM reaching the highest scores of 96.4% and 95.6%, respectively.\nNo Scaling: Surprisingly, this dataset demonstrated that some algorithms, particularly Naive Bayes and Logistic Regression, do not benefit from feature scaling. They achieved high accuracy without any scaling, indicating that the original feature values were suitable for these models.\nQuantile Transformer: The Quantile Transformer showed consistent accuracy, with Logistic Regression and SVM achieving the highest scores of 96.4% and 95.6%, respectively.\nRobust Scaler: Robust scaling led to competitive accuracy for most algorithms, with SVM achieving the highest accuracy of 97.4%.\nStandard Scaler: Standard scaling demonstrated similar results to other scaling methods, with Logistic Regression and SVM achieving the highest accuracy of 96.4%.\n\n\n\nDigits Dataset\n\nMaximum Absolute Scaler: This scaling method had a positive impact on KNN, which achieved a high accuracy of approximately 98.3%. However, Decision Tree and Logistic Regression showed lower performance.\nMin-max Scaler: Min-max scaling improved the accuracy of KNN to nearly 98.9%. Other algorithms also benefited from this scaling.\nNo Scaling: Surprisingly, the Digits dataset, particularly for KNN, demonstrated that feature scaling is not necessary for achieving high accuracy. KNN reached 99.2% accuracy without any scaling.\nQuantile Transformer: While other algorithms performed well with this scaling method, Decision Tree and KNN showed slightly reduced accuracy.\nRobust Scaler: Robust scaling did not benefit KNN, with its accuracy dropping to 81.9%. Other algorithms showed consistent performance.\nStandard Scaler: Standard scaling improved the accuracy of KNN to 98.6%, making it one of the best-performing algorithms for this dataset.\n\n\n\nIris Dataset\n\nMaximum Absolute Scaler: Scaling had minimal impact on the accuracy of algorithms for the Iris dataset. SVM achieved the highest accuracy of 96.7%, regardless of scaling.\nMin-max Scaler: Similar to Maximum Absolute Scaling, min-max scaling had a limited effect on the accuracy of algorithms. SVM consistently achieved the highest accuracy of 96.7%.\nNo Scaling: The Iris dataset was naturally well-scaled, and most algorithms, particularly SVM, achieved high accuracy without any scaling.\nQuantile Transformer: Scaling had little influence on accuracy. SVM remained the best-performing algorithm, with an accuracy of 96.7%.\nRobust Scaler: Robust scaling slightly improved the accuracy of Decision Tree and SVM but had limited impact overall.\nStandard Scaler: Standard scaling resulted in consistent accuracy for all algorithms, with SVM maintaining the highest accuracy of 96.7%.\n\n\n\nWine Dataset\n\nMaximum Absolute Scaler: This scaling method had a substantial impact on SVM, boosting its accuracy to 100%. Other algorithms also reached high accuracy levels.\nMin-max Scaler: Min-max scaling had a similar effect on SVM, resulting in perfect accuracy. Decision Tree, KNN, and Logistic Regression also reached maximum accuracy.\nNo Scaling: The Wine dataset revealed the significance of scaling, particularly for SVM. Without scaling, SVM‚Äôs accuracy was relatively low at 80.6%, highlighting the sensitivity of SVM to feature values.\nQuantile Transformer: Quantile transformation improved the accuracy of Decision Tree and Logistic Regression. However, SVM remained sensitive to scaling.\nRobust Scaler: Robust scaling had a positive impact, with SVM reaching perfect accuracy. Other algorithms also performed well.\nStandard Scaler: Standard scaling had a similar effect to other scaling methods, with SVM achieving perfect accuracy.\n\n\n\n\nConclusion\nIn summary, the impact of feature scaling on machine learning algorithms varies depending on the dataset and the algorithm used:\n\nSome datasets, like the Iris dataset, are naturally well-scaled, and most algorithms perform consistently well without any scaling.\nFeature scaling, particularly min-max and maximum absolute scaling, has a positive impact on algorithms in datasets like Breast Cancer and Digits, resulting in improved accuracy.\nThe Wine dataset demonstrated that certain algorithms, notably SVM, are highly sensitive to feature scaling. Without proper scaling, SVM‚Äôs performance can be significantly compromised.\nSurprisingly, some algorithms, such as Naive Bayes and Logistic Regression, performed well without any scaling in the Breast Cancer dataset, indicating that the original feature values were suitable for these models.\n\nIn practice, it‚Äôs essential to consider the characteristics of the dataset and the algorithm‚Äôs sensitivity to feature values when deciding whether to apply feature scaling. While scaling can improve the performance of many machine learning algorithms, there are cases where it may not be necessary and could even have a negligible or detrimental effect on model accuracy."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "href": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling",
    "text": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling\nIn the context of our machine learning analysis, it‚Äôs fascinating to observe that Naive Bayes and tree-based algorithms, such as Decision Trees and Random Forests, exhibit remarkable resilience to feature scaling. This resilience stems from the inherent characteristics of these algorithms and their method of decision-making.\n\nNaive Bayes Classifier\nNaive Bayes is a probabilistic algorithm that‚Äôs based on the Bayes‚Äô theorem. It operates under the ‚Äúnaive‚Äù assumption that features are conditionally independent, given the class label. This fundamental assumption simplifies the calculations and often leads to surprisingly good classification results, especially in text and categorical data analysis.\nThe reason why Naive Bayes remains largely unaffected by feature scaling is twofold:\n\nProbabilistic Nature: Naive Bayes calculates probabilities based on the distribution of features within each class. The relative scaling of individual features does not impact the probability ratios significantly. In other words, as long as the relationships between features and classes remain consistent, the algorithm can adapt to different feature scales.\nNormalization in Probability Calculation: When computing probabilities, Naive Bayes often involves normalizing terms. This means that even if feature values are on different scales, the normalization process effectively scales them down to a common scale during probability calculations.\n\n\n\nDecision Trees and Random Forests\nDecision Trees and their ensemble counterpart, Random Forests, are non-parametric algorithms that make decisions by recursively splitting data based on feature values. They are highly interpretable and capable of capturing complex relationships within the data.\nThe key reasons why Decision Trees and Random Forests are generally insensitive to feature scaling include:\n\nSplitting Criteria: Decision Trees make decisions based on feature values relative to certain thresholds. The order or magnitude of these thresholds doesn‚Äôt affect the decision-making process. The algorithm focuses on finding the most discriminative features and their optimal split points.\nEnsemble Nature (Random Forests): Random Forests combine multiple Decision Trees. The ensemble nature of Random Forests further reduces sensitivity to feature scaling. When individual trees make errors due to scaling, the ensemble tends to compensate for them.\nImpurity Measures: Decision Trees use impurity measures like Gini impurity and entropy to determine the quality of a split. These measures are based on class proportions within a split and are independent of feature scales."
  },
  {
    "objectID": "pages/standard.html",
    "href": "pages/standard.html",
    "title": "Investigation of Standard Scaling Influence",
    "section": "",
    "text": "Colab Link: Click here!\nThis Jupyter Notebook is dedicated to an in-depth investigation of feature scaling‚Äôs significance, specifically focusing on standard scaling, also known as Z-score normalization. Feature scaling is an indispensable preprocessing step in numerous machine learning algorithms. It often enhances model performance. This study is vital as it sheds light on the practical implications of feature scaling in real-world applications. The wine dataset from the UCI Machine Learning Repository will be employed to demonstrate the effects of feature scaling."
  },
  {
    "objectID": "pages/standard.html#overview",
    "href": "pages/standard.html#overview",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Overview",
    "text": "Overview\nFeature scaling is a vital preprocessing step in various machine learning algorithms, and one of the most used ones is the Standard Scaler. It involves rescaling each feature in the dataset to have a standard deviation of 1 and a mean of 0. This normalization is necessary for several reasons, although tree-based models are less affected by feature scaling. Other algorithms might require feature normalization for different purposes, such as improving convergence or creating different model fits."
  },
  {
    "objectID": "pages/standard.html#the-wine-dataset",
    "href": "pages/standard.html#the-wine-dataset",
    "title": "Investigation of Standard Scaling Influence",
    "section": "The Wine Dataset",
    "text": "The Wine Dataset\nThe wine dataset from UCI will be used in this study. This dataset contains continuous features that measure different properties, such as alcohol content, malic acid, amongst others. These features are heterogeneous in scale, making it an excellent example to illustrate the effects of standard scaling.\n\nData Loading and Preparation\nWe will start by loading and preparing the wine dataset for our analysis. We will also split the data into training and testing sets. This is a common practice in machine learning to evaluate the performance of a model on unseen data.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the wine dataset\nX, y = load_wine(return_X_y=True, as_frame=True)\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nscaler.set_output(transform=\"pandas\")\nscaled_X_train = scaler.fit_transform(X_train)"
  },
  {
    "objectID": "pages/standard.html#analysis-of-standard-scaling-effects",
    "href": "pages/standard.html#analysis-of-standard-scaling-effects",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Analysis of Standard Scaling Effects",
    "text": "Analysis of Standard Scaling Effects\n\nVisualizing the Effect on a K-Neighbors Model\nTo visually demonstrate the effect of standard scaling on a K-Neighbors Classifier, we select a subset of two features, ‚Äúproline‚Äù and ‚Äúhue,‚Äù which have values with different orders of magnitude. We will visualize the decision boundary of the classifier with and without scaling. The K-Neighbors Classifier is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# Define the features for visualization\nX_plot = X[[\"proline\", \"hue\"]]\nX_plot_scaled = scaler.fit_transform(X_plot)\n\n# Create K-Neighbors Classifier\nclf = KNeighborsClassifier(n_neighbors=20)\n\n# Define a function to fit and plot the model\ndef fit_and_plot_model(X_plot, y, clf, ax):\n    clf.fit(X_plot, y)\n    disp = DecisionBoundaryDisplay.from_estimator(\n        clf, X_plot, response_method=\"predict\", alpha=0.5, ax=ax\n    )\n    disp.ax_.scatter(X_plot[\"proline\"], X_plot[\"hue\"], c=y, s=20, edgecolor=\"k\")\n    disp.ax_.set_xlim((X_plot[\"proline\"].min(), X_plot[\"proline\"].max()))\n    disp.ax_.set_ylim((X_plot[\"hue\"].min(), X_plot[\"hue\"].max()))\n    return disp.ax_\n\n# Plot the decision boundaries\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nfit_and_plot_model(X_plot, y, clf, ax1)\nax1.set_title(\"KNN without scaling\")\nfit_and_plot_model(X_plot_scaled, y, clf, ax2)\nax2.set_xlabel(\"scaled proline\")\nax2.set_ylabel(\"scaled hue\")\n_ = ax2.set_title(\"KNN with scaling\")\n\n\n\n\nThe visualizations depict a significant change in the decision boundary when we scale the features. Without scaling, the variable ‚Äúproline‚Äù dominates the decision boundary due to its higher magnitude, while ‚Äúhue‚Äù is comparatively ignored. After scaling, both variables have similar impacts on the decision boundary.\n\n\nImpact of Standard Scaling on PCA\nNext, we will examine the effect of standard scaling on Principal Component Analysis (PCA). PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance. Scaling is crucial as it ensures that features with different scales do not dominate the principal components.\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\npca = PCA(n_components=2).fit(X_train)\nscaled_pca = PCA(n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\n# Visualize the weights of the first principal component\nfirst_pca_component = pd.DataFrame(\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n_ = plt.tight_layout()\n\n\n\n\nAs observed, the ‚Äúproline‚Äù feature dominates the direction of the first principal component without scaling, being about two orders of magnitude above the other features. This is contrasted when observing the first principal component for the scaled version of the data, where the orders of magnitude are roughly the same across all the features.\nWe can visualize the distribution of the principal components in both cases:\n\n# Visualize the distribution of principal components\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\ntarget_classes = range(0, 3)\ncolors = (\"blue\", \"red\", \"green\")\nmarkers = (\"^\", \"s\", \"o\")\nfor target_class, color, marker in zip(target_classes, colors, markers):\n    ax1.scatter(\n        x=X_train_transformed[y_train == target_class, 0],\n        y=X_train_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\n    ax2.scatter(\n        x=X_train_std_transformed[y_train == target_class, 0],\n        y=X_train_std_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\nax1.set_title(\"Unscaled training dataset after PCA\")\nax2.set_title(\"Standardized training dataset after PCA\")\nfor ax in (ax1, ax2):\n    ax.set_xlabel(\"1st principal component\")\n    ax.set_ylabel(\"2nd principal component\")\n    ax.legend(loc=\"upper right\")\n    ax.grid()\n_ = plt.tight_layout()\n\n\n\n\nIn the above visualizations, we can see the impact of scaling on PCA. Without scaling, one feature dominates the first principal component, while scaling results in components with similar orders of magnitude across all features."
  },
  {
    "objectID": "pages/standard.html#conclusion",
    "href": "pages/standard.html#conclusion",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Conclusion",
    "text": "Conclusion\nThis Jupyter Notebook has explored the effects of standard scaling on machine learning models using the wine dataset. We observed how standard scaling influences decision boundaries and the behavior of PCA. Scaling the features ensures that no single feature dominates the analysis and can lead to improved model performance. It is an important preprocessing step to consider when working with machine learning algorithms. Future research could focus on the effects of other scaling methods and their impact on different types of machine learning models."
  },
  {
    "objectID": "pages/standard.html#references",
    "href": "pages/standard.html#references",
    "title": "Investigation of Standard Scaling Influence",
    "section": "References",
    "text": "References\nThis notebook is based on the sklearn document titled ‚ÄúImportance of Feature Scaling‚Äù. You can find more information at the following link: Importance of Feature Scaling."
  },
  {
    "objectID": "pages/Ordinal_Classification.html",
    "href": "pages/Ordinal_Classification.html",
    "title": "Ordinal Classification",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Ordinal_Classification.html#synthetic-data-creation",
    "href": "pages/Ordinal_Classification.html#synthetic-data-creation",
    "title": "Ordinal Classification",
    "section": "Synthetic Data Creation",
    "text": "Synthetic Data Creation\nWe will consider an ordinal classification problem with 4 classes. We randomly sample 1000 points from a standard normal distribution. We fix \\beta = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, and cutpoints \\alpha_0 = -\\infty, \\alpha_1 = -2, \\alpha_2 = -1, \\alpha_3 = 2, \\alpha_4 = \\infty\n\nimport numpy as np\nnp.random.seed(69)\n\nX = np.random.normal(scale=1, size=(1000, 2))\nbeta = np.array([-1, 1])\ncutpoints = np.array([-np.inf, -2, -1, 2, np.inf])\n\n\nfrom sklearn.model_selection import train_test_split\n\nY = X@beta\n\ndef ordify(cutpoints):\n\n  def hlo(x):\n\n    for i in range(len(cutpoints)-1):\n      if cutpoints[i] &lt;= x &lt; cutpoints[i+1]:\n        return i+1\n\n  return hlo\n\nordinate = ordify(cutpoints)\nY_ord = np.array([ordinate(i) for i in Y])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, Y_ord, test_size=0.33, random_state=42)"
  },
  {
    "objectID": "pages/Ordinal_Classification.html#how-to-predict",
    "href": "pages/Ordinal_Classification.html#how-to-predict",
    "title": "Ordinal Classification",
    "section": "How to predict?",
    "text": "How to predict?\nFor a validation datapoint x_i, use the trained models to calculate estimates \\hat{N_i} \\text{ for } i = 1, 2, \\cdots, K-1. We then use these to calculate probabilities of each class as follows:\n\n\n\nClass\nProbability\n\n\n\n\n1\n1 - \\hat{N_1}\n\n\n2\n\\hat{N_1} - \\hat{N_2}\n\n\ni\n\\hat{N_{i-1}} - \\hat{N_i}\n\n\nK-1\n\\hat{N_{K-1}}\n\n\n\nThe first and last class probabilites are from a single classifier, where as the others are the difference of the outputs from a pair of consecutive (w.r.t i) classifiers.\nNote that \\hat{N_i} = \\text{Pr}(y_i &gt; i).\n\nfrom sklearn.base import clone, BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted, check_array\nfrom sklearn.utils.multiclass import check_classification_targets\nimport numpy as np\n\nclass OrdinalClassifier(BaseEstimator, ClassifierMixin):\n\n    def __init__(self,learner):\n        self.learner = learner\n        self.ordered_learners = dict()\n        self.classes = []\n\n    def fit(self,X,y):\n        self.classes = np.sort(np.unique(y))\n        assert self.classes.shape[0] &gt;= 3, f'OrdinalClassifier needs at least 3 classes, only {self.classes.shape[0]} found'\n\n        for i in range(self.classes.shape[0]-1):\n            N_i = np.vectorize(int)(y &gt; self.classes[i])\n            learner = clone(self.learner).fit(X,N_i)\n            self.ordered_learners[i] = learner\n\n    def predict(self,X):\n        return np.vectorize(lambda i: self.classes[i])(np.argmax(self.predict_proba(X), axis=1))\n\n    def predict_proba(self,X):\n        predicted = [self.ordered_learners[k].predict_proba(X)[:,1].reshape(-1,1) for k in self.ordered_learners]\n\n        N_1 = 1-predicted[0]\n        N_K  = predicted[-1]\n        N_i= [predicted[i] - predicted[i+1] for i in range(len(predicted) - 1)]\n\n        probs = np.hstack([N_1, *N_i, N_K])\n\n        return probs\n\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = OrdinalClassifier(LogisticRegression())\nmodel.fit(X, Y_ord)\n\nmodel.score(X, Y_ord)\n\n0.975"
  },
  {
    "objectID": "pages/vis.html",
    "href": "pages/vis.html",
    "title": "Visualizations for Data Science: An Overview",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/vis.html#table-of-contents",
    "href": "pages/vis.html#table-of-contents",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIntroduction\nWhy Data Visualization Matters\nGetting Started with Matplotlib\n\nBasic Line Plot\nScatter Plot\nBar Chart\nHistogram\nBox Plot\nPie Chart\n\nEnhancing Visualizations with Seaborn\n\nSeaborn vs.¬†Matplotlib\nSeaborn‚Äôs Datasets\nSeaborn Styles\nCategorical Plots\nPair Plots\nHeatmaps\nFacetGrid\n\nReal-World Data Visualization Examples\n\nExploratory Data Analysis (EDA)\nTime Series Data Visualization\nGeographic Data Visualization\nAdvanced Plots for Correlation Analysis\n\nInteractive Visualizations\n\nPlotly: A Brief Introduction\nDash: Building Interactive Web Applications\nBokeh\n\nCustomizing and Styling Plots\n\nLabels, Titles, and Legends\nColor Palettes\nPlot Annotations\n\nConclusion"
  },
  {
    "objectID": "pages/vis.html#introduction",
    "href": "pages/vis.html#introduction",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Introduction",
    "text": "Introduction\nData visualization is an indispensable tool in the field of data science. It serves as a powerful means to convey information, explore data, make informed decisions, and communicate results effectively. This notebook aims to provide an academic and comprehensive guide to visualizing data using Matplotlib and Seaborn, two fundamental libraries in data science."
  },
  {
    "objectID": "pages/vis.html#why-data-visualization-matters",
    "href": "pages/vis.html#why-data-visualization-matters",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Why Data Visualization Matters",
    "text": "Why Data Visualization Matters\nHuman beings possess an innate ability to process and understand visual information more efficiently than textual or numerical data. Visualization leverages this cognitive advantage, enabling us to:\n\nGain Insight: Visualizations can reveal patterns, trends, and outliers in data that might be elusive in raw numbers.\nSimplify Complex Data: They simplify complex datasets, making them more comprehensible.\nTell a Story: Visualizations facilitate storytelling, making it easier to convey findings and insights to diverse audiences."
  },
  {
    "objectID": "pages/vis.html#getting-started-with-matplotlib",
    "href": "pages/vis.html#getting-started-with-matplotlib",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Getting Started with Matplotlib",
    "text": "Getting Started with Matplotlib\nIn this section, we will delve into the fundamentals of Matplotlib, one of the most widely used Python libraries for data visualization. Matplotlib provides a versatile framework for creating a wide range of static and animated plots, making it an indispensable tool for data scientists and students in the field of computer science and data science. We will cover several essential plot types with detailed end-to-end examples to help you grasp the concepts and practices effectively.\n\nBasic Line Plot\nA line plot is a fundamental type of visualization that is used to represent data points as a series of connected line segments. It is particularly useful for visualizing trends or patterns in data.\nLet‚Äôs create a simple line plot using Matplotlib with end-to-end code and explanations:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 20]\n\n# Create a line plot\nplt.plot(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Simple Line Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we import Matplotlib, define sample data for the x and y coordinates, create a line plot using plt.plot(), add labels and a title for clarity, and finally, display the plot using plt.show().\n\n\nScatter Plot\nScatter plots are effective for visualizing the relationship between two variables, making them ideal for data exploration and analysis. Each data point is represented as a dot on the plot.\nHere‚Äôs an end-to-end example of creating a scatter plot:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 20]\n\n# Create a scatter plot\nplt.scatter(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Scatter Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.scatter() to create a scatter plot, with the same labeling and title setup as before.\n\n\nBar Chart\nBar charts are an effective way to compare categories or discrete data points. They visually represent data using rectangular bars, with the length of each bar corresponding to the value of the data it represents.\nHere‚Äôs how you can create a bar chart from scratch:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\ncategories = ['A', 'B', 'C', 'D']\nvalues = [30, 50, 20, 40]\n\n# Create a bar chart\nplt.bar(categories, values)\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Bar Chart')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use plt.bar() to create a bar chart, customize it with labels and a title, and finally, display the chart.\n\n\nHistogram\nHistograms are essential for visualizing the distribution of a dataset, particularly in cases where the data‚Äôs frequency distribution is of interest.\nHere‚Äôs an end-to-end example of creating a histogram:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data\ndata = np.random.normal(0, 1, 1000)\n\n# Create a histogram\nplt.hist(data, bins=30)\n\n# Adding labels and title\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.hist() to create a histogram, where we first generate random data using NumPy, specify the number of bins for the histogram, and add labels and a title.\n\n\nBox Plot\nBox plots are excellent for visualizing the distribution and spread of data, helping to identify outliers and assess the central tendency and variability of a dataset.\nHere‚Äôs an example of creating a box plot:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data\ndata = np.random.normal(0, 1, 100)\n\n# Create a box plot\nplt.boxplot(data)\n\n# Adding labels and title\nplt.ylabel('Value')\nplt.title('Box Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use plt.boxplot() to create a box plot, generate random data for illustration, and include labels and a title.\n\n\nPie Chart\nPie charts are effective for displaying the proportion of different categories within a dataset. They are particularly useful for illustrating data in a way that emphasizes the relationship between parts and the whole.\nHere‚Äôs how to create a pie chart with Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nlabels = 'Category A', 'Category B', 'Category C', 'Category D'\nsizes = [15, 30, 45, 10]\n\n# Create a pie chart\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\n\n# Adding title\nplt.title('Pie Chart')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.pie() to create a pie chart, specify the labels and sizes of the categories, and include a title for clarity.\nThese examples demonstrate how to create various types of plots using Matplotlib, from basic line plots to more complex charts like histograms and pie charts. Matplotlib provides extensive customization options to tailor your visualizations to your specific needs."
  },
  {
    "objectID": "pages/vis.html#enhancing-visualizations-with-seaborn",
    "href": "pages/vis.html#enhancing-visualizations-with-seaborn",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Enhancing Visualizations with Seaborn",
    "text": "Enhancing Visualizations with Seaborn\nSeaborn is a powerful Python library that builds on Matplotlib and offers a high-level interface for creating informative and aesthetically pleasing statistical visualizations. In this section, we will delve into Seaborn‚Äôs capabilities and explore various aspects of enhancing data visualizations using this library. We will provide comprehensive end-to-end examples for each subtopic to illustrate how Seaborn can be employed effectively.\n\nSeaborn vs.¬†Matplotlib\nUnderstanding the Distinction\nBefore diving into Seaborn, it‚Äôs essential to understand the key differences between Seaborn and Matplotlib. While Matplotlib is a versatile but somewhat low-level library for creating plots, Seaborn is designed for statistical data visualization and offers:\n\nSimplified syntax and concise API.\nBuilt-in themes and color palettes for better aesthetics.\nSpecialized functions for creating complex plots with minimal code.\n\nExample: Comparing Matplotlib and Seaborn\nLet‚Äôs illustrate the difference with a basic example. We‚Äôll create a * simple histogram using both Matplotlib and Seaborn.\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(0, 1, 1000)\nplt.hist(data, bins=30)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram (Matplotlib)')\nplt.show()\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport numpy as np\n\ndata = np.random.normal(0, 1, 1000)\nsns.histplot(data, bins=30, kde=True)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram (Seaborn)')\nplt.show()\n\n\n\n\nIn this example, Seaborn simplifies the creation of a histogram with an optional kernel density estimation (KDE) curve, enhancing both readability and aesthetics.\n\n\nSeaborn‚Äôs Datasets\nSeaborn comes with several built-in datasets that are useful for practice and experimentation. These datasets cover a wide range of scenarios and are readily available for analysis and visualization.\nLet‚Äôs load the famous iris dataset provided by Seaborn and create a pair plot to explore the relationships between the features.\n\nimport seaborn as sns\n\n# Load the iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Create a pair plot for exploring feature relationships\nsns.pairplot(iris, hue=\"species\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we load the ‚Äúiris‚Äù dataset and use Seaborn to create a pair plot that visualizes the relationships between the various species of iris flowers. The hue parameter allows us to distinguish different species with color.\n\n\nSeaborn Styles\nSeaborn provides various built-in styles to improve the aesthetics of your plots. You can easily set the style using the sns.set_style() function.\nLet‚Äôs change the plotting style using Seaborn‚Äôs built-in styles and visualize the same data with different styles.\n\nimport seaborn as sns\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a violin plot with different styles\nsns.set_style(\"darkgrid\")\nsns.violinplot(x=\"day\", y=\"total_bill\", data=tips, hue=\"day\", palette=\"Set1\", inner=\"stick\", split=True, legend=False)\nplt.title('Violin Plot with \"darkgrid\" Style')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n# Change the style to \"whitegrid\"\nsns.set_style(\"whitegrid\")\nsns.violinplot(x=\"day\", y=\"total_bill\", data=tips, hue=\"day\", palette=\"Set1\", inner=\"stick\", split=True, legend=False)\nplt.title('Violin Plot with \"whitegrid\" Style')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we change the plotting style from ‚Äúdarkgrid‚Äù to ‚Äúwhitegrid‚Äù and visualize the same data using a violin plot. Seaborn‚Äôs styles offer visual diversity to suit your preferences and the context of your data.\n\n\nCategorical Plots\nSeaborn offers a range of categorical plots for data exploration. These plots are particularly useful when dealing with categorical or discrete data. We‚Äôll demonstrate the creation of a bar plot to visualize the average tips given by day.\nLet‚Äôs use Seaborn to create a bar plot that shows the average tips given on different days of the week.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a bar plot to visualize the average tips by day\nsns.barplot(x=\"day\", y=\"tip\", data=tips)\n\n# Adding labels and title\nplt.xlabel('Day')\nplt.ylabel('Average Tip')\nplt.title('Average Tip by Day')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use Seaborn‚Äôs barplot function to create a bar plot that displays the average tips given on different days. The ci parameter is set to None to remove confidence intervals.\n\n\nPair Plots\nPair plots are an excellent tool for visualizing relationships between variables in a dataset. They provide a quick overview of how features are related to each other.\nLet‚Äôs create a pair plot to visualize the relationships between different numerical features in the iris datas\n\nimport seaborn as sns\n\n# Load the iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Create a pair plot to explore feature relationships\nsns.pairplot(iris, hue=\"species\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use Seaborn‚Äôs pairplot function to create a pair plot that illustrates how different species of iris flowers are related based on features like sepal length, sepal width, petal length, and petal width. The hue parameter allows us to distinguish different species with color.\n\n\nHeatmaps\nHeatmaps are ideal for displaying relationships between data points. They are particularly useful for visualizing correlations between variables.\nLet‚Äôs create a heatmap to visualize the correlation matrix of the ‚Äútips‚Äù dataset, which shows how different numerical features are correlated.\n\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Encode categorical variables\ntips_encoded = pd.get_dummies(tips, columns=[\"sex\", \"smoker\", \"day\", \"time\"])\n\n# Create a correlation matrix\ncorrelation_matrix = tips_encoded.corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True)\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we calculate the correlation matrix of the ‚Äútips‚Äù dataset and use Seaborn to create a heatmap that visualizes the correlations between features. The annot parameter is set to True to display the correlation values on the heatmap.\n\n\nFacetGrid\nFacetGrid in Seaborn allows you to create a grid of subplots based on the values of one or more variables. It is useful for visualizing relationships in subgroups of data.\nLet‚Äôs create a FacetGrid to visualize the relationship between the total bill and tip, differentiating by the time of day and whether the customer is a smoker.\n\nimport seaborn as sns\n\n# Create a FacetGrid\ng = sns.FacetGrid(tips, col=\"time\", row=\"smoker\")\n\n# Map a scatter plot to the grid\ng.map(sns.scatterplot, \"total_bill\", \"tip\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use a FacetGrid to create a grid of scatter plots. The FacetGrid is segmented by the time of day (lunch or dinner) and whether the customer is a smoker or not. This allows us to explore the relationship between the total bill and tip for different subsets of the data.\nSeaborn‚Äôs capabilities extend far beyond what is covered in this section. It is a versatile library that empowers data scientists to create visually appealing and informative visualizations with ease. Experiment with Seaborn to discover its full potential and enhance your data analysis and presentation."
  },
  {
    "objectID": "pages/vis.html#real-world-data-visualization-examples",
    "href": "pages/vis.html#real-world-data-visualization-examples",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Real-World Data Visualization Examples",
    "text": "Real-World Data Visualization Examples\n\nExploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) is the initial phase of data analysis where we aim to understand the dataset‚Äôs structure, detect anomalies, and identify initial trends. Visualization is a key component of EDA. Let‚Äôs consider a real-world dataset, the ‚ÄúIris‚Äù dataset, and perform EDA using Seaborn.\n\nLoad the Dataset:\nWe start by loading the Iris dataset, a classic dataset in data science, which contains measurements of three different species of iris flowers: setosa, versicolor, and virginica.\n\n\nimport seaborn as sns\n\n# Load the Iris dataset\niris = sns.load_dataset(\"iris\")\n\n\nUnivariate Analysis:\nWe can begin by visualizing the distribution of a single variable. For instance, let‚Äôs create a histogram to understand the distribution of petal lengths for all three species:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram\nsns.histplot(data=iris, x=\"petal_length\", hue=\"species\")\nplt.title(\"Petal Length Distribution by Species\")\nplt.show()\n\n\n\n\nThis histogram provides insights into the petal length distribution for each species.\n\nBivariate Analysis:\nBivariate analysis helps in understanding relationships between two variables. We can create a pair plot to visualize pairwise relationships between numeric variables:\n\n\nimport seaborn as sns\n\n# Create a pair plot\nsns.pairplot(iris, hue=\"species\")\n\n\n\n\nThe pair plot shows scatterplots for all possible pairs of numeric features and provides a quick overview of how variables relate to each other.\n\nMultivariate Analysis:\nFor a more comprehensive view, we can use a heatmap to visualize the correlation between numeric features:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Filter the DataFrame to include only numeric columns\nnumeric_columns = iris.select_dtypes(include=['float64'])\n\n# Create a correlation matrix\ncorr_matrix = numeric_columns.corr()\n\n# Create a heatmap\nsns.heatmap(corr_matrix, annot=True)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\nThe heatmap visually represents the correlation between different features. This can be especially helpful when dealing with high-dimensional datasets.\n\n\nTime Series Data Visualization\nTime series data often involves data points recorded at regular intervals, such as stock prices over time. Let‚Äôs visualize stock price data for a hypothetical company using Seaborn.\n\nLoad the Time Series Data:\nWe‚Äôll create a dataset with timestamps and stock prices for a fictional company:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndata = {\n    'date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n    'price': 100 + 2 * np.random.randn(100)\n}\n\nstock_data = pd.DataFrame(data)\n\n\nVisualize Stock Prices Over Time:\nNext, we can create a line plot to visualize how the stock price of the company changes over time:\n\n\n# Create a line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=\"date\", y=\"price\", data=stock_data)\nplt.title(\"Stock Price Over Time\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThis line plot provides a visual representation of how the stock price fluctuates over the specified time period.\n\n\nGeographic Data Visualization\nVisualizing geographic data is crucial when working with location-based information. Let‚Äôs consider a simple example of visualizing cities on a map.\n\nPrepare Geographic Data:\nSuppose you have a dataset with information about cities, including their names, latitudes, and longitudes.\n\n\nimport pandas as pd\n\ndata = {\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n    'Latitude': [40.7128, 34.0522, 41.8781, 29.7604],\n    'Longitude': [-74.0060, -118.2437, -87.6298, -95.3698]\n}\n\ncities_data = pd.DataFrame(data)\n\n\nVisualize Cities on a Map:\nTo visualize these cities on a map, you can use libraries like Folium or geospatial data visualization tools. Here‚Äôs a simplified example using Folium:\n\n\nimport folium\n\n# Create a map object centered on the United States\nm = folium.Map(location=[37.0902, -95.7129], zoom_start=4)\n\n# Add markers for each city\nfor index, row in cities_data.iterrows():\n    folium.Marker([row['Latitude'], row['Longitude']], popup=row['City']).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThis code generates a map with markers representing the cities‚Äô locations.\n\n\nAdvanced Plots for Correlation Analysis\nFor advanced correlation analysis, Seaborn provides various plots. Let‚Äôs consider a scenario where we want to explore the correlation between features in a real-world dataset.\n\nLoad the Dataset:\nWe can load a dataset that contains numeric variables to analyze their correlation. For example, we can use Seaborn‚Äôs built-in ‚Äúdiamonds‚Äù dataset:\n\n\nimport seaborn as sns\n\n# Load the Diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n\nCreate Advanced Correlation Plots:\nSeaborn offers advanced plots for correlation analysis. For instance, we can create a pair plot with regression lines to understand relationships between numeric features, considering factors like carat, cut, and price:\n\n\nimport seaborn as sns\n\n# Create a pair plot with regression lines\nsns.pairplot(diamonds, vars=['carat', 'price'], hue='cut', kind='reg')\n\n\n\n\nThis pair plot provides insights into how carat, price, and cut are correlated.\nThese examples demonstrate how to apply data visualization techniques to real-world scenarios, such as exploratory data analysis, time series data, geographic data, and advanced correlation analysis. Effective visualization is essential for gaining insights and making data-driven decisions in data science and analysis."
  },
  {
    "objectID": "pages/vis.html#interactive-visualizations",
    "href": "pages/vis.html#interactive-visualizations",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\nIn the world of data science, static visualizations are undeniably powerful for understanding and communicating insights. However, there are times when you need to take your data visualization to the next level by making it interactive. Interactive visualizations allow users to explore data on their terms, providing a dynamic and engaging experience. In this section, we will explore two prominent libraries for creating interactive visualizations: Plotly and Dash, and Bokeh.\n\nPlotly: A Brief Introduction\nPlotly is a versatile library for creating interactive, web-based visualizations. It supports a wide range of chart types and is known for its user-friendly API. Here‚Äôs an end-to-end example of creating an interactive line plot using Plotly:\n\nimport plotly.express as px\n\n# Sample data\nimport pandas as pd\ndata = pd.DataFrame({\n    'X': [1, 2, 3, 4, 5],\n    'Y': [10, 15, 13, 18, 20]\n})\n\n# Create an interactive line plot\nfig = px.line(data, x='X', y='Y', title='Interactive Line Plot')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nIn this example, we use Plotly Express to create a line plot from a pandas DataFrame. The resulting plot is interactive, allowing users to zoom, pan, and hover over data points for more information.\n\n\nDash: Building Interactive Web Applications\nDash is a framework built on top of Plotly that enables you to create interactive web applications for data visualization. Dash allows you to build interactive dashboards, data exploration tools, and more. Here‚Äôs a simple example of a Dash web application that displays a dynamic line plot:\n\nimport dash\nfrom dash import dcc\nfrom dash import html\nfrom dash.dependencies import Input, Output\nimport pandas as pd\nimport plotly.graph_objs as go\n\n# Sample data\ndata = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [10, 15, 13, 18, 20]})\n\n# Create a Dash web application\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    html.H1('Interactive Dash Line Plot'),\n    dcc.Graph(id='line-plot'),\n])\n\n@app.callback(\n    Output('line-plot', 'figure'),\n    [Input('line-plot', 'relayoutData')]\n)\ndef update_line_plot(relayoutData):\n    # Your data processing logic here\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=data['X'], y=data['Y'], mode='lines'))\n    fig.update_layout(title='Interactive Line Plot')\n    return fig\n\nif __name__ == '__main__':\n    # app.run_server(debug=True)    # Uncomment this line to run the server\n    pass\n\nIn this example, we create a Dash web application that renders an interactive line plot. Users can interact with the plot, and any changes they make are reflected dynamically. Dash provides extensive capabilities for building custom, interactive data applications.\n\n\nBokeh\nBokeh is another library for creating interactive visualizations. It is designed for constructing interactive plots, dashboards, and applications in Python. Bokeh offers a high level of interactivity and customization. Here‚Äôs an example of creating an interactive scatter plot with Bokeh:\n\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.models import ColumnDataSource, HoverTool\nimport pandas as pd\n\n# Output the plot to the Jupyter Notebook\noutput_notebook()\n\n# Sample data\ndata = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [10, 15, 13, 18, 20]})\n\n# Create a Bokeh figure\np = figure(title=\"Interactive Scatter Plot\", tools=\"pan,box_zoom,reset,hover\")\n\n# Add data source\nsource = ColumnDataSource(data=data)\n\n# Create a scatter plot\nscatter = p.circle(x='X', y='Y', source=source, size=10)\n\n# Add hover tool for interactivity\nhover = HoverTool()\nhover.tooltips = [(\"X\", \"@X\"), (\"Y\", \"@Y\")]\np.add_tools(hover)\n\n# Show the plot within the Jupyter Notebook\nshow(p, notebook_handle=True)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\n  \n\n\n\n\n\n&lt;Bokeh Notebook handle for In[28]&gt;\n\n\nIn this Bokeh example, we create an interactive scatter plot with a hover tool that displays data values when hovering over data points. Bokeh provides a wide range of interactive tools and widgets to enhance your visualizations.\nInteractive visualizations with Plotly, Dash, and Bokeh open up exciting possibilities for data exploration, analysis, and presentation. They allow users to dive deeper into the data and interact with visualizations in a way that static plots cannot achieve. Whether you need to build interactive dashboards, explore complex datasets, or create dynamic reports, these libraries are valuable tools in your data science toolkit."
  },
  {
    "objectID": "pages/vis.html#customizing-and-styling-plots",
    "href": "pages/vis.html#customizing-and-styling-plots",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Customizing and Styling Plots",
    "text": "Customizing and Styling Plots\nCustomizing and styling plots is a critical aspect of data visualization that significantly enhances the clarity and aesthetics of your visualizations. In this section, we will explore three key subtopics:\n\nLabels, Titles, and Legends\nColor Palettes\nPlot Annotations\n\nWe will provide detailed end-to-end examples for each subtopic to demonstrate their importance in creating informative and visually appealing data visualizations.\n\nLabels, Titles, and Legends\nLabels, titles, and legends are essential components of a well-structured data visualization. They provide context and help the audience understand the information presented. Let‚Äôs look at an example using Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny1 = [10, 15, 13, 18, 20]\ny2 = [5, 8, 6, 9, 10]\n\n# Create a line plot with labels and legends\nplt.plot(x, y1, label='Series A')\nplt.plot(x, y2, label='Series B')\n\n# Adding labels, title, and legend\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Customizing Labels, Titles, and Legends')\nplt.legend()\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we have added labels to the x and y-axes, a title to the plot, and a legend to differentiate between two series. These components make the visualization self-explanatory.\n\n\nColor Palettes\nChoosing the right color palette is crucial for improving the visual appeal of your plots. Seaborn provides various color palettes to suit different types of data. Here‚Äôs an example using Seaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = sns.load_dataset(\"iris\")\n\n# Create a pair plot with a custom color palette\ncustom_palette = ['red', 'green', 'blue']\nsns.set_palette(custom_palette)\nsns.pairplot(data, hue='species')\n\n# Adding a title\nplt.suptitle('Custom Color Palette for Pair Plot', y=1.02)\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we‚Äôve selected a custom color palette to style a pair plot, making it visually appealing and distinctive. Seaborn‚Äôs palettes offer a wide range of choices to fit the tone and theme of your visualizations.\n\n\nPlot Annotations\nAnnotations are valuable for highlighting specific data points or features within your visualizations. They improve interpretability and guide the viewer‚Äôs attention. Here‚Äôs an example using Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 16]\n\n# Create a line plot\nplt.plot(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Plot with Annotations')\n\n# Annotating a data point\nplt.annotate('Peak Value', xy=(4, 18), xytext=(4.1, 16),\n             arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we‚Äôve added an annotation to highlight a specific data point in the plot. Annotations can be used to provide additional context or emphasize key findings.\nCustomizing and styling plots not only enhances the aesthetics but also aids in conveying your data-driven message effectively. These techniques are valuable in creating professional and informative data visualizations."
  },
  {
    "objectID": "pages/vis.html#conclusion",
    "href": "pages/vis.html#conclusion",
    "title": "Visualizations for Data Science: An Overview",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, data visualization plays a pivotal role in data science, enabling data practitioners to explore, analyze, and communicate their findings effectively. This notebook has provided a comprehensive overview of data visualization using Matplotlib and Seaborn, from basic plots to advanced techniques. We encourage you to practice and apply your knowledge to real-world projects, as data visualization is an essential skill for any data scientist or analyst."
  },
  {
    "objectID": "pages/Bivariate_EM.html",
    "href": "pages/Bivariate_EM.html",
    "title": "EM Algorithm",
    "section": "",
    "text": "Colab: Click here!\n\n!wget https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/hw/hw02/old_faithful.csv\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom scipy.stats import multivariate_normal\nimport pandas as pd\nfrom matplotlib import mlab\n\nX = pd.read_csv('old_faithful.csv')[['eruptions', 'waiting']].to_numpy()\nplt.scatter(X[:, 0], X[:, 1])\n\nM, N = np.mgrid[min(X[:, 0])-1:max(X[:, 0])+1:0.01, min(X[:, 1])-1:max(X[:, 1])+1:0.01]\ngrid = np.dstack((M, N))\n\n--2023-11-09 15:07:06--  https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/hw/hw02/old_faithful.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2270 (2.2K) [text/plain]\nSaving to: ‚Äòold_faithful.csv.5‚Äô\n\nold_faithful.csv.5    0%[                    ]       0  --.-KB/s               old_faithful.csv.5  100%[===================&gt;]   2.22K  --.-KB/s    in 0s      \n\n2023-11-09 15:07:06 (38.5 MB/s) - ‚Äòold_faithful.csv.5‚Äô saved [2270/2270]\n\n\n\n\n\n\n\nK = 2\n\nrng = np.random.default_rng()\ncentres = X[rng.integers(low=0, high=len(X), size=K)]\nl = np.array([(lambda i: [int(i==j) for j in range(K)])(np.argmin([np.linalg.norm(p-centre) for centre in centres])) for p in X])\npi = l.sum(axis=0)/l.sum()\n\ncov = []\nfor i in range(K):\n  cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\ncov = np.array(cov)\n\ndef plot():\n\n  plt.scatter(X[:, 0], X[:, 1], color='black', marker='+')\n  plt.scatter(centres[:, 0], centres[:, 1], color='red', marker='x')\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  plt.contour(M, N, probability_grid)\n  plt.show()\n\nplot()\n\n\n\n\n\n# Expectation Step\ndef Exp(l):\n  pi = l.sum(axis=0)/l.sum()\n  centres, cov = [], []\n  for i in range(K):\n    centres.append(np.dot(l[:, i], X)/l[:, i].sum())\n    cov.append(np.cov(X.T, ddof=0, aweights=l[:, i]))\n\n  return pi, np.array(centres), np.array(cov)\n\n# Maximization step\ndef Max(pi, centres, cov):\n  l = []\n  for i in X:\n    p = np.array([pi[k] * multivariate_normal.pdf(i, mean=centres[k], cov=cov[k]) for k in range(K)])\n    p = p/p.sum()\n    l.append(p)\n\n  return np.array(l)\n\n# Convergence criterion\nnorm_theta = lambda pi, centres, cov: np.linalg.norm(np.r_[pi, centres.reshape(-1), cov.reshape(-1)])\n\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\nprev_norm = norm_theta(pi, centres, cov)\n\nfig, ax = plt.subplots()\nartists = []\n\nwhile True:\n  l = Max(pi, centres, cov)\n  pi, centres, cov = Exp(l)\n\n  frame = []\n  frame.append(ax.scatter(X[:, 0], X[:, 1], color='black', marker='+'))\n  frame.append(ax.scatter(centres[:, 0], centres[:, 1], color='red', marker='x'))\n\n  probability_grid = np.zeros(grid.shape[:2])\n  for i in range(K):\n    probability_grid += pi[i] * multivariate_normal(centres[i], cov[i]).pdf(grid)\n\n  frame += list(ax.contour(M, N, probability_grid).collections)\n  artists.append(frame)\n\n  curr_norm = norm_theta(pi, centres, cov)\n\n  if abs(curr_norm-prev_norm) &lt; 1:\n    break\n  else:\n    prev_norm = curr_norm\n\nplt.close()\nanim = animation.ArtistAnimation(fig, artists, interval=200, repeat=False, blit=False);\nHTML(anim.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html",
    "href": "pages/dealing_with_missing_values.html",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#introduction",
    "href": "pages/dealing_with_missing_values.html#introduction",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#bias",
    "href": "pages/dealing_with_missing_values.html#bias",
    "title": "Dealing with missing-values",
    "section": "Bias",
    "text": "Bias\nWhen data is missing, the remaining data may not always accurately reflect the true population. The type of missingness and the way we deal with it can dictate bias in our results.\nFor example, in a survey recording income of individuals, those with low income may choose not to respond. The use of average income to impute the missing values can lead to bias as the average of the available data may not represent that of the population."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-completely-at-random-mcar",
    "href": "pages/dealing_with_missing_values.html#missing-completely-at-random-mcar",
    "title": "Dealing with missing-values",
    "section": "Missing Completely At Random (MCAR)",
    "text": "Missing Completely At Random (MCAR)\nIn situations characterized by Missing Completely At Random (MCAR), the absence of data occurs randomly and is unrelated to any variable in the dataset or the missing values themselves. The probability of missingness is same for all observations. There exists no underlying pattern to the missingness. The missing values are completely independent of other data. All statistical analysis performed on the dataset will remain unbiased in this case.\nFor example, during data collection, if some responses were not collected due to a technical error, then the missing data is completely at random."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-at-random-mar",
    "href": "pages/dealing_with_missing_values.html#missing-at-random-mar",
    "title": "Dealing with missing-values",
    "section": "Missing At Random (MAR)",
    "text": "Missing At Random (MAR)\nIn instances of Missing At Random (MAR), the absence of data can be entirely explained by the values of other known variables in the dataset. There exists some form of pattern in the missing values.\nFor example, In a survey, women might be unwilling to disclose their age. Here the missingness of the variable age can be explained by another observed variable ‚Äúgender‚Äù."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-not-at-random-mnar",
    "href": "pages/dealing_with_missing_values.html#missing-not-at-random-mnar",
    "title": "Dealing with missing-values",
    "section": "Missing Not At Random (MNAR)",
    "text": "Missing Not At Random (MNAR)\nIn this case, the missingness is neither MCAR nor MAR. The fact that a datapoint is missing is dependent on the value of the data point. In order to correct for the bias we would have to make modelling assumptions about the nature of the bias.\nFor example, in a social survey where individuals are asked about their income, respondents may not disclose it if it is too high or too low. Thus the missingness in the feature income is directly related to the values of that feature.\nFor example, if a patient‚Äôs measurement was not taken because the doctor felt he was too sick, that observation would not be MAR or MCAR. In this case the missing data mechanism causes our observed training data to give a distorted picture of the true population, and data imputation is dangerous in this instance."
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#identifying-the-type-of-missingness",
    "href": "pages/dealing_with_missing_values.html#identifying-the-type-of-missingness",
    "title": "Dealing with missing-values",
    "section": "Identifying the type of missingness",
    "text": "Identifying the type of missingness\n\nUnderstanding the data collection process, the features involved and the research domain can help identify possible patterns as to why data is missing\nGraphical representation of the missing data and heatmaps can help identify relationships between features that can be utilized to make better imputation of the missing data\nWe can impute the data using different techniques while also making assumptions on the nature of missingness. Subsequently, we analyze the results to observe the assumptions that have led to consistent results.\nFor categorical features, we can code missing as an additional class(feature). We can then fit our model on the training data and see if the class ‚ÄúMissing‚Äù is predictive of the response(label)"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#dealing-with-missing-values",
    "href": "pages/dealing_with_missing_values.html#dealing-with-missing-values",
    "title": "Dealing with missing-values",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nIf we assume that the features are missing completely at random, there are three approaches that we can follow\n\nDiscard the observations with any missing values\nChoose an algorithm that inherently deals with missing values\nImpute the missing values before training"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#missing-value-imputation",
    "href": "pages/dealing_with_missing_values.html#missing-value-imputation",
    "title": "Dealing with missing-values",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\nIn many cases it might not be feasible to discard observations that contain missing values. This could be due to the availability of lesser number of samples or due to the importance of each observation. In such cases we can employ imputation which deals with replacing the missing values with some predicted value. We will have a look at two simple imputation techniques that can be implemented using the sklearn package.\n\nSimple Imputer\nK Nearest neighbours Imputer"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#simple-imputer",
    "href": "pages/dealing_with_missing_values.html#simple-imputer",
    "title": "Dealing with missing-values",
    "section": "Simple Imputer",
    "text": "Simple Imputer\nSimple Imputation is a univariate imputation technique where the missing values of a feature are imputed using the non-missing values of the same feature. It offers a basic approach to filling missing data wherein we replace the missing data with a constant value or utilize statistics such as ‚Äúmean‚Äù, ‚Äúmode‚Äù, or ‚Äúmedian‚Äù of the available values.\nThe technique is useful for its simplicity and can serve as a reference technique. It is also worth noting that this can lead to biased or unrealistic results\n\nSimple Imputer - Mean\n\nimport numpy as np\nimport pandas as pd\n\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mean strategy. The mean value of that feature will be \\frac{( 1+1+2+4)}{4} = 2\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 2 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\ndata = {\"feature_1\":[1,2,3,4,5],\n        \"feature_2\":[1,1,np.nan,2,4],\n        \"feature_3\":[3,3,3,4,5]}\n\n\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1\n1.0\n3\n\n\n1\n2\n1.0\n3\n\n\n2\n3\nNaN\n3\n\n\n3\n4\n2.0\n4\n\n\n4\n5\n4.0\n5\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom sklearn.impute import SimpleImputer\n\n\nimputer = SimpleImputer(strategy='mean')\nimputed_data = imputer.fit_transform(df)\n\n\nimputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nimputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n2.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSimple Imputer - Mode\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mode strategy. The value that appears most often is 1.\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nmode_imputer = SimpleImputer(strategy='most_frequent')\nimputed_data = mode_imputer.fit_transform(df)\n\n\nmode_imputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nmode_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#k-nearest-neighbours",
    "href": "pages/dealing_with_missing_values.html#k-nearest-neighbours",
    "title": "Dealing with missing-values",
    "section": "K Nearest Neighbours",
    "text": "K Nearest Neighbours\nThis technique operates on the principle of finding the K most similar data points to the one with missing values and using their known values to estimate and impute the missing values.\nKNN imputation is especially useful when there is a pattern or structure in the data that can be captured by considering the relationships between neighboring data points. It‚Äôs a flexible method that can be applied to various types of data, but the choice of K and the distance metric are critical parameters that can impact imputation accuracy.\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix}   We can fill the missing value using KNN Imputer with value of k = 2. Firstly, we need to find the distance of our entry(row) containing the missing value, with other entries(rows).   The distance of \\begin{bmatrix} 3 & nan & 3 \\end{bmatrix} with \\begin{bmatrix} 1 & 1 & 3 \\end{bmatrix} is given by ,  \\displaystyle \\sqrt{\\frac{3}{2}\\left(( 3-1)^{2} +( 3-3)^{2}\\right)} =\\ \\sqrt{6}  In this example the nearest neighbors based on Euclidean distance will be the points \\begin{bmatrix} 2 & 1 & 3 \\end{bmatrix} and \\begin{bmatrix} 4 & 2 & 4 \\end{bmatrix}.Now to fill the missing value, we take the average of the values of the feature from the 2 nearest neighbors identified above.\n\n\\begin{align*}\n\\frac{\\text{Sum of values of the feature from 2 neares neighbors}}{\\text{Number of values}} & =\\frac{1+2}{2} \\ =\\ 1.5\n\\end{align*}\n\nThus the updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1.5 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nfrom sklearn.impute import KNNImputer\n\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nknn_imputed_data = knn_imputer.fit_transform(df)\n\n\nknn_imputed_df = pd.DataFrame(knn_imputed_data, columns=['feature_1','feature_2','feature_3'])\nknn_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.5\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#exercises",
    "href": "pages/dealing_with_missing_values.html#exercises",
    "title": "Dealing with missing-values",
    "section": "Exercises",
    "text": "Exercises\nUse the links provided below to access the datasets. Treat the dataset like an unsupervised learning problem with the only ‚Äúconstraint‚Äù being that findings/results should pertain to missingness. Share your findings/results in a ipynb file.\n\nLink to dataset 1: https://drive.google.com/file/d/1Fk5V6GNNfdgm8qB4sA1EinB9gQhNdg4Z/view?usp=drive_link\nLink to dataset 2: https://drive.google.com/file/d/15HYh3p6SZjic_Ytef84b1vsgYcWoqh4L/view?usp=drive_link"
  },
  {
    "objectID": "pages/dealing_with_missing_values.html#additional-info",
    "href": "pages/dealing_with_missing_values.html#additional-info",
    "title": "Dealing with missing-values",
    "section": "Additional Info",
    "text": "Additional Info\n\nDealing with missingness in Tree-Based Methods\nFor tree based models we can try a couple of different approaches\nApproach 1: The first is applicable to categorical predictors: we simply make a new category for ‚Äúmissing.‚Äù From this we might discover that observations with missing values for some measurement behave differently than those with nonmissing values.\nApproach 2: The second more general approach is the construction of surrogate variables. Surrogate splits use alternative variables when the primary variable has missing values. The idea is to leverage the correlation or similarity between variables, allowing them to substitute for each other in the splitting process. This enables the inclusion of records with missing values in the training data, ensuring they are split based on the best available variable. The effectiveness of this technique is dependent on the quality and availability of surrogate variables.\n\n\nMissingness Indicator\nThe idea of missingness Indicator is to make use of a binary indicator that highlights the missing values in your dataset. This can be used when we want the ML models to capture information about the missingness\n\n\nEstimators that handle NaN values\nCART, MARS, PRIM, GBM"
  }
]
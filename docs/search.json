[
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html",
    "href": "pages/Scaling multi-dataset multi-algo.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "href": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "href": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "href": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Datasets",
    "text": "Loading the Datasets\nWe start by loading the four datasets and inspecting their structures.\n\n# Load the datasets\niris = load_iris()\ndigits = load_digits()\nwine = load_wine()\nbreast_cancer = load_breast_cancer()\n\n# Create DataFrames for the datasets\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\ndigits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\nbreast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n\n# Display the first few rows of iris dataset\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\n# Display the first few rows of digits dataset\ndigits_df.head()\n\n\n\n\n\n\n\n\npixel_0_0\npixel_0_1\npixel_0_2\npixel_0_3\npixel_0_4\npixel_0_5\npixel_0_6\npixel_0_7\npixel_1_0\npixel_1_1\n...\npixel_6_7\npixel_7_0\npixel_7_1\npixel_7_2\npixel_7_3\npixel_7_4\npixel_7_5\npixel_7_6\npixel_7_7\ntarget\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n1.0\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n2.0\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n3.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n4.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n\n# Display the first few rows of wine dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\n\n# Display the first few rows of breast cancer dataset\nbreast_cancer_df.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows × 31 columns\n\n\n\nThe datasets contain various features related to their respective domains, with a ‘target’ column indicating the class labels."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "href": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Define a function to create noisy datasets\ndef create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n    X = dataset.data\n    y = dataset.target\n\n    np.random.seed(random_state)\n    noise = np.random.normal(0, noise_std, size=X.shape)\n    X_noisy = X + noise\n\n    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n\n    return X_train_noisy, X_test_noisy, y_train, y_test\n\n# Create noisy datasets for all four datasets\nX_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\nX_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\nX_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\nX_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "href": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (\\(SS\\)) transforms the data so that it has a mean (\\(\\mu\\)) of 0 and a standard deviation (\\(\\sigma\\)) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\\[\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\\]\nwhere \\(x\\) is the original feature vector, \\(\\mu\\) is the mean of the feature vector, and \\(\\sigma\\) is the standard deviation of the feature vector.\n\n# Define a function to apply Standard Scaler to a dataset\ndef apply_standard_scaler(X_train, X_test):\n    standard_scaler = StandardScaler()\n    X_train_scaled = standard_scaler.fit_transform(X_train)\n    X_test_scaled = standard_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Standard Scaler to all four datasets\nX_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (\\(MMS\\)) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\\[\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\\]\nwhere \\(x\\) is the original feature vector, \\(x_{min}\\) is the smallest value in the feature vector, and \\(x_{max}\\) is the largest value in the feature vector.\n\n# Define a function to apply Min-max Scaler to a dataset\ndef apply_min_max_scaler(X_train, X_test):\n    min_max_scaler = MinMaxScaler()\n    X_train_scaled = min_max_scaler.fit_transform(X_train)\n    X_test_scaled = min_max_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Min-max Scaler to all four datasets\nX_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (\\(MAS\\)) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\\[\nMAS(x) = \\frac{x}{|x_{max}|}\n\\]\nwhere \\(x\\) is the original feature vector, and \\(x_{max, abs}\\) is the maximum absolute value in the feature vector.\n\n# Define a function to apply Maximum Absolute Scaler to a dataset\ndef apply_max_abs_scaler(X_train, X_test):\n    max_abs_scaler = MaxAbsScaler()\n    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n    X_test_scaled = max_abs_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Maximum Absolute Scaler to all four datasets\nX_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (\\(RS\\)) scales the data using the median (\\(Q_2\\)) and the interquartile range (\\(IQR\\), \\(Q_3 - Q_1\\)), making it robust to outliers. The transformation is given by:\n\\[\nRS(x) = \\frac{x - Q_2}{IQR}\n\\]\nwhere \\(x\\) is the original feature vector, \\(Q_2\\) is the median of the feature vector, and \\(IQR\\) is the interquartile range of the feature vector.\n\n# Define a function to apply Robust Scaler to a dataset\ndef apply_robust_scaler(X_train, X_test):\n    robust_scaler = RobustScaler()\n    X_train_scaled = robust_scaler.fit_transform(X_train)\n    X_test_scaled = robust_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Robust Scaler to all four datasets\nX_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (\\(QT\\)) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\\[\nQT(x) = F^{-1}(F(x))\n\\]\nwhere \\(F(x)\\) is the cumulative distribution function of the data, and \\(F^{-1}\\) is the inverse function of \\(F\\).\n\n# Define a function to apply Quantile Transformer to a dataset\ndef apply_quantile_transformer(X_train, X_test):\n    quantile_transformer = QuantileTransformer(output_distribution='normal')\n    X_train_scaled = quantile_transformer.fit_transform(X_train)\n    X_test_scaled = quantile_transformer.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Quantile Transformer to all four datasets\nX_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "href": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of six classification models on the different scaled datasets. The models we will use are:\n\nRandom Forest\nSupport Vector Machine (SVM)\nDecision Tree\nNaive Bayes (GaussianNB)\nK-Nearest Neighbors (KNN)\nLogistic Regression\n\nFor each scaling method, we will train and evaluate all six models for all four datasets.\n\n# Initialize the classifiers\nrf_classifier = RandomForestClassifier(random_state=42)\nsvm_classifier = SVC(random_state=42)\ndt_classifier = DecisionTreeClassifier(random_state=42)\nnb_classifier = GaussianNB()\nknn_classifier = KNeighborsClassifier()\nlr_classifier = LogisticRegression()\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each dataset and scaling method, and evaluate the models\ndatasets = [\n    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n]\n\nscaling_methods = {\n    \"No Scaling\": {\n        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n    },\n    \"Standard Scaler\": {\n        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n    },\n    \"Min-max Scaler\": {\n        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n    },\n    \"Maximum Absolute Scaler\": {\n        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n    },\n    \"Robust Scaler\": {\n        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n    },\n    \"Quantile Transformer\": {\n        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n    }\n}\n\n# Loop through datasets and scaling methods\nfor dataset_name, X_train, X_test, y_train, y_test in datasets:\n    for scaler_name, scaled_data in scaling_methods.items():\n        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n\n        # Train and evaluate all six models\n        for classifier, classifier_name in zip([rf_classifier, svm_classifier, dt_classifier, nb_classifier, knn_classifier, lr_classifier], [\"Random Forest\", \"SVM\", \"Decision Tree\", \"Naive Bayes\", \"K-Nearest Neighbors\", \"Logistic Regression\"]):\n            classifier.fit(X_train_scaled, y_train)\n            predictions = classifier.predict(X_test_scaled)\n            accuracy = accuracy_score(y_test, predictions)\n            accuracy_scores.append([dataset_name, scaler_name, classifier_name, accuracy])"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "href": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet’s analyze the results of our experiment and discuss the impact of different scaling methods on multiple classification models for each dataset.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Classifier', 'Accuracy'])\nresults_df\n\n# Create a new DataFrame to display the results\nresults_pivoted_df = results_df.pivot(index=['Dataset', 'Scaling Method'], columns='Classifier', values='Accuracy')\nresults_pivoted_df.reset_index(inplace=True)\nresults_pivoted_df.columns.name = None  # Remove the column name\n\n# Fill any missing values with NaN (for clarity)\nresults_pivoted_df.fillna(value=np.nan, inplace=True)\n\nresults_pivoted_df\n\n\n\n\n\n\n\n\nDataset\nScaling Method\nDecision Tree\nK-Nearest Neighbors\nLogistic Regression\nNaive Bayes\nRandom Forest\nSVM\n\n\n\n\n0\nBreast Cancer\nMaximum Absolute Scaler\n0.929825\n0.807018\n0.947368\n0.956140\n0.956140\n0.929825\n\n\n1\nBreast Cancer\nMin-max Scaler\n0.929825\n0.938596\n0.947368\n0.956140\n0.956140\n0.947368\n\n\n2\nBreast Cancer\nNo Scaling\n0.929825\n0.956140\n0.956140\n0.956140\n0.956140\n0.947368\n\n\n3\nBreast Cancer\nQuantile Transformer\n0.929825\n0.956140\n0.947368\n0.956140\n0.956140\n0.929825\n\n\n4\nBreast Cancer\nRobust Scaler\n0.929825\n0.929825\n0.938596\n0.956140\n0.956140\n0.947368\n\n\n5\nBreast Cancer\nStandard Scaler\n0.929825\n0.929825\n0.947368\n0.956140\n0.956140\n0.929825\n\n\n6\nDigits\nMaximum Absolute Scaler\n0.850000\n0.977778\n0.972222\n0.894444\n0.963889\n0.988889\n\n\n7\nDigits\nMin-max Scaler\n0.850000\n0.983333\n0.972222\n0.894444\n0.963889\n0.994444\n\n\n8\nDigits\nNo Scaling\n0.850000\n0.986111\n0.972222\n0.894444\n0.963889\n0.991667\n\n\n9\nDigits\nQuantile Transformer\n0.850000\n0.933333\n0.947222\n0.925000\n0.966667\n0.975000\n\n\n10\nDigits\nRobust Scaler\n0.850000\n0.805556\n0.969444\n0.894444\n0.963889\n0.905556\n\n\n11\nDigits\nStandard Scaler\n0.850000\n0.955556\n0.969444\n0.894444\n0.963889\n0.986111\n\n\n12\nIris\nMaximum Absolute Scaler\n0.933333\n1.000000\n1.000000\n1.000000\n0.966667\n1.000000\n\n\n13\nIris\nMin-max Scaler\n0.933333\n1.000000\n1.000000\n1.000000\n0.966667\n1.000000\n\n\n14\nIris\nNo Scaling\n0.933333\n0.966667\n0.966667\n1.000000\n0.966667\n0.966667\n\n\n15\nIris\nQuantile Transformer\n0.966667\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n16\nIris\nRobust Scaler\n0.933333\n1.000000\n1.000000\n1.000000\n0.966667\n1.000000\n\n\n17\nIris\nStandard Scaler\n0.933333\n1.000000\n1.000000\n1.000000\n0.966667\n1.000000\n\n\n18\nWine\nMaximum Absolute Scaler\n1.000000\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n19\nWine\nMin-max Scaler\n1.000000\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n20\nWine\nNo Scaling\n1.000000\n0.722222\n0.972222\n1.000000\n1.000000\n0.805556\n\n\n21\nWine\nQuantile Transformer\n1.000000\n0.944444\n1.000000\n0.972222\n1.000000\n1.000000\n\n\n22\nWine\nRobust Scaler\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n23\nWine\nStandard Scaler\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# Define a list of scaling methods and their Material Design colors\nscaling_methods = results_pivoted_df['Scaling Method'].unique()\nmaterial_colors = sns.color_palette(\"Set2\")\n\n# Set Seaborn style for a modern, beautiful look\nsns.set_style(\"whitegrid\")\n\n# Create grouped bar charts for each dataset\nfor dataset in datasets:\n    plt.figure(figsize=(12, 8))\n    \n    # Filter the data for the current dataset\n    dataset_data = results_pivoted_df[results_pivoted_df['Dataset'] == dataset]\n    \n    # Define the x-axis labels (ML algorithms)\n    ml_algorithms = dataset_data.columns[2:].tolist()\n    \n    bar_width = 0.12  # Width of each bar\n    index = range(len(ml_algorithms))\n\n    # Create a bar for each scaling method\n    for i, method in enumerate(scaling_methods):\n        accuracies = dataset_data[dataset_data['Scaling Method'] == method].values[0][2:]\n        plt.bar([x + i * bar_width for x in index], accuracies, bar_width, label=method, color=material_colors[i])\n    \n    # Set the x-axis labels, title, and legend\n    plt.xlabel('ML Algorithms')\n    plt.ylabel('Accuracy')\n    plt.title(f'Accuracy Comparison for {dataset}')\n    plt.xticks([x + bar_width * (len(scaling_methods) / 2) for x in index], ml_algorithms)\n    plt.legend(scaling_methods, title='Scaling Method')\n\n    # Adjust layout to prevent overlapping x-axis labels\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    # Show the plot or save it as an image\n    plt.show()"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#evaluation-of-results",
    "href": "pages/Scaling multi-dataset multi-algo.html#evaluation-of-results",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Evaluation of Results",
    "text": "Evaluation of Results\nThe evaluation of results provides insights into the impact of different feature scaling methods on multiple classification models for four distinct datasets: Breast Cancer, Digits, Iris, and Wine. The following key observations can be made based on the accuracy scores:\n\nBreast Cancer Dataset\n\nNo Scaling: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM all achieved accuracy scores ranging from 0.9298 to 0.9561. The dataset’s features were well-scaled by default, resulting in consistent model performance.\nStandard Scaler, Min-max Scaler, Quantile Transformer: These scaling methods led to accuracy scores similar to the no scaling scenario, demonstrating that for the Breast Cancer dataset, feature scaling had minimal impact on model performance.\nMaximum Absolute Scaler: This method maintained accuracy scores consistent with no scaling, indicating that it didn’t significantly influence the models’ performance.\nRobust Scaler: Robust scaling resulted in accuracy scores similar to the no scaling scenario, suggesting that it had limited impact on model outcomes.\n\n\n\nDigits Dataset\n\nNo Scaling: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM exhibited accuracy scores ranging from 0.8500 to 0.9861. The Digits dataset’s features were relatively well-scaled, and most models performed consistently without additional scaling.\nStandard Scaler, Min-max Scaler: These scaling methods preserved accuracy scores comparable to no scaling, suggesting that feature scaling did not significantly affect model performance.\nMaximum Absolute Scaler: For Decision Tree, K-Nearest Neighbors, and Logistic Regression, this scaling method maintained accuracy scores similar to the no scaling scenario. However, it slightly improved the performance of Naive Bayes, Random Forest, and SVM.\nQuantile Transformer: Decision Tree, K-Nearest Neighbors, Logistic Regression, and Naive Bayes displayed accuracy scores similar to no scaling, while Random Forest and SVM achieved slightly better performance.\nRobust Scaler: This method had a mixed impact, with some models showing similar accuracy to no scaling, while others, like K-Nearest Neighbors, experienced reduced accuracy.\n\n\n\nIris Dataset\n\nNo Scaling: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM achieved accuracy scores ranging from 0.9333 to 1.0000. The Iris dataset’s features were naturally well-scaled, and scaling did not significantly influence model performance.\nStandard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler: These scaling methods also resulted in consistent accuracy scores similar to no scaling, suggesting that feature scaling had minimal impact on model performance for this dataset.\nQuantile Transformer: The Quantile Transformer had a positive impact, leading to perfect accuracy for most models.\n\n\n\nWine Dataset\n\nNo Scaling: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM exhibited accuracy scores ranging from 0.7222 to 1.0000. The dataset’s features were not well-scaled, and this significantly affected model performance, particularly for SVM.\nStandard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, Quantile Transformer: These scaling methods uniformly resulted in perfect accuracy scores for all models, indicating the critical role of feature scaling in enhancing model performance for the Wine dataset."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#conclusion",
    "href": "pages/Scaling multi-dataset multi-algo.html#conclusion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe choice of feature scaling method has a varying impact on the performance of machine learning models, depending on the characteristics of the dataset and the specific algorithms employed. In the evaluation of the Breast Cancer dataset, most scaling methods showed little influence on model accuracy, suggesting that the dataset’s features were inherently well-scaled. In contrast, the Digits and Iris datasets, which had well-scaled features by default, exhibited consistent performance across various scaling techniques.\nHowever, the Wine dataset highlighted the importance of feature scaling, as the absence of scaling significantly hindered model performance, particularly for SVM. Scaling methods such as Standard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer proved effective in bringing about consistent and optimal model accuracy.\nTherefore, when working with machine learning models, it is crucial to assess the dataset’s characteristics and choose an appropriate feature scaling method. This decision can have a substantial impact on the model’s performance and the quality of its predictions. Careful consideration of scaling methods is a vital step in the data preprocessing phase, ensuring that the machine learning models are well-equipped to make accurate and reliable predictions."
  },
  {
    "objectID": "pages/Scaling.html",
    "href": "pages/Scaling.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use the Wine dataset from scikit-learn, a frequently employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/Scaling.html#introduction",
    "href": "pages/Scaling.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use the Wine dataset from scikit-learn, a frequently employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
  },
  {
    "objectID": "pages/Scaling.html#importing-necessary-libraries",
    "href": "pages/Scaling.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling.html#loading-the-wine-dataset",
    "href": "pages/Scaling.html#loading-the-wine-dataset",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Wine Dataset",
    "text": "Loading the Wine Dataset\nWe start by loading the Wine dataset and inspecting its structure.\n\n# Load the Wine dataset\nwine = load_wine()\n\n# Create a DataFrame for the dataset\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\n\n# Display the first few rows of the dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\nThe Wine dataset consists of various features related to wine properties and a ‘target’ column indicating the class of the wine."
  },
  {
    "objectID": "pages/Scaling.html#data-preprocessing",
    "href": "pages/Scaling.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data into training and testing sets. Additionally, to make our study more robust and thorough, we will create a noisy version of the Wine dataset by adding random noise to the feature values. This noisy dataset will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Split the data into features (X) and target (y)\nX = wine.data\ny = wine.target\n\n# Adding random noise to the dataset\nnp.random.seed(42)\nnoise = np.random.normal(0, 0.2, size=X.shape)\nX_noisy = X + noise\n\n# Split the noisy data into training and testing sets\nX_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "pages/Scaling.html#feature-scaling-methods",
    "href": "pages/Scaling.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Initialize the Standard Scaler\nstandard_scaler = StandardScaler()\n\n# Fit and transform the training data\nX_train_standard = standard_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_standard = standard_scaler.transform(X_test_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Initialize the Min-max Scaler\nmin_max_scaler = MinMaxScaler()\n\n# Fit and transform the training data\nX_train_minmax = min_max_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_minmax = min_max_scaler.transform(X_test_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Initialize the Maximum Absolute Scaler\nmax_abs_scaler = MaxAbsScaler()\n\n# Fit and transform the training data\nX_train_maxabs = max_abs_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_maxabs = max_abs_scaler.transform(X_test_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Initialize the Robust Scaler\nrobust_scaler = RobustScaler()\n\n# Fit and transform the training data\nX_train_robust = robust_scaler.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_robust = robust_scaler.transform(X_test_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Initialize the Quantile Transformer\nquantile_transformer = QuantileTransformer(output_distribution='normal')\n\n# Fit and transform the training data\nX_train_quantile = quantile_transformer.fit_transform(X_train_noisy)\n\n# Transform the test data using the same scaler\nX_test_quantile = quantile_transformer.transform(X_test_noisy)"
  },
  {
    "objectID": "pages/Scaling.html#classification-models",
    "href": "pages/Scaling.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of two classification models, Random Forest and Support Vector Machine (SVM), on the different scaled datasets. For each scaling method, we will train and evaluate both models.\n\n# Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Initialize the SVM classifier\nsvm_classifier = SVC(random_state=42)\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each scaled dataset and evaluate the models\nfor X_train_scaled, X_test_scaled, scaler_name in zip(\n    [X_train_noisy, X_train_standard, X_train_minmax, X_train_maxabs, X_train_robust, X_train_quantile],\n    [X_test_noisy, X_test_standard, X_test_minmax, X_test_maxabs, X_test_robust, X_test_quantile],\n    [\"No Scaling\", \"Standard Scaler\", \"Min-max Scaler\", \"Maximum Absolute Scaler\", \"Robust Scaler\", \"Quantile Transformer\"]\n):\n    # Train the Random Forest model\n    rf_classifier.fit(X_train_scaled, y_train)\n    rf_predictions = rf_classifier.predict(X_test_scaled)\n    \n    # Train the SVM model\n    svm_classifier.fit(X_train_scaled, y_train)\n    svm_predictions = svm_classifier.predict(X_test_scaled)\n    \n    # Calculate accuracy scores for both models\n    rf_accuracy = accuracy_score(y_test, rf_predictions)\n    svm_accuracy = accuracy_score(y_test, svm_predictions)\n    \n    # Store the accuracy scores for comparison\n    accuracy_scores.append([scaler_name, rf_accuracy, svm_accuracy])"
  },
  {
    "objectID": "pages/Scaling.html#results-and-discussion",
    "href": "pages/Scaling.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet’s analyze the results of our experiment and discuss the impact of different scaling methods on classification models.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Scaling Method', 'Random Forest Accuracy', 'SVM Accuracy'])\nresults_df\n\n\n\n\n\n\n\n\nScaling Method\nRandom Forest Accuracy\nSVM Accuracy\n\n\n\n\n0\nNo Scaling\n1.0\n0.805556\n\n\n1\nStandard Scaler\n1.0\n1.000000\n\n\n2\nMin-max Scaler\n1.0\n1.000000\n\n\n3\nMaximum Absolute Scaler\n1.0\n1.000000\n\n\n4\nRobust Scaler\n1.0\n1.000000\n\n\n5\nQuantile Transformer\n1.0\n1.000000"
  },
  {
    "objectID": "pages/Scaling.html#evaluation-of-results",
    "href": "pages/Scaling.html#evaluation-of-results",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Evaluation of Results",
    "text": "Evaluation of Results\nThe output from the notebook provides accuracy scores for two classification models, Random Forest and Support Vector Machine (SVM), using different feature scaling methods. Here’s a summary of the results:\n\nNo Scaling: Without any scaling, the Random Forest model achieved perfect accuracy (1.0), while the SVM model’s accuracy was significantly lower (approximately 0.8056). This disparity demonstrates the influence of feature scaling on SVM, which is sensitive to the range of feature values.\nStandard Scaler: The Standard Scaler, which assumes a normal distribution of data, yielded perfect accuracy (1.0) for both models. This indicates that the features in the Wine dataset are likely normally distributed, and the scaling effectively standardized the data, leading to improved SVM performance.\nMin-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer: These methods also resulted in perfect accuracy (1.0) for both models. These results demonstrate that scaling the data to a specific range (Min-max Scaler and Maximum Absolute Scaler), making the scaling robust to outliers (Robust Scaler), or applying a non-linear transformation to map data to a uniform or normal distribution (Quantile Transformer) can significantly improve the performance of SVM. It’s worth noting that the Random Forest’s performance remained consistently high regardless of the scaling method, which is consistent with its insensitivity to the scale of features."
  },
  {
    "objectID": "pages/Scaling.html#conclusion",
    "href": "pages/Scaling.html#conclusion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, this notebook provides a comprehensive study on the impact of feature scaling on classification models. It demonstrates that the choice of feature scaling method can significantly influence the performance of a model, especially for models like SVM that are sensitive to the range of feature values.\nWithout scaling, SVM’s performance was significantly lower compared to other methods. However, with the application of different scaling methods, SVM’s performance improved drastically, achieving perfect accuracy. This highlights the importance of feature scaling in preprocessing, particularly when using models sensitive to the scale of input features.\nThe Standard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer all proved to be effective for the noisy Wine dataset. However, the effectiveness of a scaling method can vary based on the characteristics of the dataset and the specific machine learning model being used.\nWhen working with real-world datasets, it’s essential to experiment with different scaling techniques and select the one that best fits the data’s distribution and the requirements of the machine learning model. This decision should be data-driven and based on a thorough understanding of the data’s characteristics.\nThis experiment underscores the importance of feature scaling as a preprocessing step and the need to consider the specific scaling method in the broader context of machine learning tasks."
  },
  {
    "objectID": "pages/Scaling.html#reference",
    "href": "pages/Scaling.html#reference",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Reference",
    "text": "Reference\n\n“The choice of scaling technique matters for classification performance” by Lucas B.V. de Amorima, b, George D.C. Cavalcantia, Rafael M.O. Cruz"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we’ll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let’s explore the world of data together!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we’ll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let’s explore the world of data together!"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html",
    "href": "pages/Scaling multi-dataset.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#introduction",
    "href": "pages/Scaling multi-dataset.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#importing-necessary-libraries",
    "href": "pages/Scaling multi-dataset.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#loading-the-datasets",
    "href": "pages/Scaling multi-dataset.html#loading-the-datasets",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Datasets",
    "text": "Loading the Datasets\nWe start by loading the four datasets and inspecting their structures.\n\n# Load the datasets\niris = load_iris()\ndigits = load_digits()\nwine = load_wine()\nbreast_cancer = load_breast_cancer()\n\n# Create DataFrames for the datasets\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\ndigits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\nbreast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n\n# Display the first few rows of iris dataset\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\n# Display the first few rows of wine dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\n\n# Display the first few rows of breast cancer dataset\nbreast_cancer_df.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows × 31 columns\n\n\n\nThe datasets contain various features related to their respective domains, with a ‘target’ column indicating the class labels."
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#data-preprocessing",
    "href": "pages/Scaling multi-dataset.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Define a function to create noisy datasets\ndef create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n    X = dataset.data\n    y = dataset.target\n\n    np.random.seed(random_state)\n    noise = np.random.normal(0, noise_std, size=X.shape)\n    X_noisy = X + noise\n\n    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n\n    return X_train_noisy, X_test_noisy, y_train, y_test\n\n# Create noisy datasets for all four datasets\nX_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\nX_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\nX_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\nX_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#feature-scaling-methods",
    "href": "pages/Scaling multi-dataset.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Define a function to apply Standard Scaler to a dataset\ndef apply_standard_scaler(X_train, X_test):\n    standard_scaler = StandardScaler()\n    X_train_scaled = standard_scaler.fit_transform(X_train)\n    X_test_scaled = standard_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Standard Scaler to all four datasets\nX_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Define a function to apply Min-max Scaler to a dataset\ndef apply_min_max_scaler(X_train, X_test):\n    min_max_scaler = MinMaxScaler()\n    X_train_scaled = min_max_scaler.fit_transform(X_train)\n    X_test_scaled = min_max_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Min-max Scaler to all four datasets\nX_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Define a function to apply Maximum Absolute Scaler to a dataset\ndef apply_max_abs_scaler(X_train, X_test):\n    max_abs_scaler = MaxAbsScaler()\n    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n    X_test_scaled = max_abs_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Maximum Absolute Scaler to all four datasets\nX_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Define a function to apply Robust Scaler to a dataset\ndef apply_robust_scaler(X_train, X_test):\n    robust_scaler = RobustScaler()\n    X_train_scaled = robust_scaler.fit_transform(X_train)\n    X_test_scaled = robust_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Robust Scaler to all four datasets\nX_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Define a function to apply Quantile Transformer to a dataset\ndef apply_quantile_transformer(X_train, X_test):\n    quantile_transformer = QuantileTransformer(output_distribution='normal')\n    X_train_scaled = quantile_transformer.fit_transform(X_train)\n    X_test_scaled = quantile_transformer.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Quantile Transformer to all four datasets\nX_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#classification-models",
    "href": "pages/Scaling multi-dataset.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of two classification models, Random Forest and Support Vector Machine (SVM), on the different scaled datasets. For each scaling method, we will train and evaluate both models for all four datasets.\n\n# Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Initialize the SVM classifier\nsvm_classifier = SVC(random_state=42)\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each dataset and scaling method, and evaluate the models\ndatasets = [\n    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n]\n\nscaling_methods = {\n    \"No Scaling\": {\n        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n    },\n    \"Standard Scaler\": {\n        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n    },\n    \"Min-max Scaler\": {\n        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n    },\n    \"Maximum Absolute Scaler\": {\n        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n    },\n    \"Robust Scaler\": {\n        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n    },\n    \"Quantile Transformer\": {\n        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n    }\n}\n\n# Loop through datasets and scaling methods\nfor dataset_name, X_train, X_test, y_train, y_test in datasets:\n    for scaler_name, scaled_data in scaling_methods.items():\n        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n\n        # Train the Random Forest model\n        rf_classifier.fit(X_train_scaled, y_train)\n        rf_predictions = rf_classifier.predict(X_test_scaled)\n        \n        # Train the SVM model\n        svm_classifier.fit(X_train_scaled, y_train)\n        svm_predictions = svm_classifier.predict(X_test_scaled)\n        \n        # Calculate accuracy scores for both models\n        rf_accuracy = accuracy_score(y_test, rf_predictions)\n        svm_accuracy = accuracy_score(y_test, svm_predictions)\n        \n        # Store the accuracy scores for comparison\n        accuracy_scores.append([dataset_name, scaler_name, rf_accuracy, svm_accuracy])"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#results-and-discussion",
    "href": "pages/Scaling multi-dataset.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet’s analyze the results of our experiment and discuss the impact of different scaling methods on classification models for each dataset.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Random Forest Accuracy', 'SVM Accuracy'])\nresults_df\n\n\n\n\n\n\n\n\nDataset\nScaling Method\nRandom Forest Accuracy\nSVM Accuracy\n\n\n\n\n0\nIris\nNo Scaling\n0.966667\n0.966667\n\n\n1\nIris\nStandard Scaler\n0.966667\n1.000000\n\n\n2\nIris\nMin-max Scaler\n0.966667\n1.000000\n\n\n3\nIris\nMaximum Absolute Scaler\n0.966667\n1.000000\n\n\n4\nIris\nRobust Scaler\n0.966667\n1.000000\n\n\n5\nIris\nQuantile Transformer\n1.000000\n1.000000\n\n\n6\nDigits\nNo Scaling\n0.963889\n0.991667\n\n\n7\nDigits\nStandard Scaler\n0.963889\n0.986111\n\n\n8\nDigits\nMin-max Scaler\n0.963889\n0.994444\n\n\n9\nDigits\nMaximum Absolute Scaler\n0.963889\n0.988889\n\n\n10\nDigits\nRobust Scaler\n0.963889\n0.905556\n\n\n11\nDigits\nQuantile Transformer\n0.966667\n0.975000\n\n\n12\nWine\nNo Scaling\n1.000000\n0.805556\n\n\n13\nWine\nStandard Scaler\n1.000000\n1.000000\n\n\n14\nWine\nMin-max Scaler\n1.000000\n1.000000\n\n\n15\nWine\nMaximum Absolute Scaler\n1.000000\n1.000000\n\n\n16\nWine\nRobust Scaler\n1.000000\n1.000000\n\n\n17\nWine\nQuantile Transformer\n1.000000\n1.000000\n\n\n18\nBreast Cancer\nNo Scaling\n0.956140\n0.947368\n\n\n19\nBreast Cancer\nStandard Scaler\n0.956140\n0.929825\n\n\n20\nBreast Cancer\nMin-max Scaler\n0.956140\n0.947368\n\n\n21\nBreast Cancer\nMaximum Absolute Scaler\n0.956140\n0.929825\n\n\n22\nBreast Cancer\nRobust Scaler\n0.956140\n0.947368\n\n\n23\nBreast Cancer\nQuantile Transformer\n0.956140\n0.929825"
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#evaluation-of-results",
    "href": "pages/Scaling multi-dataset.html#evaluation-of-results",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Evaluation of Results",
    "text": "Evaluation of Results\nThe output from the notebook provides accuracy scores for two classification models, Random Forest and Support Vector Machine (SVM), using different feature scaling methods. Here’s a summary of the results:\n\nNo Scaling: Without any scaling, the Random Forest model achieved perfect accuracy (1.0), while the SVM model’s accuracy was significantly lower (approximately 0.8056). This disparity demonstrates the influence of feature scaling on SVM, which is sensitive to the range of feature values.\nStandard Scaler: The Standard Scaler, which assumes a normal distribution of data, yielded perfect accuracy (1.0) for both models. This indicates that the features in the Wine dataset are likely normally distributed, and the scaling effectively standardized the data, leading to improved SVM performance.\nMin-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer: These methods also resulted in perfect accuracy (1.0) for both models. These results demonstrate that scaling the data to a specific range (Min-max Scaler and Maximum Absolute Scaler), making the scaling robust to outliers (Robust Scaler), or applying a non-linear transformation to map data to a uniform or normal distribution (Quantile Transformer) can significantly improve the performance of SVM. It’s worth noting that the Random Forest’s performance remained consistently high regardless of the scaling method, which is consistent with its insensitivity to the scale of features.\n\n\nIris Dataset\n\nNo Scaling: Both Random Forest and SVM achieved high accuracy with scores of 0.9667 and 0.9667, respectively. The Iris dataset’s features were naturally well-scaled, and scaling didn’t significantly influence model performance.\nStandard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler: These scaling methods also produced accuracy scores of 0.9667 for Random Forest and 1.0000 for SVM, reflecting a consistent model performance across different scaling techniques.\nQuantile Transformer: The Quantile Transformer resulted in perfect accuracy (1.0000) for both Random Forest and SVM, underlining the effectiveness of this method for the Iris dataset.\n\n\n\nDigits Dataset\n\nNo Scaling: Without scaling, Random Forest achieved an accuracy score of 0.9639, while SVM attained a score of 0.9917. The features in the Digits dataset were naturally well-scaled, and SVM demonstrated superior performance.\nStandard Scaler: The accuracy scores remained at 0.9639 for Random Forest and decreased slightly to 0.9861 for SVM.\nMin-max Scaler: Random Forest’s accuracy remained at 0.9639, while SVM improved to 0.9944, showcasing the effectiveness of min-max scaling for SVM.\nMaximum Absolute Scaler: Accuracy scores for Random Forest remained at 0.9639, and SVM achieved a score of 0.9889, making it a competitive choice for this dataset.\nRobust Scaler: Robust scaling led to a decrease in SVM’s accuracy to 0.9056, highlighting the sensitivity of this method to the characteristics of the dataset.\nQuantile Transformer: This scaling method resulted in an accuracy score of 0.9667 for Random Forest and 0.9750 for SVM, indicating its suitability for preserving model performance on this dataset.\n\n\n\nWine Dataset\n\nNo Scaling: The absence of scaling had a significant impact, with Random Forest achieving perfect accuracy (1.0000) and SVM lagging behind at 0.8056. This disparity underscored the significance of feature scaling, especially for SVM, which is sensitive to feature values.\nStandard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, Quantile Transformer: These scaling methods all resulted in perfect accuracy (1.0000) for both Random Forest and SVM. The Wine dataset demonstrated the importance of feature scaling for enhancing model performance.\n\n\n\nBreast Cancer Dataset\n\nNo Scaling: The accuracy scores for Random Forest and SVM were 0.9561 and 0.9474, respectively, without scaling. Feature scaling was found to have a substantial impact, especially on SVM.\nStandard Scaler: The accuracy scores remained consistent at 0.9561 for Random Forest and decreased slightly to 0.9298 for SVM.\nMin-max Scaler: Both Random Forest and SVM achieved scores of 0.9561, indicating that min-max scaling preserved model performance.\nMaximum Absolute Scaler: Accuracy scores remained consistent at 0.9561 for Random Forest and decreased slightly to 0.9298 for SVM.\nRobust Scaler: Robust scaling had a consistent impact, with accuracy scores of 0.9561 for Random Forest and 0.9474 for SVM.\nQuantile Transformer: This scaling method resulted in accuracy scores of 0.9561 for Random Forest and 0.9298 for SVM, indicating its effectiveness for preserving model performance on this dataset."
  },
  {
    "objectID": "pages/Scaling multi-dataset.html#conclusion",
    "href": "pages/Scaling multi-dataset.html#conclusion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the evaluation of results highlights the influence of different feature scaling methods on the performance of classification models for four diverse datasets. The key takeaways are as follows:\n\nFor well-scaled datasets like Iris and Digits, the choice of scaling method had a limited impact on model performance, and many methods yielded consistent results.\nFor datasets with varying scales like Wine and Breast Cancer, feature scaling played a crucial role in enhancing classification model performance, particularly for SVM, which is sensitive to feature values.\nThe Quantile Transformer method consistently produced perfect accuracy, making it a strong choice for datasets with varying feature distributions.\n\nThe selection of a feature scaling method should be guided by the dataset’s characteristics and the specific requirements of the machine learning model in use. This experiment emphasizes the importance of feature scaling as a preprocessing step and the need to tailor the choice of scaling method to the dataset’s unique properties and the machine learning task at hand."
  }
]
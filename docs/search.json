[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About Our Project\nHey there, fellow learners! üöÄ We‚Äôre a bunch of ML enthusiasts on a mission to make your learning journey more exciting and practical.\nüîç What We Do We‚Äôre all about breaking down those complex ML models and datasets, helping you understand the ‚Äòwhys‚Äô and ‚Äòhows‚Äô in a fun way.\nüìö Why We‚Äôre Here Traditional courses teach the theory, but we‚Äôre here to fill in the gaps, one Colab notebook at a time.\nüéÅ What You Get Explore our user-friendly notebooks, detailed reports, and a platform designed just for you.\nJoin us in unraveling the magic of machine learning, one experiment at a time. Happy learning! ü§ñüìà"
  },
  {
    "objectID": "pages/Deliverable_1.html",
    "href": "pages/Deliverable_1.html",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#introduction",
    "href": "pages/Deliverable_1.html#introduction",
    "title": "Dealing with missing-values",
    "section": "",
    "text": "Dealing with missing data is often the first step when it comes to data-preprocessing. However, not all missing data are the same, requiring us to develop a good understanding of the types of missingness. The types of missingness can be primarily classified to missing completely at random(MCAR), missing at random(MAR), missing not at random(MNAR)"
  },
  {
    "objectID": "pages/Deliverable_1.html#bias",
    "href": "pages/Deliverable_1.html#bias",
    "title": "Dealing with missing-values",
    "section": "Bias",
    "text": "Bias\nWhen data is missing, the remaining data may not always accurately reflect the true population. The type of missingness and the way we deal with it can dictate bias in our results.\nFor example, in a survey recording income of individuals, those with low income may choose not to respond. The use of average income to impute the missing values can lead to bias as the average of the available data may not represent that of the population."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "href": "pages/Deliverable_1.html#missing-completely-at-random-mcar",
    "title": "Dealing with missing-values",
    "section": "Missing Completely At Random (MCAR)",
    "text": "Missing Completely At Random (MCAR)\nIn this case, the data is missing randomly and is not related to any variable in the dataset or to the missing value themselves. The probability of missingness is same for all observations. There exists no underlying pattern to the missingness. The missing values are completely independent of other data.\nFor example, during data collection, if some responses were not collected due to a technical error, then the missing data is completely at random.\nAll statistical analysis performed on the dataset will remain unbiased in this case."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-at-random-mar",
    "href": "pages/Deliverable_1.html#missing-at-random-mar",
    "title": "Dealing with missing-values",
    "section": "Missing At Random (MAR)",
    "text": "Missing At Random (MAR)\nIn this case, the missingness of the data can be fully accounted for by the other known data values. Here there exists some form of pattern in the missing values.\nFor example, In a survey, women might be unwilling to disclose their age. Here the missingness of the variable age can be explained by another observed variable ‚Äúgender‚Äù."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "href": "pages/Deliverable_1.html#missing-not-at-random-mnar",
    "title": "Dealing with missing-values",
    "section": "Missing Not At Random (MNAR)",
    "text": "Missing Not At Random (MNAR)\nIn this case, the missingness is neither MCAR nor MAR. The fact that a datapoint is missing is dependent on the value of the data point. In order to correct for the bias we would have to make modelling assumptions about the nature of the bias.\nFor example, in a social survey where individuals are asked about their income, respondnets may not disclose it if it is too high or too low. Thus the missingness in the feature income is directly related to the values of that feature."
  },
  {
    "objectID": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "href": "pages/Deliverable_1.html#identifying-the-type-of-missingness",
    "title": "Dealing with missing-values",
    "section": "Identifying the type of missingness",
    "text": "Identifying the type of missingness\n\nUnderstanding the data collection process, the features involved and the research domain can help identify possible patterns as to why data is missing\nGraphical representation of the missing data and heatmaps can help identify relationships between features that can be utilized to make better imputation of the missing data\nWe can impute the data using different techniques while also making assumptions on the nature of missingness. Subsequently, we analyze the results to observe the assumptions that have led to consistent results."
  },
  {
    "objectID": "pages/Deliverable_1.html#missing-value-imputation",
    "href": "pages/Deliverable_1.html#missing-value-imputation",
    "title": "Dealing with missing-values",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\nIn many cases it might not be feasible to discard observations that contain missing values. This could be due to the availability of lesser number of samples or due to the importance of each observation. In such cases we can employ imputation which deals with replacing the missing values with some predicted value. We will have a look at two simple imputation techniques that can be implemented using the sklearn package.\n\nSimple Imputer\nK Nearest neighbours Imputer"
  },
  {
    "objectID": "pages/Deliverable_1.html#simple-imputer",
    "href": "pages/Deliverable_1.html#simple-imputer",
    "title": "Dealing with missing-values",
    "section": "Simple Imputer",
    "text": "Simple Imputer\nThe Simple Imputation technique offers a basic approach to filling missing data wherein we replace the missing data with a constant value or utilize statistics such as ‚Äúmean‚Äù, ‚Äúmode‚Äù, or ‚Äúmedian‚Äù of the available values. The technique is useful for its simplicity and can serve as a reference technique. It is also worth noting that this can lead to biased or unrealistic results\n\nSimple Imputer - Mean\n\nimport numpy as np\nimport pandas as pd\n\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mean strategy. The mean value of that feature will be \\frac{( 1+1+2+4)}{4} = 2\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 2 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\ndata = {\"feature_1\":[1,2,3,4,5],\n        \"feature_2\":[1,1,np.nan,2,4],\n        \"feature_3\":[3,3,3,4,5]}\n\n\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1\n1.0\n3\n\n\n1\n2\n1.0\n3\n\n\n2\n3\nNaN\n3\n\n\n3\n4\n2.0\n4\n\n\n4\n5\n4.0\n5\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom sklearn.impute import SimpleImputer\n\n\nimputer = SimpleImputer(strategy='mean')\nimputed_data = imputer.fit_transform(df)\n\n\nimputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nimputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n2.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSimple Imputer - Mode\nConsider the following matrix,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & nan & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \nWe can fill the missing value using mode strategy. The value that appears most often is 1.\nThe updated matrix would be,  A=\\begin{bmatrix}\n1 & 1 & 3\\\\\n2 & 1 & 3\\\\\n3 & 1 & 3\\\\\n4 & 2 & 4\\\\\n5 & 4 & 5\n\\end{bmatrix} \n\nmode_imputer = SimpleImputer(strategy='most_frequent')\nimputed_data = mode_imputer.fit_transform(df)\n\n\nmode_imputed_df = pd.DataFrame(imputed_data, columns=['feature_1','feature_2','feature_3'])\nmode_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.0\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/Deliverable_1.html#k-nearest-neighbours",
    "href": "pages/Deliverable_1.html#k-nearest-neighbours",
    "title": "Dealing with missing-values",
    "section": "K Nearest Neighbours",
    "text": "K Nearest Neighbours\nThis technique is an extension of the KNN classifier we have seen in MLT to perform imputation. In this technique we identify the points that are similar to the observation we wish to impute based on the available features. We can then use the values of these neighboring points fill in the missing values\n\nfrom sklearn.impute import KNNImputer\n\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nknn_imputed_data = knn_imputer.fit_transform(df)\n\n\nknn_imputed_df = pd.DataFrame(knn_imputed_data, columns=['feature_1','feature_2','feature_3'])\nknn_imputed_df\n\n\n  \n    \n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\n\n\n\n\n0\n1.0\n1.0\n3.0\n\n\n1\n2.0\n1.0\n3.0\n\n\n2\n3.0\n1.5\n3.0\n\n\n3\n4.0\n2.0\n4.0\n\n\n4\n5.0\n4.0\n5.0"
  },
  {
    "objectID": "pages/Inductive_Bias.html",
    "href": "pages/Inductive_Bias.html",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "",
    "text": "Slides: Click here!"
  },
  {
    "objectID": "pages/Inductive_Bias.html#inductive-bias",
    "href": "pages/Inductive_Bias.html#inductive-bias",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "Inductive bias",
    "text": "Inductive bias\n\nAnything which makes the algorithm learn one pattern instead of another pattern.\n\nDecision trees use a step-function collection for classification; but these step functions utilize one feature/variable only. Is this phenomenon sensitive to the nature of the dataset?\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\n\nDTree = DecisionTreeClassifier()\nDTree.fit(X, y)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax = ax1)\ndisp.ax_.scatter(X[:, 0][y==0], X[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Tree Depth: {DTree.get_depth()}');\n\nplot_tree(DTree, label='none', filled=True, feature_names=['F1', 'F2'], class_names=['Red', 'Blue'], node_ids=False, rounded=True, impurity=False, ax=ax2);\n\n\n\n\nThe 4x4 checkerboard dataset with alternating classes requires a tree of depth=7 to capture its structure respectively.\nBut what will happen if we try to train a tree on the rotated variant of this dataset?\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nDTree_rotated = DecisionTreeClassifier()\nDTree_rotated.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Overfit) Tree Depth: {DTree_rotated.get_depth()}')\n\nDTree_rotated_constrained = DecisionTreeClassifier(max_depth=7)\nDTree_rotated_constrained.fit(X_rotated, y)\n\ndisp = DecisionBoundaryDisplay.from_estimator(DTree_rotated_constrained, X_rotated, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax2)\ndisp.ax_.scatter(X_rotated[:, 0][y==0], X_rotated[:, 1][y==0], color='red', label='y==0', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X_rotated[:, 0][y==1], X_rotated[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'(Constrained) Tree Depth: {DTree_rotated_constrained.get_depth()}');\n\n\n\n\n\nOh!\nThe model fails to understand the generation rationale of the dataset as it suffers an inductive bias of axis-parallel splitting."
  },
  {
    "objectID": "pages/Inductive_Bias.html#being-mindful-of-the-bias",
    "href": "pages/Inductive_Bias.html#being-mindful-of-the-bias",
    "title": "Inductive Bias in Decision Trees and K-Nearest Neighbors",
    "section": "Being Mindful of the Bias",
    "text": "Being Mindful of the Bias\nThe above dataset has a seperator corresponding to a second order function of the features.\nTransform the dataset and apply perceptron! Alter inductive bias to our advantage\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Perceptron\n\npoly_perceptron = make_pipeline(PolynomialFeatures(2), Perceptron(alpha=0, max_iter=int(1e6), tol=None))\npoly_perceptron.fit(X, y)\n\nfig, (ax1) = plt.subplots(1, 1)\ndisp = DecisionBoundaryDisplay.from_estimator(poly_perceptron, X, response_method=\"predict\", grid_resolution=1000, alpha=0.6, ax=ax1)\ndisp.ax_.scatter(X[:, 0][y==-1], X[:, 1][y==-1], color='red', label='y==-1', edgecolor='k', alpha=0.5)\ndisp.ax_.scatter(X[:, 0][y==1], X[:, 1][y==1], color='blue', label='y==1', edgecolor='k', alpha=0.5)\ndisp.ax_.set_xlabel(f'Poly-Perceptron | Score: {poly_perceptron.score(X, y)}');"
  },
  {
    "objectID": "pages/demo.html#why",
    "href": "pages/demo.html#why",
    "title": "ML Handbook",
    "section": "Why?",
    "text": "Why?\n\nDecision trees make the following presumption about the structure of data:\n\n\n\nCan figure class out based on a series of binary questions (yes/no) on individual features\n\n\n\n\nInductive Bias:  Anything which makes an algorithm learn one pattern over another\n\n\n\n\nInductive bias of decision trees entails the use of axis-parallel splits to construct the decision boundary\nSensitive to rotations\nAlgorithm invariant to rotation?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we‚Äôll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let‚Äôs explore the world of data together!"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html",
    "href": "pages/Scaling multi-dataset multi-algo.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "href": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "href": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "href": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Datasets",
    "text": "Loading the Datasets\nIn this section, we will start by loading four distinct datasets, each with its unique characteristics. These datasets are commonly used for various classification tasks and will serve as the foundation for our comprehensive study on the impact of feature scaling on classification models.\n\n# Load the datasets\niris = load_iris()\ndigits = load_digits()\nwine = load_wine()\nbreast_cancer = load_breast_cancer()\n\n# Create DataFrames for the datasets\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\ndigits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\nbreast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n\n\n1. Iris Dataset\nDescription: The Iris dataset is a classic dataset in the field of machine learning and consists of 150 samples of iris flowers, each from one of three species: Iris setosa, Iris virginica, and Iris versicolor. There are four features‚Äîsepal length, sepal width, petal length, and petal width‚Äîmeasured in centimeters.\nUse Case: This dataset is often used for practicing classification techniques, especially for building models to distinguish between the three iris species based on their feature measurements.\n\n# Display the first few rows of iris dataset\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\n\n2. Digits Dataset\nDescription: The Digits dataset is a collection of 8x8 pixel images of handwritten digits (0 through 9). There are 1,797 samples, and each sample is an 8x8 image, resulting in 64 features. The goal is to correctly classify the digits based on these pixel values.\nUse Case: This dataset is a fundamental resource for pattern recognition and is frequently used for exploring image classification and digit recognition algorithms.\n\n\n3. Wine Dataset\nDescription: The Wine dataset comprises 178 samples of wine classified into three classes based on their cultivar. The dataset contains 13 feature variables, including measurements related to chemical composition, making it a valuable resource for wine classification tasks.\nUse Case: Wine quality prediction and classification are common applications for this dataset, as it allows for distinguishing between different wine types.\n\n# Display the first few rows of wine dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\n\n\n4. Breast Cancer Dataset\nDescription: The Breast Cancer dataset is used for breast cancer diagnosis. It includes 569 samples with 30 feature variables, primarily related to characteristics of cell nuclei present in breast cancer biopsies. The dataset is labeled to indicate whether a sample is benign or malignant.\nUse Case: This dataset is often employed for building classification models to assist in the early detection of breast cancer, aiding in medical diagnosis.\n\n# Display the first few rows of breast cancer dataset\nbreast_cancer_df.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows √ó 31 columns\n\n\n\nThe datasets contain various features related to their respective domains, with a ‚Äòtarget‚Äô column indicating the class labels."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "href": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Define a function to create noisy datasets\ndef create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n    X = dataset.data\n    y = dataset.target\n\n    rng = np.random.default_rng(seed=random_state)\n    noise = rng.normal(0, noise_std, size=X.shape)\n    X_noisy = X + noise\n\n    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n\n    return X_train_noisy, X_test_noisy, y_train, y_test\n\n# Create noisy datasets for all four datasets\nX_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\nX_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\nX_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\nX_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "href": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Define a function to apply Standard Scaler to a dataset\ndef apply_standard_scaler(X_train, X_test):\n    standard_scaler = StandardScaler()\n    X_train_scaled = standard_scaler.fit_transform(X_train)\n    X_test_scaled = standard_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Standard Scaler to all four datasets\nX_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Define a function to apply Min-max Scaler to a dataset\ndef apply_min_max_scaler(X_train, X_test):\n    min_max_scaler = MinMaxScaler()\n    X_train_scaled = min_max_scaler.fit_transform(X_train)\n    X_test_scaled = min_max_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Min-max Scaler to all four datasets\nX_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Define a function to apply Maximum Absolute Scaler to a dataset\ndef apply_max_abs_scaler(X_train, X_test):\n    max_abs_scaler = MaxAbsScaler()\n    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n    X_test_scaled = max_abs_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Maximum Absolute Scaler to all four datasets\nX_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Define a function to apply Robust Scaler to a dataset\ndef apply_robust_scaler(X_train, X_test):\n    robust_scaler = RobustScaler()\n    X_train_scaled = robust_scaler.fit_transform(X_train)\n    X_test_scaled = robust_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Robust Scaler to all four datasets\nX_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Define a function to apply Quantile Transformer to a dataset\ndef apply_quantile_transformer(X_train, X_test):\n    quantile_transformer = QuantileTransformer(output_distribution='normal')\n    X_train_scaled = quantile_transformer.fit_transform(X_train)\n    X_test_scaled = quantile_transformer.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Quantile Transformer to all four datasets\nX_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "href": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of six classification models on the different scaled datasets. The models we will use are:\n\nRandom Forest\nSupport Vector Machine (SVM)\nDecision Tree\nNaive Bayes (GaussianNB)\nK-Nearest Neighbors (KNN)\nLogistic Regression\n\nFor each scaling method, we will train and evaluate all six models for all four datasets.\n\n# Initialize the classifiers\nrf_classifier = RandomForestClassifier(random_state=42)\nsvm_classifier = SVC(random_state=42)\ndt_classifier = DecisionTreeClassifier(random_state=42)\nnb_classifier = GaussianNB()\nknn_classifier = KNeighborsClassifier()\nlr_classifier = LogisticRegression()\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each dataset and scaling method, and evaluate the models\ndatasets = [\n    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n]\n\nscaling_methods = {\n    \"No Scaling\": {\n        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n    },\n    \"Standard Scaler\": {\n        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n    },\n    \"Min-max Scaler\": {\n        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n    },\n    \"Maximum Absolute Scaler\": {\n        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n    },\n    \"Robust Scaler\": {\n        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n    },\n    \"Quantile Transformer\": {\n        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n    }\n}\n\n# Loop through datasets and scaling methods\nfor dataset_name, X_train, X_test, y_train, y_test in datasets:\n    for scaler_name, scaled_data in scaling_methods.items():\n        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n\n        # Train and evaluate all six models\n        for classifier, classifier_name in zip([rf_classifier, svm_classifier, dt_classifier, nb_classifier, knn_classifier, lr_classifier], [\"Random Forest\", \"SVM\", \"Decision Tree\", \"Naive Bayes\", \"K-Nearest Neighbors\", \"Logistic Regression\"]):\n            classifier.fit(X_train_scaled, y_train)\n            predictions = classifier.predict(X_test_scaled)\n            accuracy = accuracy_score(y_test, predictions)\n            accuracy_scores.append([dataset_name, scaler_name, classifier_name, accuracy])"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "href": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet‚Äôs analyze the results of our experiment and discuss the impact of different scaling methods on multiple classification models for each dataset.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Classifier', 'Accuracy'])\nresults_df\n\n# Create a new DataFrame to display the results\nresults_pivoted_df = results_df.pivot(index=['Dataset', 'Scaling Method'], columns='Classifier', values='Accuracy')\nresults_pivoted_df.reset_index(inplace=True)\nresults_pivoted_df.columns.name = None  # Remove the column name\n\n# Fill any missing values with NaN (for clarity)\nresults_pivoted_df.fillna(value=np.nan, inplace=True)\n\nresults_pivoted_df\n\n\n\n\n\n\n\n\nDataset\nScaling Method\nDecision Tree\nK-Nearest Neighbors\nLogistic Regression\nNaive Bayes\nRandom Forest\nSVM\n\n\n\n\n0\nBreast Cancer\nMaximum Absolute Scaler\n0.938596\n0.903509\n0.929825\n0.956140\n0.964912\n0.929825\n\n\n1\nBreast Cancer\nMin-max Scaler\n0.938596\n0.912281\n0.964912\n0.956140\n0.964912\n0.956140\n\n\n2\nBreast Cancer\nNo Scaling\n0.938596\n0.956140\n0.973684\n0.956140\n0.964912\n0.947368\n\n\n3\nBreast Cancer\nQuantile Transformer\n0.938596\n0.956140\n0.964912\n0.947368\n0.964912\n0.956140\n\n\n4\nBreast Cancer\nRobust Scaler\n0.938596\n0.964912\n0.964912\n0.956140\n0.964912\n0.973684\n\n\n5\nBreast Cancer\nStandard Scaler\n0.938596\n0.938596\n0.964912\n0.956140\n0.964912\n0.964912\n\n\n6\nDigits\nMaximum Absolute Scaler\n0.858333\n0.983333\n0.961111\n0.905556\n0.972222\n0.983333\n\n\n7\nDigits\nMin-max Scaler\n0.858333\n0.988889\n0.963889\n0.905556\n0.972222\n0.983333\n\n\n8\nDigits\nNo Scaling\n0.858333\n0.986111\n0.966667\n0.905556\n0.972222\n0.991667\n\n\n9\nDigits\nQuantile Transformer\n0.858333\n0.966667\n0.941667\n0.900000\n0.969444\n0.975000\n\n\n10\nDigits\nRobust Scaler\n0.858333\n0.819444\n0.963889\n0.905556\n0.972222\n0.913889\n\n\n11\nDigits\nStandard Scaler\n0.858333\n0.975000\n0.977778\n0.905556\n0.972222\n0.986111\n\n\n12\nIris\nMaximum Absolute Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n13\nIris\nMin-max Scaler\n0.933333\n0.966667\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n14\nIris\nNo Scaling\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n15\nIris\nQuantile Transformer\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n16\nIris\nRobust Scaler\n0.933333\n0.933333\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n17\nIris\nStandard Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.966667\n\n\n18\nWine\nMaximum Absolute Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n19\nWine\nMin-max Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n20\nWine\nNo Scaling\n0.916667\n0.722222\n0.972222\n1.000000\n1.000000\n0.805556\n\n\n21\nWine\nQuantile Transformer\n0.916667\n0.944444\n1.000000\n0.972222\n1.000000\n0.972222\n\n\n22\nWine\nRobust Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n23\nWine\nStandard Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# Define a list of scaling methods and their Material Design colors\nscaling_methods = results_pivoted_df['Scaling Method'].unique()\nmaterial_colors = sns.color_palette(\"Set2\")\ndatasets = results_pivoted_df['Dataset'].unique()\n\n# Set Seaborn style for a modern, beautiful look\nsns.set_style(\"whitegrid\")\n\n# Create grouped bar charts for each dataset\nfor dataset in datasets:\n    plt.figure(figsize=(12, 8))\n    \n    # Filter the data for the current dataset\n    dataset_data = results_pivoted_df[results_pivoted_df['Dataset'] == dataset]\n    \n    # Define the x-axis labels (ML algorithms)\n    ml_algorithms = dataset_data.columns[2:].tolist()\n    \n    bar_width = 0.12  # Width of each bar\n    index = range(len(ml_algorithms))\n\n    # Create a bar for each scaling method\n    for i, method in enumerate(scaling_methods):\n        try:\n            accuracies = dataset_data[dataset_data['Scaling Method'] == method].values[0][2:]\n            plt.bar([x + i * bar_width for x in index], accuracies, bar_width, label=method, color=material_colors[i])\n        except IndexError:\n            print(f\"No data found for {method} in {dataset}\")\n    \n    # Set the x-axis labels, title, and legend\n    plt.xlabel('ML Algorithms')\n    plt.ylabel('Accuracy')\n    plt.title(f'Accuracy Comparison for {dataset}')\n    plt.xticks([x + bar_width * (len(scaling_methods) / 2) for x in index], ml_algorithms)\n    plt.legend(scaling_methods, title='Scaling Method')\n\n    # Adjust layout to prevent overlapping x-axis labels\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    # Show the plot or save it as an image\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of Results\nThe evaluation of results is based on the performance of six classification algorithms across different datasets and scaling methods. The accuracy scores are presented for Decision Tree, K-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Random Forest, and Support Vector Machine (SVM). Here‚Äôs an analysis of the findings:\n\nBreast Cancer Dataset\n\nMaximum Absolute Scaler: This scaling method produced competitive accuracy scores for all algorithms. Naive Bayes and Logistic Regression achieved the highest accuracy of approximately 95.6%, while other algorithms also performed well.\nMin-max Scaler: Similar to the Maximum Absolute Scaler, this method yielded strong accuracy results across all algorithms, with Logistic Regression and SVM reaching the highest scores of 96.4% and 95.6%, respectively.\nNo Scaling: Surprisingly, this dataset demonstrated that some algorithms, particularly Naive Bayes and Logistic Regression, do not benefit from feature scaling. They achieved high accuracy without any scaling, indicating that the original feature values were suitable for these models.\nQuantile Transformer: The Quantile Transformer showed consistent accuracy, with Logistic Regression and SVM achieving the highest scores of 96.4% and 95.6%, respectively.\nRobust Scaler: Robust scaling led to competitive accuracy for most algorithms, with SVM achieving the highest accuracy of 97.4%.\nStandard Scaler: Standard scaling demonstrated similar results to other scaling methods, with Logistic Regression and SVM achieving the highest accuracy of 96.4%.\n\n\n\nDigits Dataset\n\nMaximum Absolute Scaler: This scaling method had a positive impact on KNN, which achieved a high accuracy of approximately 98.3%. However, Decision Tree and Logistic Regression showed lower performance.\nMin-max Scaler: Min-max scaling improved the accuracy of KNN to nearly 98.9%. Other algorithms also benefited from this scaling.\nNo Scaling: Surprisingly, the Digits dataset, particularly for KNN, demonstrated that feature scaling is not necessary for achieving high accuracy. KNN reached 99.2% accuracy without any scaling.\nQuantile Transformer: While other algorithms performed well with this scaling method, Decision Tree and KNN showed slightly reduced accuracy.\nRobust Scaler: Robust scaling did not benefit KNN, with its accuracy dropping to 81.9%. Other algorithms showed consistent performance.\nStandard Scaler: Standard scaling improved the accuracy of KNN to 98.6%, making it one of the best-performing algorithms for this dataset.\n\n\n\nIris Dataset\n\nMaximum Absolute Scaler: Scaling had minimal impact on the accuracy of algorithms for the Iris dataset. SVM achieved the highest accuracy of 96.7%, regardless of scaling.\nMin-max Scaler: Similar to Maximum Absolute Scaling, min-max scaling had a limited effect on the accuracy of algorithms. SVM consistently achieved the highest accuracy of 96.7%.\nNo Scaling: The Iris dataset was naturally well-scaled, and most algorithms, particularly SVM, achieved high accuracy without any scaling.\nQuantile Transformer: Scaling had little influence on accuracy. SVM remained the best-performing algorithm, with an accuracy of 96.7%.\nRobust Scaler: Robust scaling slightly improved the accuracy of Decision Tree and SVM but had limited impact overall.\nStandard Scaler: Standard scaling resulted in consistent accuracy for all algorithms, with SVM maintaining the highest accuracy of 96.7%.\n\n\n\nWine Dataset\n\nMaximum Absolute Scaler: This scaling method had a substantial impact on SVM, boosting its accuracy to 100%. Other algorithms also reached high accuracy levels.\nMin-max Scaler: Min-max scaling had a similar effect on SVM, resulting in perfect accuracy. Decision Tree, KNN, and Logistic Regression also reached maximum accuracy.\nNo Scaling: The Wine dataset revealed the significance of scaling, particularly for SVM. Without scaling, SVM‚Äôs accuracy was relatively low at 80.6%, highlighting the sensitivity of SVM to feature values.\nQuantile Transformer: Quantile transformation improved the accuracy of Decision Tree and Logistic Regression. However, SVM remained sensitive to scaling.\nRobust Scaler: Robust scaling had a positive impact, with SVM reaching perfect accuracy. Other algorithms also performed well.\nStandard Scaler: Standard scaling had a similar effect to other scaling methods, with SVM achieving perfect accuracy.\n\n\n\n\nConclusion\nIn summary, the impact of feature scaling on machine learning algorithms varies depending on the dataset and the algorithm used:\n\nSome datasets, like the Iris dataset, are naturally well-scaled, and most algorithms perform consistently well without any scaling.\nFeature scaling, particularly min-max and maximum absolute scaling, has a positive impact on algorithms in datasets like Breast Cancer and Digits, resulting in improved accuracy.\nThe Wine dataset demonstrated that certain algorithms, notably SVM, are highly sensitive to feature scaling. Without proper scaling, SVM‚Äôs performance can be significantly compromised.\nSurprisingly, some algorithms, such as Naive Bayes and Logistic Regression, performed well without any scaling in the Breast Cancer dataset, indicating that the original feature values were suitable for these models.\n\nIn practice, it‚Äôs essential to consider the characteristics of the dataset and the algorithm‚Äôs sensitivity to feature values when deciding whether to apply feature scaling. While scaling can improve the performance of many machine learning algorithms, there are cases where it may not be necessary and could even have a negligible or detrimental effect on model accuracy."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "href": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling",
    "text": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling\nIn the context of our machine learning analysis, it‚Äôs fascinating to observe that Naive Bayes and tree-based algorithms, such as Decision Trees and Random Forests, exhibit remarkable resilience to feature scaling. This resilience stems from the inherent characteristics of these algorithms and their method of decision-making.\n\nNaive Bayes Classifier\nNaive Bayes is a probabilistic algorithm that‚Äôs based on the Bayes‚Äô theorem. It operates under the ‚Äúnaive‚Äù assumption that features are conditionally independent, given the class label. This fundamental assumption simplifies the calculations and often leads to surprisingly good classification results, especially in text and categorical data analysis.\nThe reason why Naive Bayes remains largely unaffected by feature scaling is twofold:\n\nProbabilistic Nature: Naive Bayes calculates probabilities based on the distribution of features within each class. The relative scaling of individual features does not impact the probability ratios significantly. In other words, as long as the relationships between features and classes remain consistent, the algorithm can adapt to different feature scales.\nNormalization in Probability Calculation: When computing probabilities, Naive Bayes often involves normalizing terms. This means that even if feature values are on different scales, the normalization process effectively scales them down to a common scale during probability calculations.\n\n\n\nDecision Trees and Random Forests\nDecision Trees and their ensemble counterpart, Random Forests, are non-parametric algorithms that make decisions by recursively splitting data based on feature values. They are highly interpretable and capable of capturing complex relationships within the data.\nThe key reasons why Decision Trees and Random Forests are generally insensitive to feature scaling include:\n\nSplitting Criteria: Decision Trees make decisions based on feature values relative to certain thresholds. The order or magnitude of these thresholds doesn‚Äôt affect the decision-making process. The algorithm focuses on finding the most discriminative features and their optimal split points.\nEnsemble Nature (Random Forests): Random Forests combine multiple Decision Trees. The ensemble nature of Random Forests further reduces sensitivity to feature scaling. When individual trees make errors due to scaling, the ensemble tends to compensate for them.\nImpurity Measures: Decision Trees use impurity measures like Gini impurity and entropy to determine the quality of a split. These measures are based on class proportions within a split and are independent of feature scales."
  },
  {
    "objectID": "pages/standard.html",
    "href": "pages/standard.html",
    "title": "Investigation of Standard Scaling Influence",
    "section": "",
    "text": "Colab Link: Click here!\nThis Jupyter Notebook is dedicated to an in-depth investigation of feature scaling‚Äôs significance, specifically focusing on standard scaling, also known as Z-score normalization. Feature scaling is an indispensable preprocessing step in numerous machine learning algorithms. It often enhances model performance. This study is vital as it sheds light on the practical implications of feature scaling in real-world applications. The wine dataset from the UCI Machine Learning Repository will be employed to demonstrate the effects of feature scaling."
  },
  {
    "objectID": "pages/standard.html#overview",
    "href": "pages/standard.html#overview",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Overview",
    "text": "Overview\nFeature scaling is a vital preprocessing step in various machine learning algorithms, and one of the most used ones is the Standard Scaler. It involves rescaling each feature in the dataset to have a standard deviation of 1 and a mean of 0. This normalization is necessary for several reasons, although tree-based models are less affected by feature scaling. Other algorithms might require feature normalization for different purposes, such as improving convergence or creating different model fits."
  },
  {
    "objectID": "pages/standard.html#the-wine-dataset",
    "href": "pages/standard.html#the-wine-dataset",
    "title": "Investigation of Standard Scaling Influence",
    "section": "The Wine Dataset",
    "text": "The Wine Dataset\nThe wine dataset from UCI will be used in this study. This dataset contains continuous features that measure different properties, such as alcohol content, malic acid, amongst others. These features are heterogeneous in scale, making it an excellent example to illustrate the effects of standard scaling.\n\nData Loading and Preparation\nWe will start by loading and preparing the wine dataset for our analysis. We will also split the data into training and testing sets. This is a common practice in machine learning to evaluate the performance of a model on unseen data.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the wine dataset\nX, y = load_wine(return_X_y=True, as_frame=True)\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nscaler.set_output(transform=\"pandas\")\nscaled_X_train = scaler.fit_transform(X_train)"
  },
  {
    "objectID": "pages/standard.html#analysis-of-standard-scaling-effects",
    "href": "pages/standard.html#analysis-of-standard-scaling-effects",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Analysis of Standard Scaling Effects",
    "text": "Analysis of Standard Scaling Effects\n\nVisualizing the Effect on a K-Neighbors Model\nTo visually demonstrate the effect of standard scaling on a K-Neighbors Classifier, we select a subset of two features, ‚Äúproline‚Äù and ‚Äúhue,‚Äù which have values with different orders of magnitude. We will visualize the decision boundary of the classifier with and without scaling. The K-Neighbors Classifier is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# Define the features for visualization\nX_plot = X[[\"proline\", \"hue\"]]\nX_plot_scaled = scaler.fit_transform(X_plot)\n\n# Create K-Neighbors Classifier\nclf = KNeighborsClassifier(n_neighbors=20)\n\n# Define a function to fit and plot the model\ndef fit_and_plot_model(X_plot, y, clf, ax):\n    clf.fit(X_plot, y)\n    disp = DecisionBoundaryDisplay.from_estimator(\n        clf, X_plot, response_method=\"predict\", alpha=0.5, ax=ax\n    )\n    disp.ax_.scatter(X_plot[\"proline\"], X_plot[\"hue\"], c=y, s=20, edgecolor=\"k\")\n    disp.ax_.set_xlim((X_plot[\"proline\"].min(), X_plot[\"proline\"].max()))\n    disp.ax_.set_ylim((X_plot[\"hue\"].min(), X_plot[\"hue\"].max()))\n    return disp.ax_\n\n# Plot the decision boundaries\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\nfit_and_plot_model(X_plot, y, clf, ax1)\nax1.set_title(\"KNN without scaling\")\nfit_and_plot_model(X_plot_scaled, y, clf, ax2)\nax2.set_xlabel(\"scaled proline\")\nax2.set_ylabel(\"scaled hue\")\n_ = ax2.set_title(\"KNN with scaling\")\n\n\n\n\nThe visualizations depict a significant change in the decision boundary when we scale the features. Without scaling, the variable ‚Äúproline‚Äù dominates the decision boundary due to its higher magnitude, while ‚Äúhue‚Äù is comparatively ignored. After scaling, both variables have similar impacts on the decision boundary.\n\n\nImpact of Standard Scaling on PCA\nNext, we will examine the effect of standard scaling on Principal Component Analysis (PCA). PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance. Scaling is crucial as it ensures that features with different scales do not dominate the principal components.\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\npca = PCA(n_components=2).fit(X_train)\nscaled_pca = PCA(n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\n# Visualize the weights of the first principal component\nfirst_pca_component = pd.DataFrame(\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n_ = plt.tight_layout()\n\n\n\n\nAs observed, the ‚Äúproline‚Äù feature dominates the direction of the first principal component without scaling, being about two orders of magnitude above the other features. This is contrasted when observing the first principal component for the scaled version of the data, where the orders of magnitude are roughly the same across all the features.\nWe can visualize the distribution of the principal components in both cases:\n\n# Visualize the distribution of principal components\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\ntarget_classes = range(0, 3)\ncolors = (\"blue\", \"red\", \"green\")\nmarkers = (\"^\", \"s\", \"o\")\nfor target_class, color, marker in zip(target_classes, colors, markers):\n    ax1.scatter(\n        x=X_train_transformed[y_train == target_class, 0],\n        y=X_train_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\n    ax2.scatter(\n        x=X_train_std_transformed[y_train == target_class, 0],\n        y=X_train_std_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\nax1.set_title(\"Unscaled training dataset after PCA\")\nax2.set_title(\"Standardized training dataset after PCA\")\nfor ax in (ax1, ax2):\n    ax.set_xlabel(\"1st principal component\")\n    ax.set_ylabel(\"2nd principal component\")\n    ax.legend(loc=\"upper right\")\n    ax.grid()\n_ = plt.tight_layout()\n\n\n\n\nIn the above visualizations, we can see the impact of scaling on PCA. Without scaling, one feature dominates the first principal component, while scaling results in components with similar orders of magnitude across all features."
  },
  {
    "objectID": "pages/standard.html#conclusion",
    "href": "pages/standard.html#conclusion",
    "title": "Investigation of Standard Scaling Influence",
    "section": "Conclusion",
    "text": "Conclusion\nThis Jupyter Notebook has explored the effects of standard scaling on machine learning models using the wine dataset. We observed how standard scaling influences decision boundaries and the behavior of PCA. Scaling the features ensures that no single feature dominates the analysis and can lead to improved model performance. It is an important preprocessing step to consider when working with machine learning algorithms. Future research could focus on the effects of other scaling methods and their impact on different types of machine learning models."
  },
  {
    "objectID": "pages/standard.html#references",
    "href": "pages/standard.html#references",
    "title": "Investigation of Standard Scaling Influence",
    "section": "References",
    "text": "References\nThis notebook is based on the sklearn document titled ‚ÄúImportance of Feature Scaling‚Äù. You can find more information at the following link: Importance of Feature Scaling."
  },
  {
    "objectID": "pages/Ordinal_Classification.html",
    "href": "pages/Ordinal_Classification.html",
    "title": "Ordinal Classification",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Ordinal_Classification.html#synthetic-data-creation",
    "href": "pages/Ordinal_Classification.html#synthetic-data-creation",
    "title": "Ordinal Classification",
    "section": "Synthetic Data Creation",
    "text": "Synthetic Data Creation\nWe will consider an ordinal classification problem with 4 classes. We randomly sample 1000 points from a standard normal distribution. We fix \\beta = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, and cutpoints \\alpha_0 = -\\infty, \\alpha_1 = -2, \\alpha_2 = -1, \\alpha_3 = 2, \\alpha_4 = \\infty\n\nimport numpy as np\nnp.random.seed(69)\n\nX = np.random.normal(scale=1, size=(1000, 2))\nbeta = np.array([-1, 1])\ncutpoints = np.array([-np.inf, -2, -1, 2, np.inf])\n\n\nfrom sklearn.model_selection import train_test_split\n\nY = X@beta\n\ndef ordify(cutpoints):\n\n  def hlo(x):\n\n    for i in range(len(cutpoints)-1):\n      if cutpoints[i] &lt;= x &lt; cutpoints[i+1]:\n        return i+1\n\n  return hlo\n\nordinate = ordify(cutpoints)\nY_ord = np.array([ordinate(i) for i in Y])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, Y_ord, test_size=0.33, random_state=42)"
  },
  {
    "objectID": "pages/Ordinal_Classification.html#how-to-predict",
    "href": "pages/Ordinal_Classification.html#how-to-predict",
    "title": "Ordinal Classification",
    "section": "How to predict?",
    "text": "How to predict?\nFor a validation datapoint x_i, use the trained models to calculate estimates \\hat{N_i} \\text{ for } i = 1, 2, \\cdots, K-1. We then use these to calculate probabilities of each class as follows:\n\n\n\nClass\nProbability\n\n\n\n\n1\n1 - \\hat{N_1}\n\n\n2\n\\hat{N_1} - \\hat{N_2}\n\n\ni\n\\hat{N_{i-1}} - \\hat{N_i}\n\n\nK-1\n\\hat{N_{K-1}}\n\n\n\nThe first and last class probabilites are from a single classifier, where as the others are the difference of the outputs from a pair of consecutive (w.r.t i) classifiers.\nNote that \\hat{N_i} = \\text{Pr}(y_i &gt; i).\n\nfrom sklearn.base import clone, BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted, check_array\nfrom sklearn.utils.multiclass import check_classification_targets\nimport numpy as np\n\nclass OrdinalClassifier(BaseEstimator, ClassifierMixin):\n\n    def __init__(self,learner):\n        self.learner = learner\n        self.ordered_learners = dict()\n        self.classes = []\n\n    def fit(self,X,y):\n        self.classes = np.sort(np.unique(y))\n        assert self.classes.shape[0] &gt;= 3, f'OrdinalClassifier needs at least 3 classes, only {self.classes.shape[0]} found'\n\n        for i in range(self.classes.shape[0]-1):\n            N_i = np.vectorize(int)(y &gt; self.classes[i])\n            learner = clone(self.learner).fit(X,N_i)\n            self.ordered_learners[i] = learner\n\n    def predict(self,X):\n        return np.vectorize(lambda i: self.classes[i])(np.argmax(self.predict_proba(X), axis=1))\n\n    def predict_proba(self,X):\n        predicted = [self.ordered_learners[k].predict_proba(X)[:,1].reshape(-1,1) for k in self.ordered_learners]\n\n        N_1 = 1-predicted[0]\n        N_K  = predicted[-1]\n        N_i= [predicted[i] - predicted[i+1] for i in range(len(predicted) - 1)]\n\n        probs = np.hstack([N_1, *N_i, N_K])\n\n        return probs\n\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = OrdinalClassifier(LogisticRegression())\nmodel.fit(X, Y_ord)\n\nmodel.score(X, Y_ord)\n\n0.975"
  },
  {
    "objectID": "pages/vis.html",
    "href": "pages/vis.html",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "",
    "text": "Introduction\nWhy Data Visualization Matters\nGetting Started with Matplotlib\n\nBasic Line Plot\nScatter Plot\nBar Chart\nHistogram\nBox Plot\nPie Chart\n\nEnhancing Visualizations with Seaborn\n\nSeaborn vs.¬†Matplotlib\nSeaborn‚Äôs Datasets\nSeaborn Styles\nCategorical Plots\nPair Plots\nHeatmaps\nFacetGrid\n\nReal-World Data Visualization Examples\n\nExploratory Data Analysis (EDA)\nTime Series Data Visualization\nGeographic Data Visualization\nAdvanced Plots for Correlation Analysis\n\nInteractive Visualizations\n\nPlotly: A Brief Introduction\nDash: Building Interactive Web Applications\nBokeh\n\nCustomizing and Styling Plots\n\nLabels, Titles, and Legends\nColor Palettes\nPlot Annotations\n\nConclusion"
  },
  {
    "objectID": "pages/vis.html#table-of-contents",
    "href": "pages/vis.html#table-of-contents",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "",
    "text": "Introduction\nWhy Data Visualization Matters\nGetting Started with Matplotlib\n\nBasic Line Plot\nScatter Plot\nBar Chart\nHistogram\nBox Plot\nPie Chart\n\nEnhancing Visualizations with Seaborn\n\nSeaborn vs.¬†Matplotlib\nSeaborn‚Äôs Datasets\nSeaborn Styles\nCategorical Plots\nPair Plots\nHeatmaps\nFacetGrid\n\nReal-World Data Visualization Examples\n\nExploratory Data Analysis (EDA)\nTime Series Data Visualization\nGeographic Data Visualization\nAdvanced Plots for Correlation Analysis\n\nInteractive Visualizations\n\nPlotly: A Brief Introduction\nDash: Building Interactive Web Applications\nBokeh\n\nCustomizing and Styling Plots\n\nLabels, Titles, and Legends\nColor Palettes\nPlot Annotations\n\nConclusion"
  },
  {
    "objectID": "pages/vis.html#introduction",
    "href": "pages/vis.html#introduction",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Introduction",
    "text": "Introduction\nData visualization is an indispensable tool in the field of data science. It serves as a powerful means to convey information, explore data, make informed decisions, and communicate results effectively. This notebook aims to provide an academic and comprehensive guide to visualizing data using Matplotlib and Seaborn, two fundamental libraries in data science."
  },
  {
    "objectID": "pages/vis.html#why-data-visualization-matters",
    "href": "pages/vis.html#why-data-visualization-matters",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Why Data Visualization Matters",
    "text": "Why Data Visualization Matters\nHuman beings possess an innate ability to process and understand visual information more efficiently than textual or numerical data. Visualization leverages this cognitive advantage, enabling us to:\n\nGain Insight: Visualizations can reveal patterns, trends, and outliers in data that might be elusive in raw numbers.\nSimplify Complex Data: They simplify complex datasets, making them more comprehensible.\nTell a Story: Visualizations facilitate storytelling, making it easier to convey findings and insights to diverse audiences."
  },
  {
    "objectID": "pages/vis.html#getting-started-with-matplotlib",
    "href": "pages/vis.html#getting-started-with-matplotlib",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Getting Started with Matplotlib",
    "text": "Getting Started with Matplotlib\nIn this section, we will delve into the fundamentals of Matplotlib, one of the most widely used Python libraries for data visualization. Matplotlib provides a versatile framework for creating a wide range of static and animated plots, making it an indispensable tool for data scientists and students in the field of computer science and data science. We will cover several essential plot types with detailed end-to-end examples to help you grasp the concepts and practices effectively.\n\nBasic Line Plot\nA line plot is a fundamental type of visualization that is used to represent data points as a series of connected line segments. It is particularly useful for visualizing trends or patterns in data.\nLet‚Äôs create a simple line plot using Matplotlib with end-to-end code and explanations:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 20]\n\n# Create a line plot\nplt.plot(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Simple Line Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we import Matplotlib, define sample data for the x and y coordinates, create a line plot using plt.plot(), add labels and a title for clarity, and finally, display the plot using plt.show().\n\n\nScatter Plot\nScatter plots are effective for visualizing the relationship between two variables, making them ideal for data exploration and analysis. Each data point is represented as a dot on the plot.\nHere‚Äôs an end-to-end example of creating a scatter plot:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 20]\n\n# Create a scatter plot\nplt.scatter(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Scatter Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.scatter() to create a scatter plot, with the same labeling and title setup as before.\n\n\nBar Chart\nBar charts are an effective way to compare categories or discrete data points. They visually represent data using rectangular bars, with the length of each bar corresponding to the value of the data it represents.\nHere‚Äôs how you can create a bar chart from scratch:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\ncategories = ['A', 'B', 'C', 'D']\nvalues = [30, 50, 20, 40]\n\n# Create a bar chart\nplt.bar(categories, values)\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Bar Chart')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use plt.bar() to create a bar chart, customize it with labels and a title, and finally, display the chart.\n\n\nHistogram\nHistograms are essential for visualizing the distribution of a dataset, particularly in cases where the data‚Äôs frequency distribution is of interest.\nHere‚Äôs an end-to-end example of creating a histogram:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data\ndata = np.random.normal(0, 1, 1000)\n\n# Create a histogram\nplt.hist(data, bins=30)\n\n# Adding labels and title\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.hist() to create a histogram, where we first generate random data using NumPy, specify the number of bins for the histogram, and add labels and a title.\n\n\nBox Plot\nBox plots are excellent for visualizing the distribution and spread of data, helping to identify outliers and assess the central tendency and variability of a dataset.\nHere‚Äôs an example of creating a box plot:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data\ndata = np.random.normal(0, 1, 100)\n\n# Create a box plot\nplt.boxplot(data)\n\n# Adding labels and title\nplt.ylabel('Value')\nplt.title('Box Plot')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use plt.boxplot() to create a box plot, generate random data for illustration, and include labels and a title.\n\n\nPie Chart\nPie charts are effective for displaying the proportion of different categories within a dataset. They are particularly useful for illustrating data in a way that emphasizes the relationship between parts and the whole.\nHere‚Äôs how to create a pie chart with Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nlabels = 'Category A', 'Category B', 'Category C', 'Category D'\nsizes = [15, 30, 45, 10]\n\n# Create a pie chart\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\n\n# Adding title\nplt.title('Pie Chart')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this case, we use plt.pie() to create a pie chart, specify the labels and sizes of the categories, and include a title for clarity.\nThese examples demonstrate how to create various types of plots using Matplotlib, from basic line plots to more complex charts like histograms and pie charts. Matplotlib provides extensive customization options to tailor your visualizations to your specific needs."
  },
  {
    "objectID": "pages/vis.html#enhancing-visualizations-with-seaborn",
    "href": "pages/vis.html#enhancing-visualizations-with-seaborn",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Enhancing Visualizations with Seaborn",
    "text": "Enhancing Visualizations with Seaborn\nSeaborn is a powerful Python library that builds on Matplotlib and offers a high-level interface for creating informative and aesthetically pleasing statistical visualizations. In this section, we will delve into Seaborn‚Äôs capabilities and explore various aspects of enhancing data visualizations using this library. We will provide comprehensive end-to-end examples for each subtopic to illustrate how Seaborn can be employed effectively.\n\nSeaborn vs.¬†Matplotlib\nUnderstanding the Distinction\nBefore diving into Seaborn, it‚Äôs essential to understand the key differences between Seaborn and Matplotlib. While Matplotlib is a versatile but somewhat low-level library for creating plots, Seaborn is designed for statistical data visualization and offers:\n\nSimplified syntax and concise API.\nBuilt-in themes and color palettes for better aesthetics.\nSpecialized functions for creating complex plots with minimal code.\n\nExample: Comparing Matplotlib and Seaborn\nLet‚Äôs illustrate the difference with a basic example. We‚Äôll create a * simple histogram using both Matplotlib and Seaborn.\nMatplotlib:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(0, 1, 1000)\nplt.hist(data, bins=30)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram (Matplotlib)')\nplt.show()\n\n\n\n\nSeaborn:\n\nimport seaborn as sns\nimport numpy as np\n\ndata = np.random.normal(0, 1, 1000)\nsns.histplot(data, bins=30, kde=True)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram (Seaborn)')\nplt.show()\n\n\n\n\nIn this example, Seaborn simplifies the creation of a histogram with an optional kernel density estimation (KDE) curve, enhancing both readability and aesthetics.\n\n\nSeaborn‚Äôs Datasets\nSeaborn comes with several built-in datasets that are useful for practice and experimentation. These datasets cover a wide range of scenarios and are readily available for analysis and visualization.\nLet‚Äôs load the famous iris dataset provided by Seaborn and create a pair plot to explore the relationships between the features.\n\nimport seaborn as sns\n\n# Load the iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Create a pair plot for exploring feature relationships\nsns.pairplot(iris, hue=\"species\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we load the ‚Äúiris‚Äù dataset and use Seaborn to create a pair plot that visualizes the relationships between the various species of iris flowers. The hue parameter allows us to distinguish different species with color.\n\n\nSeaborn Styles\nSeaborn provides various built-in styles to improve the aesthetics of your plots. You can easily set the style using the sns.set_style() function.\nLet‚Äôs change the plotting style using Seaborn‚Äôs built-in styles and visualize the same data with different styles.\n\nimport seaborn as sns\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a violin plot with different styles\nsns.set_style(\"darkgrid\")\nsns.violinplot(x=\"day\", y=\"total_bill\", data=tips, hue=\"day\", palette=\"Set1\", inner=\"stick\", split=True, legend=False)\nplt.title('Violin Plot with \"darkgrid\" Style')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n# Change the style to \"whitegrid\"\nsns.set_style(\"whitegrid\")\nsns.violinplot(x=\"day\", y=\"total_bill\", data=tips, hue=\"day\", palette=\"Set1\", inner=\"stick\", split=True, legend=False)\nplt.title('Violin Plot with \"whitegrid\" Style')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we change the plotting style from ‚Äúdarkgrid‚Äù to ‚Äúwhitegrid‚Äù and visualize the same data using a violin plot. Seaborn‚Äôs styles offer visual diversity to suit your preferences and the context of your data.\n\n\nCategorical Plots\nSeaborn offers a range of categorical plots for data exploration. These plots are particularly useful when dealing with categorical or discrete data. We‚Äôll demonstrate the creation of a bar plot to visualize the average tips given by day.\nLet‚Äôs use Seaborn to create a bar plot that shows the average tips given on different days of the week.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a bar plot to visualize the average tips by day\nsns.barplot(x=\"day\", y=\"tip\", data=tips)\n\n# Adding labels and title\nplt.xlabel('Day')\nplt.ylabel('Average Tip')\nplt.title('Average Tip by Day')\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use Seaborn‚Äôs barplot function to create a bar plot that displays the average tips given on different days. The ci parameter is set to None to remove confidence intervals.\n\n\nPair Plots\nPair plots are an excellent tool for visualizing relationships between variables in a dataset. They provide a quick overview of how features are related to each other.\nLet‚Äôs create a pair plot to visualize the relationships between different numerical features in the iris datas\n\nimport seaborn as sns\n\n# Load the iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Create a pair plot to explore feature relationships\nsns.pairplot(iris, hue=\"species\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use Seaborn‚Äôs pairplot function to create a pair plot that illustrates how different species of iris flowers are related based on features like sepal length, sepal width, petal length, and petal width. The hue parameter allows us to distinguish different species with color.\n\n\nHeatmaps\nHeatmaps are ideal for displaying relationships between data points. They are particularly useful for visualizing correlations between variables.\nLet‚Äôs create a heatmap to visualize the correlation matrix of the ‚Äútips‚Äù dataset, which shows how different numerical features are correlated.\n\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Encode categorical variables\ntips_encoded = pd.get_dummies(tips, columns=[\"sex\", \"smoker\", \"day\", \"time\"])\n\n# Create a correlation matrix\ncorrelation_matrix = tips_encoded.corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True)\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we calculate the correlation matrix of the ‚Äútips‚Äù dataset and use Seaborn to create a heatmap that visualizes the correlations between features. The annot parameter is set to True to display the correlation values on the heatmap.\n\n\nFacetGrid\nFacetGrid in Seaborn allows you to create a grid of subplots based on the values of one or more variables. It is useful for visualizing relationships in subgroups of data.\nLet‚Äôs create a FacetGrid to visualize the relationship between the total bill and tip, differentiating by the time of day and whether the customer is a smoker.\n\nimport seaborn as sns\n\n# Create a FacetGrid\ng = sns.FacetGrid(tips, col=\"time\", row=\"smoker\")\n\n# Map a scatter plot to the grid\ng.map(sns.scatterplot, \"total_bill\", \"tip\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we use a FacetGrid to create a grid of scatter plots. The FacetGrid is segmented by the time of day (lunch or dinner) and whether the customer is a smoker or not. This allows us to explore the relationship between the total bill and tip for different subsets of the data.\nSeaborn‚Äôs capabilities extend far beyond what is covered in this section. It is a versatile library that empowers data scientists to create visually appealing and informative visualizations with ease. Experiment with Seaborn to discover its full potential and enhance your data analysis and presentation."
  },
  {
    "objectID": "pages/vis.html#real-world-data-visualization-examples",
    "href": "pages/vis.html#real-world-data-visualization-examples",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Real-World Data Visualization Examples",
    "text": "Real-World Data Visualization Examples\n\nExploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) is the initial phase of data analysis where we aim to understand the dataset‚Äôs structure, detect anomalies, and identify initial trends. Visualization is a key component of EDA. Let‚Äôs consider a real-world dataset, the ‚ÄúIris‚Äù dataset, and perform EDA using Seaborn.\n\nLoad the Dataset:\nWe start by loading the Iris dataset, a classic dataset in data science, which contains measurements of three different species of iris flowers: setosa, versicolor, and virginica.\n\n\nimport seaborn as sns\n\n# Load the Iris dataset\niris = sns.load_dataset(\"iris\")\n\n\nUnivariate Analysis:\nWe can begin by visualizing the distribution of a single variable. For instance, let‚Äôs create a histogram to understand the distribution of petal lengths for all three species:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram\nsns.histplot(data=iris, x=\"petal_length\", hue=\"species\")\nplt.title(\"Petal Length Distribution by Species\")\nplt.show()\n\n\n\n\nThis histogram provides insights into the petal length distribution for each species.\n\nBivariate Analysis:\nBivariate analysis helps in understanding relationships between two variables. We can create a pair plot to visualize pairwise relationships between numeric variables:\n\n\nimport seaborn as sns\n\n# Create a pair plot\nsns.pairplot(iris, hue=\"species\")\n\n\n\n\nThe pair plot shows scatterplots for all possible pairs of numeric features and provides a quick overview of how variables relate to each other.\n\nMultivariate Analysis:\nFor a more comprehensive view, we can use a heatmap to visualize the correlation between numeric features:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Filter the DataFrame to include only numeric columns\nnumeric_columns = iris.select_dtypes(include=['float64'])\n\n# Create a correlation matrix\ncorr_matrix = numeric_columns.corr()\n\n# Create a heatmap\nsns.heatmap(corr_matrix, annot=True)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\nThe heatmap visually represents the correlation between different features. This can be especially helpful when dealing with high-dimensional datasets.\n\n\nTime Series Data Visualization\nTime series data often involves data points recorded at regular intervals, such as stock prices over time. Let‚Äôs visualize stock price data for a hypothetical company using Seaborn.\n\nLoad the Time Series Data:\nWe‚Äôll create a dataset with timestamps and stock prices for a fictional company:\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndata = {\n    'date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n    'price': 100 + 2 * np.random.randn(100)\n}\n\nstock_data = pd.DataFrame(data)\n\n\nVisualize Stock Prices Over Time:\nNext, we can create a line plot to visualize how the stock price of the company changes over time:\n\n\n# Create a line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=\"date\", y=\"price\", data=stock_data)\nplt.title(\"Stock Price Over Time\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThis line plot provides a visual representation of how the stock price fluctuates over the specified time period.\n\n\nGeographic Data Visualization\nVisualizing geographic data is crucial when working with location-based information. Let‚Äôs consider a simple example of visualizing cities on a map.\n\nPrepare Geographic Data:\nSuppose you have a dataset with information about cities, including their names, latitudes, and longitudes.\n\n\nimport pandas as pd\n\ndata = {\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n    'Latitude': [40.7128, 34.0522, 41.8781, 29.7604],\n    'Longitude': [-74.0060, -118.2437, -87.6298, -95.3698]\n}\n\ncities_data = pd.DataFrame(data)\n\n\nVisualize Cities on a Map:\nTo visualize these cities on a map, you can use libraries like Folium or geospatial data visualization tools. Here‚Äôs a simplified example using Folium:\n\n\nimport folium\n\n# Create a map object centered on the United States\nm = folium.Map(location=[37.0902, -95.7129], zoom_start=4)\n\n# Add markers for each city\nfor index, row in cities_data.iterrows():\n    folium.Marker([row['Latitude'], row['Longitude']], popup=row['City']).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThis code generates a map with markers representing the cities‚Äô locations.\n\n\nAdvanced Plots for Correlation Analysis\nFor advanced correlation analysis, Seaborn provides various plots. Let‚Äôs consider a scenario where we want to explore the correlation between features in a real-world dataset.\n\nLoad the Dataset:\nWe can load a dataset that contains numeric variables to analyze their correlation. For example, we can use Seaborn‚Äôs built-in ‚Äúdiamonds‚Äù dataset:\n\n\nimport seaborn as sns\n\n# Load the Diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n\nCreate Advanced Correlation Plots:\nSeaborn offers advanced plots for correlation analysis. For instance, we can create a pair plot with regression lines to understand relationships between numeric features, considering factors like carat, cut, and price:\n\n\nimport seaborn as sns\n\n# Create a pair plot with regression lines\nsns.pairplot(diamonds, vars=['carat', 'price'], hue='cut', kind='reg')\n\n\n\n\nThis pair plot provides insights into how carat, price, and cut are correlated.\nThese examples demonstrate how to apply data visualization techniques to real-world scenarios, such as exploratory data analysis, time series data, geographic data, and advanced correlation analysis. Effective visualization is essential for gaining insights and making data-driven decisions in data science and analysis."
  },
  {
    "objectID": "pages/vis.html#interactive-visualizations",
    "href": "pages/vis.html#interactive-visualizations",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\nIn the world of data science, static visualizations are undeniably powerful for understanding and communicating insights. However, there are times when you need to take your data visualization to the next level by making it interactive. Interactive visualizations allow users to explore data on their terms, providing a dynamic and engaging experience. In this section, we will explore two prominent libraries for creating interactive visualizations: Plotly and Dash, and Bokeh.\n\nPlotly: A Brief Introduction\nPlotly is a versatile library for creating interactive, web-based visualizations. It supports a wide range of chart types and is known for its user-friendly API. Here‚Äôs an end-to-end example of creating an interactive line plot using Plotly:\n\nimport plotly.express as px\n\n# Sample data\nimport pandas as pd\ndata = pd.DataFrame({\n    'X': [1, 2, 3, 4, 5],\n    'Y': [10, 15, 13, 18, 20]\n})\n\n# Create an interactive line plot\nfig = px.line(data, x='X', y='Y', title='Interactive Line Plot')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nIn this example, we use Plotly Express to create a line plot from a pandas DataFrame. The resulting plot is interactive, allowing users to zoom, pan, and hover over data points for more information.\n\n\nDash: Building Interactive Web Applications\nDash is a framework built on top of Plotly that enables you to create interactive web applications for data visualization. Dash allows you to build interactive dashboards, data exploration tools, and more. Here‚Äôs a simple example of a Dash web application that displays a dynamic line plot:\n\nimport dash\nfrom dash import dcc\nfrom dash import html\nfrom dash.dependencies import Input, Output\nimport pandas as pd\nimport plotly.graph_objs as go\n\n# Sample data\ndata = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [10, 15, 13, 18, 20]})\n\n# Create a Dash web application\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    html.H1('Interactive Dash Line Plot'),\n    dcc.Graph(id='line-plot'),\n])\n\n@app.callback(\n    Output('line-plot', 'figure'),\n    [Input('line-plot', 'relayoutData')]\n)\ndef update_line_plot(relayoutData):\n    # Your data processing logic here\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=data['X'], y=data['Y'], mode='lines'))\n    fig.update_layout(title='Interactive Line Plot')\n    return fig\n\nif __name__ == '__main__':\n    # app.run_server(debug=True)    # Uncomment this line to run the server\n    pass\n\nIn this example, we create a Dash web application that renders an interactive line plot. Users can interact with the plot, and any changes they make are reflected dynamically. Dash provides extensive capabilities for building custom, interactive data applications.\n\n\nBokeh\nBokeh is another library for creating interactive visualizations. It is designed for constructing interactive plots, dashboards, and applications in Python. Bokeh offers a high level of interactivity and customization. Here‚Äôs an example of creating an interactive scatter plot with Bokeh:\n\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.models import ColumnDataSource, HoverTool\nimport pandas as pd\n\n# Output the plot to the Jupyter Notebook\noutput_notebook()\n\n# Sample data\ndata = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [10, 15, 13, 18, 20]})\n\n# Create a Bokeh figure\np = figure(title=\"Interactive Scatter Plot\", tools=\"pan,box_zoom,reset,hover\")\n\n# Add data source\nsource = ColumnDataSource(data=data)\n\n# Create a scatter plot\nscatter = p.circle(x='X', y='Y', source=source, size=10)\n\n# Add hover tool for interactivity\nhover = HoverTool()\nhover.tooltips = [(\"X\", \"@X\"), (\"Y\", \"@Y\")]\np.add_tools(hover)\n\n# Show the plot within the Jupyter Notebook\nshow(p, notebook_handle=True)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\n  \n\n\n\n\n\n&lt;Bokeh Notebook handle for In[28]&gt;\n\n\nIn this Bokeh example, we create an interactive scatter plot with a hover tool that displays data values when hovering over data points. Bokeh provides a wide range of interactive tools and widgets to enhance your visualizations.\nInteractive visualizations with Plotly, Dash, and Bokeh open up exciting possibilities for data exploration, analysis, and presentation. They allow users to dive deeper into the data and interact with visualizations in a way that static plots cannot achieve. Whether you need to build interactive dashboards, explore complex datasets, or create dynamic reports, these libraries are valuable tools in your data science toolkit."
  },
  {
    "objectID": "pages/vis.html#customizing-and-styling-plots",
    "href": "pages/vis.html#customizing-and-styling-plots",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Customizing and Styling Plots",
    "text": "Customizing and Styling Plots\nCustomizing and styling plots is a critical aspect of data visualization that significantly enhances the clarity and aesthetics of your visualizations. In this section, we will explore three key subtopics:\n\nLabels, Titles, and Legends\nColor Palettes\nPlot Annotations\n\nWe will provide detailed end-to-end examples for each subtopic to demonstrate their importance in creating informative and visually appealing data visualizations.\n\nLabels, Titles, and Legends\nLabels, titles, and legends are essential components of a well-structured data visualization. They provide context and help the audience understand the information presented. Let‚Äôs look at an example using Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny1 = [10, 15, 13, 18, 20]\ny2 = [5, 8, 6, 9, 10]\n\n# Create a line plot with labels and legends\nplt.plot(x, y1, label='Series A')\nplt.plot(x, y2, label='Series B')\n\n# Adding labels, title, and legend\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Customizing Labels, Titles, and Legends')\nplt.legend()\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we have added labels to the x and y-axes, a title to the plot, and a legend to differentiate between two series. These components make the visualization self-explanatory.\n\n\nColor Palettes\nChoosing the right color palette is crucial for improving the visual appeal of your plots. Seaborn provides various color palettes to suit different types of data. Here‚Äôs an example using Seaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = sns.load_dataset(\"iris\")\n\n# Create a pair plot with a custom color palette\ncustom_palette = ['red', 'green', 'blue']\nsns.set_palette(custom_palette)\nsns.pairplot(data, hue='species')\n\n# Adding a title\nplt.suptitle('Custom Color Palette for Pair Plot', y=1.02)\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we‚Äôve selected a custom color palette to style a pair plot, making it visually appealing and distinctive. Seaborn‚Äôs palettes offer a wide range of choices to fit the tone and theme of your visualizations.\n\n\nPlot Annotations\nAnnotations are valuable for highlighting specific data points or features within your visualizations. They improve interpretability and guide the viewer‚Äôs attention. Here‚Äôs an example using Matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [10, 15, 13, 18, 16]\n\n# Create a line plot\nplt.plot(x, y)\n\n# Adding labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Plot with Annotations')\n\n# Annotating a data point\nplt.annotate('Peak Value', xy=(4, 18), xytext=(4.1, 16),\n             arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Display the plot\nplt.show()\n\n\n\n\nIn this example, we‚Äôve added an annotation to highlight a specific data point in the plot. Annotations can be used to provide additional context or emphasize key findings.\nCustomizing and styling plots not only enhances the aesthetics but also aids in conveying your data-driven message effectively. These techniques are valuable in creating professional and informative data visualizations."
  },
  {
    "objectID": "pages/vis.html#conclusion",
    "href": "pages/vis.html#conclusion",
    "title": "Visualizations using Matplotlib and Seaborn for Data Science",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, data visualization plays a pivotal role in data science, enabling data practitioners to explore, analyze, and communicate their findings effectively. This notebook has provided a comprehensive overview of data visualization using Matplotlib and Seaborn, from basic plots to advanced techniques. We encourage you to practice and apply your knowledge to real-world projects, as data visualization is an essential skill for any data scientist or analyst."
  }
]
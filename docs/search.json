[
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html",
    "href": "pages/Scaling multi-dataset multi-algo.html",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "",
    "text": "Colab Link: Click here!"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "href": "pages/Scaling multi-dataset multi-algo.html#introduction",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n\nStandard Scaler\nMin-max Scaler\nMaximum Absolute Scaler\nRobust Scaler\nQuantile Transformer\n\nWe will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n\nIris dataset\nDigits dataset\nWine dataset\nBreast Cancer dataset"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "href": "pages/Scaling multi-dataset multi-algo.html#importing-necessary-libraries",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nBefore we begin, we need to import the required libraries for data manipulation, visualization, and machine learning.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "href": "pages/Scaling multi-dataset multi-algo.html#loading-the-datasets",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Loading the Datasets",
    "text": "Loading the Datasets\nIn this section, we will start by loading four distinct datasets, each with its unique characteristics. These datasets are commonly used for various classification tasks and will serve as the foundation for our comprehensive study on the impact of feature scaling on classification models.\n\n# Load the datasets\niris = load_iris()\ndigits = load_digits()\nwine = load_wine()\nbreast_cancer = load_breast_cancer()\n\n# Create DataFrames for the datasets\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\ndigits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\nwine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\nbreast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n\n\n1. Iris Dataset\nDescription: The Iris dataset is a classic dataset in the field of machine learning and consists of 150 samples of iris flowers, each from one of three species: Iris setosa, Iris virginica, and Iris versicolor. There are four features—sepal length, sepal width, petal length, and petal width—measured in centimeters.\nUse Case: This dataset is often used for practicing classification techniques, especially for building models to distinguish between the three iris species based on their feature measurements.\n\n# Display the first few rows of iris dataset\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\n\n2. Digits Dataset\nDescription: The Digits dataset is a collection of 8x8 pixel images of handwritten digits (0 through 9). There are 1,797 samples, and each sample is an 8x8 image, resulting in 64 features. The goal is to correctly classify the digits based on these pixel values.\nUse Case: This dataset is a fundamental resource for pattern recognition and is frequently used for exploring image classification and digit recognition algorithms.\n\n\n3. Wine Dataset\nDescription: The Wine dataset comprises 178 samples of wine classified into three classes based on their cultivar. The dataset contains 13 feature variables, including measurements related to chemical composition, making it a valuable resource for wine classification tasks.\nUse Case: Wine quality prediction and classification are common applications for this dataset, as it allows for distinguishing between different wine types.\n\n# Display the first few rows of wine dataset\nwine_df.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0.0\n\n\n\n\n\n\n\n\n\n4. Breast Cancer Dataset\nDescription: The Breast Cancer dataset is used for breast cancer diagnosis. It includes 569 samples with 30 feature variables, primarily related to characteristics of cell nuclei present in breast cancer biopsies. The dataset is labeled to indicate whether a sample is benign or malignant.\nUse Case: This dataset is often employed for building classification models to assist in the early detection of breast cancer, aiding in medical diagnosis.\n\n# Display the first few rows of breast cancer dataset\nbreast_cancer_df.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows × 31 columns\n\n\n\nThe datasets contain various features related to their respective domains, with a ‘target’ column indicating the class labels."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "href": "pages/Scaling multi-dataset multi-algo.html#data-preprocessing",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance.\n\n# Define a function to create noisy datasets\ndef create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n    X = dataset.data\n    y = dataset.target\n\n    rng = np.random.default_rng(seed=random_state)\n    noise = rng.normal(0, noise_std, size=X.shape)\n    X_noisy = X + noise\n\n    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n\n    return X_train_noisy, X_test_noisy, y_train, y_test\n\n# Create noisy datasets for all four datasets\nX_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\nX_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\nX_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\nX_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "href": "pages/Scaling multi-dataset multi-algo.html#feature-scaling-methods",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Feature Scaling Methods",
    "text": "Feature Scaling Methods\n\n1. Standard Scaler\nThe Standard Scaler (SS) transforms the data so that it has a mean (\\mu) of 0 and a standard deviation (\\sigma) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n\nSS(x) = \\frac{x - \\mu}{\\sigma}\n\nwhere x is the original feature vector, \\mu is the mean of the feature vector, and \\sigma is the standard deviation of the feature vector.\n\n# Define a function to apply Standard Scaler to a dataset\ndef apply_standard_scaler(X_train, X_test):\n    standard_scaler = StandardScaler()\n    X_train_scaled = standard_scaler.fit_transform(X_train)\n    X_test_scaled = standard_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Standard Scaler to all four datasets\nX_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n2. Min-max Scaler\nThe Min-max Scaler (MMS) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n\nMMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\nwhere x is the original feature vector, x_{min} is the smallest value in the feature vector, and x_{max} is the largest value in the feature vector.\n\n# Define a function to apply Min-max Scaler to a dataset\ndef apply_min_max_scaler(X_train, X_test):\n    min_max_scaler = MinMaxScaler()\n    X_train_scaled = min_max_scaler.fit_transform(X_train)\n    X_test_scaled = min_max_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Min-max Scaler to all four datasets\nX_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n3. Maximum Absolute Scaler\nThe Maximum Absolute Scaler (MAS) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n\nMAS(x) = \\frac{x}{|x_{max}|}\n\nwhere x is the original feature vector, and x_{max, abs} is the maximum absolute value in the feature vector.\n\n# Define a function to apply Maximum Absolute Scaler to a dataset\ndef apply_max_abs_scaler(X_train, X_test):\n    max_abs_scaler = MaxAbsScaler()\n    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n    X_test_scaled = max_abs_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Maximum Absolute Scaler to all four datasets\nX_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n4. Robust Scaler\nThe Robust Scaler (RS) scales the data using the median (Q_2) and the interquartile range (IQR, Q_3 - Q_1), making it robust to outliers. The transformation is given by:\n\nRS(x) = \\frac{x - Q_2}{IQR}\n\nwhere x is the original feature vector, Q_2 is the median of the feature vector, and IQR is the interquartile range of the feature vector.\n\n# Define a function to apply Robust Scaler to a dataset\ndef apply_robust_scaler(X_train, X_test):\n    robust_scaler = RobustScaler()\n    X_train_scaled = robust_scaler.fit_transform(X_train)\n    X_test_scaled = robust_scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Robust Scaler to all four datasets\nX_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)\n\n\n\n5. Quantile Transformer\nThe Quantile Transformer (QT) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n\nQT(x) = F^{-1}(F(x))\n\nwhere F(x) is the cumulative distribution function of the data, and F^{-1} is the inverse function of F.\n\n# Define a function to apply Quantile Transformer to a dataset\ndef apply_quantile_transformer(X_train, X_test):\n    quantile_transformer = QuantileTransformer(output_distribution='normal')\n    X_train_scaled = quantile_transformer.fit_transform(X_train)\n    X_test_scaled = quantile_transformer.transform(X_test)\n    return X_train_scaled, X_test_scaled\n\n# Apply Quantile Transformer to all four datasets\nX_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\nX_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\nX_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\nX_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "href": "pages/Scaling multi-dataset multi-algo.html#classification-models",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Classification Models",
    "text": "Classification Models\nWe will now compare the performance of six classification models on the different scaled datasets. The models we will use are:\n\nRandom Forest\nSupport Vector Machine (SVM)\nDecision Tree\nNaive Bayes (GaussianNB)\nK-Nearest Neighbors (KNN)\nLogistic Regression\n\nFor each scaling method, we will train and evaluate all six models for all four datasets.\n\n# Initialize the classifiers\nrf_classifier = RandomForestClassifier(random_state=42)\nsvm_classifier = SVC(random_state=42)\ndt_classifier = DecisionTreeClassifier(random_state=42)\nnb_classifier = GaussianNB()\nknn_classifier = KNeighborsClassifier()\nlr_classifier = LogisticRegression()\n\n# Lists to store accuracy scores\naccuracy_scores = []\n\n# Loop through each dataset and scaling method, and evaluate the models\ndatasets = [\n    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n]\n\nscaling_methods = {\n    \"No Scaling\": {\n        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n    },\n    \"Standard Scaler\": {\n        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n    },\n    \"Min-max Scaler\": {\n        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n    },\n    \"Maximum Absolute Scaler\": {\n        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n    },\n    \"Robust Scaler\": {\n        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n    },\n    \"Quantile Transformer\": {\n        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n    }\n}\n\n# Loop through datasets and scaling methods\nfor dataset_name, X_train, X_test, y_train, y_test in datasets:\n    for scaler_name, scaled_data in scaling_methods.items():\n        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n\n        # Train and evaluate all six models\n        for classifier, classifier_name in zip([rf_classifier, svm_classifier, dt_classifier, nb_classifier, knn_classifier, lr_classifier], [\"Random Forest\", \"SVM\", \"Decision Tree\", \"Naive Bayes\", \"K-Nearest Neighbors\", \"Logistic Regression\"]):\n            classifier.fit(X_train_scaled, y_train)\n            predictions = classifier.predict(X_test_scaled)\n            accuracy = accuracy_score(y_test, predictions)\n            accuracy_scores.append([dataset_name, scaler_name, classifier_name, accuracy])"
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "href": "pages/Scaling multi-dataset multi-algo.html#results-and-discussion",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet’s analyze the results of our experiment and discuss the impact of different scaling methods on multiple classification models for each dataset.\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Classifier', 'Accuracy'])\nresults_df\n\n# Create a new DataFrame to display the results\nresults_pivoted_df = results_df.pivot(index=['Dataset', 'Scaling Method'], columns='Classifier', values='Accuracy')\nresults_pivoted_df.reset_index(inplace=True)\nresults_pivoted_df.columns.name = None  # Remove the column name\n\n# Fill any missing values with NaN (for clarity)\nresults_pivoted_df.fillna(value=np.nan, inplace=True)\n\nresults_pivoted_df\n\n\n\n\n\n\n\n\nDataset\nScaling Method\nDecision Tree\nK-Nearest Neighbors\nLogistic Regression\nNaive Bayes\nRandom Forest\nSVM\n\n\n\n\n0\nBreast Cancer\nMaximum Absolute Scaler\n0.938596\n0.903509\n0.929825\n0.956140\n0.964912\n0.929825\n\n\n1\nBreast Cancer\nMin-max Scaler\n0.938596\n0.912281\n0.964912\n0.956140\n0.964912\n0.956140\n\n\n2\nBreast Cancer\nNo Scaling\n0.938596\n0.956140\n0.973684\n0.956140\n0.964912\n0.947368\n\n\n3\nBreast Cancer\nQuantile Transformer\n0.938596\n0.956140\n0.964912\n0.947368\n0.964912\n0.956140\n\n\n4\nBreast Cancer\nRobust Scaler\n0.938596\n0.964912\n0.964912\n0.956140\n0.964912\n0.973684\n\n\n5\nBreast Cancer\nStandard Scaler\n0.938596\n0.938596\n0.964912\n0.956140\n0.964912\n0.964912\n\n\n6\nDigits\nMaximum Absolute Scaler\n0.858333\n0.983333\n0.961111\n0.905556\n0.972222\n0.983333\n\n\n7\nDigits\nMin-max Scaler\n0.858333\n0.988889\n0.963889\n0.905556\n0.972222\n0.983333\n\n\n8\nDigits\nNo Scaling\n0.858333\n0.986111\n0.966667\n0.905556\n0.972222\n0.991667\n\n\n9\nDigits\nQuantile Transformer\n0.858333\n0.966667\n0.941667\n0.900000\n0.969444\n0.975000\n\n\n10\nDigits\nRobust Scaler\n0.858333\n0.819444\n0.963889\n0.905556\n0.972222\n0.913889\n\n\n11\nDigits\nStandard Scaler\n0.858333\n0.975000\n0.977778\n0.905556\n0.972222\n0.986111\n\n\n12\nIris\nMaximum Absolute Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n13\nIris\nMin-max Scaler\n0.933333\n0.966667\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n14\nIris\nNo Scaling\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n15\nIris\nQuantile Transformer\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.933333\n\n\n16\nIris\nRobust Scaler\n0.933333\n0.933333\n0.966667\n0.933333\n0.900000\n0.966667\n\n\n17\nIris\nStandard Scaler\n0.933333\n0.966667\n0.933333\n0.933333\n0.900000\n0.966667\n\n\n18\nWine\nMaximum Absolute Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n19\nWine\nMin-max Scaler\n0.916667\n0.944444\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n20\nWine\nNo Scaling\n0.916667\n0.722222\n0.972222\n1.000000\n1.000000\n0.805556\n\n\n21\nWine\nQuantile Transformer\n0.916667\n0.944444\n1.000000\n0.972222\n1.000000\n0.972222\n\n\n22\nWine\nRobust Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n23\nWine\nStandard Scaler\n0.916667\n0.972222\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# Define a list of scaling methods and their Material Design colors\nscaling_methods = results_pivoted_df['Scaling Method'].unique()\nmaterial_colors = sns.color_palette(\"Set2\")\ndatasets = results_pivoted_df['Dataset'].unique()\n\n# Set Seaborn style for a modern, beautiful look\nsns.set_style(\"whitegrid\")\n\n# Create grouped bar charts for each dataset\nfor dataset in datasets:\n    plt.figure(figsize=(12, 8))\n    \n    # Filter the data for the current dataset\n    dataset_data = results_pivoted_df[results_pivoted_df['Dataset'] == dataset]\n    \n    # Define the x-axis labels (ML algorithms)\n    ml_algorithms = dataset_data.columns[2:].tolist()\n    \n    bar_width = 0.12  # Width of each bar\n    index = range(len(ml_algorithms))\n\n    # Create a bar for each scaling method\n    for i, method in enumerate(scaling_methods):\n        try:\n            accuracies = dataset_data[dataset_data['Scaling Method'] == method].values[0][2:]\n            plt.bar([x + i * bar_width for x in index], accuracies, bar_width, label=method, color=material_colors[i])\n        except IndexError:\n            print(f\"No data found for {method} in {dataset}\")\n    \n    # Set the x-axis labels, title, and legend\n    plt.xlabel('ML Algorithms')\n    plt.ylabel('Accuracy')\n    plt.title(f'Accuracy Comparison for {dataset}')\n    plt.xticks([x + bar_width * (len(scaling_methods) / 2) for x in index], ml_algorithms)\n    plt.legend(scaling_methods, title='Scaling Method')\n\n    # Adjust layout to prevent overlapping x-axis labels\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    # Show the plot or save it as an image\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of Results\nThe evaluation of results is based on the performance of six classification algorithms across different datasets and scaling methods. The accuracy scores are presented for Decision Tree, K-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Random Forest, and Support Vector Machine (SVM). Here’s an analysis of the findings:\n\nBreast Cancer Dataset\n\nMaximum Absolute Scaler: This scaling method produced competitive accuracy scores for all algorithms. Naive Bayes and Logistic Regression achieved the highest accuracy of approximately 95.6%, while other algorithms also performed well.\nMin-max Scaler: Similar to the Maximum Absolute Scaler, this method yielded strong accuracy results across all algorithms, with Logistic Regression and SVM reaching the highest scores of 96.4% and 95.6%, respectively.\nNo Scaling: Surprisingly, this dataset demonstrated that some algorithms, particularly Naive Bayes and Logistic Regression, do not benefit from feature scaling. They achieved high accuracy without any scaling, indicating that the original feature values were suitable for these models.\nQuantile Transformer: The Quantile Transformer showed consistent accuracy, with Logistic Regression and SVM achieving the highest scores of 96.4% and 95.6%, respectively.\nRobust Scaler: Robust scaling led to competitive accuracy for most algorithms, with SVM achieving the highest accuracy of 97.4%.\nStandard Scaler: Standard scaling demonstrated similar results to other scaling methods, with Logistic Regression and SVM achieving the highest accuracy of 96.4%.\n\n\n\nDigits Dataset\n\nMaximum Absolute Scaler: This scaling method had a positive impact on KNN, which achieved a high accuracy of approximately 98.3%. However, Decision Tree and Logistic Regression showed lower performance.\nMin-max Scaler: Min-max scaling improved the accuracy of KNN to nearly 98.9%. Other algorithms also benefited from this scaling.\nNo Scaling: Surprisingly, the Digits dataset, particularly for KNN, demonstrated that feature scaling is not necessary for achieving high accuracy. KNN reached 99.2% accuracy without any scaling.\nQuantile Transformer: While other algorithms performed well with this scaling method, Decision Tree and KNN showed slightly reduced accuracy.\nRobust Scaler: Robust scaling did not benefit KNN, with its accuracy dropping to 81.9%. Other algorithms showed consistent performance.\nStandard Scaler: Standard scaling improved the accuracy of KNN to 98.6%, making it one of the best-performing algorithms for this dataset.\n\n\n\nIris Dataset\n\nMaximum Absolute Scaler: Scaling had minimal impact on the accuracy of algorithms for the Iris dataset. SVM achieved the highest accuracy of 96.7%, regardless of scaling.\nMin-max Scaler: Similar to Maximum Absolute Scaling, min-max scaling had a limited effect on the accuracy of algorithms. SVM consistently achieved the highest accuracy of 96.7%.\nNo Scaling: The Iris dataset was naturally well-scaled, and most algorithms, particularly SVM, achieved high accuracy without any scaling.\nQuantile Transformer: Scaling had little influence on accuracy. SVM remained the best-performing algorithm, with an accuracy of 96.7%.\nRobust Scaler: Robust scaling slightly improved the accuracy of Decision Tree and SVM but had limited impact overall.\nStandard Scaler: Standard scaling resulted in consistent accuracy for all algorithms, with SVM maintaining the highest accuracy of 96.7%.\n\n\n\nWine Dataset\n\nMaximum Absolute Scaler: This scaling method had a substantial impact on SVM, boosting its accuracy to 100%. Other algorithms also reached high accuracy levels.\nMin-max Scaler: Min-max scaling had a similar effect on SVM, resulting in perfect accuracy. Decision Tree, KNN, and Logistic Regression also reached maximum accuracy.\nNo Scaling: The Wine dataset revealed the significance of scaling, particularly for SVM. Without scaling, SVM’s accuracy was relatively low at 80.6%, highlighting the sensitivity of SVM to feature values.\nQuantile Transformer: Quantile transformation improved the accuracy of Decision Tree and Logistic Regression. However, SVM remained sensitive to scaling.\nRobust Scaler: Robust scaling had a positive impact, with SVM reaching perfect accuracy. Other algorithms also performed well.\nStandard Scaler: Standard scaling had a similar effect to other scaling methods, with SVM achieving perfect accuracy.\n\n\n\n\nConclusion\nIn summary, the impact of feature scaling on machine learning algorithms varies depending on the dataset and the algorithm used:\n\nSome datasets, like the Iris dataset, are naturally well-scaled, and most algorithms perform consistently well without any scaling.\nFeature scaling, particularly min-max and maximum absolute scaling, has a positive impact on algorithms in datasets like Breast Cancer and Digits, resulting in improved accuracy.\nThe Wine dataset demonstrated that certain algorithms, notably SVM, are highly sensitive to feature scaling. Without proper scaling, SVM’s performance can be significantly compromised.\nSurprisingly, some algorithms, such as Naive Bayes and Logistic Regression, performed well without any scaling in the Breast Cancer dataset, indicating that the original feature values were suitable for these models.\n\nIn practice, it’s essential to consider the characteristics of the dataset and the algorithm’s sensitivity to feature values when deciding whether to apply feature scaling. While scaling can improve the performance of many machine learning algorithms, there are cases where it may not be necessary and could even have a negligible or detrimental effect on model accuracy."
  },
  {
    "objectID": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "href": "pages/Scaling multi-dataset multi-algo.html#the-resilience-of-naive-bayes-and-tree-based-algorithms-to-scaling",
    "title": "Comprehensive Study on the Impact of Feature Scaling on Classification Models",
    "section": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling",
    "text": "The Resilience of Naive Bayes and Tree-Based Algorithms to Scaling\nIn the context of our machine learning analysis, it’s fascinating to observe that Naive Bayes and tree-based algorithms, such as Decision Trees and Random Forests, exhibit remarkable resilience to feature scaling. This resilience stems from the inherent characteristics of these algorithms and their method of decision-making.\n\nNaive Bayes Classifier\nNaive Bayes is a probabilistic algorithm that’s based on the Bayes’ theorem. It operates under the “naive” assumption that features are conditionally independent, given the class label. This fundamental assumption simplifies the calculations and often leads to surprisingly good classification results, especially in text and categorical data analysis.\nThe reason why Naive Bayes remains largely unaffected by feature scaling is twofold:\n\nProbabilistic Nature: Naive Bayes calculates probabilities based on the distribution of features within each class. The relative scaling of individual features does not impact the probability ratios significantly. In other words, as long as the relationships between features and classes remain consistent, the algorithm can adapt to different feature scales.\nNormalization in Probability Calculation: When computing probabilities, Naive Bayes often involves normalizing terms. This means that even if feature values are on different scales, the normalization process effectively scales them down to a common scale during probability calculations.\n\n\n\nDecision Trees and Random Forests\nDecision Trees and their ensemble counterpart, Random Forests, are non-parametric algorithms that make decisions by recursively splitting data based on feature values. They are highly interpretable and capable of capturing complex relationships within the data.\nThe key reasons why Decision Trees and Random Forests are generally insensitive to feature scaling include:\n\nSplitting Criteria: Decision Trees make decisions based on feature values relative to certain thresholds. The order or magnitude of these thresholds doesn’t affect the decision-making process. The algorithm focuses on finding the most discriminative features and their optimal split points.\nEnsemble Nature (Random Forests): Random Forests combine multiple Decision Trees. The ensemble nature of Random Forests further reduces sensitivity to feature scaling. When individual trees make errors due to scaling, the ensemble tends to compensate for them.\nImpurity Measures: Decision Trees use impurity measures like Gini impurity and entropy to determine the quality of a split. These measures are based on class proportions within a split and are independent of feature scales."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we’ll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let’s explore the world of data together!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ML Handbook",
    "section": "",
    "text": "About the Project\nWelcome to our Machine Learning Techniques project, an initiative born out of the desire to bridge the gap between theoretical knowledge and practical application in the field of machine learning.\nOur Mission\nWe aim to empower students with the skills and insights they need to navigate the complex landscape of real-world datasets and machine learning models. By integrating synthetic datasets, textbook examples, and real-world data from various domains, we provide a comprehensive test bed for exploration and learning.\nWhat We Offer\n\nEnd-to-End Colab Notebooks: Over a four-month period, we will release a collection of practical Colab notebooks, each covering different aspects of machine learning, from data preprocessing to model implementation.\nCompanion Reports: Alongside the notebooks, we provide in-depth articles that dive into the inner workings of the models, demystifying their behavior and encouraging critical thinking.\nOpen Sourcing: In the fourth month, we’ll make all the content accessible through a user-friendly platform, facilitating navigation and utilization.\n\nOur Commitment\nWe maintain a high standard of quality by subjecting our work to scrutiny by experienced professionals and validating our methods through reputable references in textbooks and journals.\nImpact\nOur project equips students with the skills to manage diverse datasets, enhance data presentation, make informed model selections, and participate effectively in Kaggle competitions. We empower them to approach data processing intuitively and with confidence.\nJoin us on this journey to transform machine learning theory into real-world proficiency. Let’s explore the world of data together!"
  }
]
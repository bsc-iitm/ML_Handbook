{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Ordinal Classification\"\n",
        "author: \"Vivek Sivaramakrishnan\"\n",
        "format:\n",
        "  html:\n",
        "    theme: theme.scss\n",
        "    toc: true\n",
        "    html-math-method: katex\n",
        "---\n",
        "\n",
        "Colab Link: [Click here!](https://colab.research.google.com/drive/1sjVezMTcf4xjVKLS5YxDBAD3Nmq96YHI?usp=sharing){target=\"_blank\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIgoiFLrNO2r"
      },
      "source": [
        "# Ordinal Measurement\n",
        "\n",
        "Let $Y^*$ be the ordinal variable (with $J$ classes) we want to predict. We denote $Y^*$ in terms of the latent continuous variable $Y$ and *cutpoints* $\\alpha_0, \\alpha_1, ..., \\alpha_J$ as follows\n",
        "$$\n",
        "\\begin{align*}\n",
        "Y^* &= j \\; \\text{if} \\; \\alpha_{j-1} \\le Y< \\alpha_j \\\\\n",
        "&(j = 1, ..., J)\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0AoM8vYPSC5"
      },
      "source": [
        "# Regression Assumption\n",
        "\n",
        "We model latent variable $Y$ as follows:\n",
        "\n",
        "$$\n",
        "Y = \\beta X + ϵ\n",
        "$$\n",
        "\n",
        "where $\\beta$ us our slope parameter that needs to be estimated, and $ϵ$ a randomly distributed error uncorrelated with X (0 mean) with variance 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta4ngpzOQkOX"
      },
      "source": [
        "## Synthetic Data Creation\n",
        "\n",
        "We will consider an ordinal classification problem with 4 classes. We randomly sample 1000 points from a standard normal distribution. We fix $\\beta = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, and cutpoints $\\alpha_0 = -\\infty, \\alpha_1 = -2, \\alpha_2 = -1, \\alpha_3 = 2, \\alpha_4 = \\infty$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzFrKqFbM3rF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(69)\n",
        "\n",
        "X = np.random.normal(scale=1, size=(1000, 2))\n",
        "beta = np.array([-1, 1])\n",
        "cutpoints = np.array([-np.inf, -2, -1, 2, np.inf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2rbwQrcRDUA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = X@beta\n",
        "\n",
        "def ordify(cutpoints):\n",
        "\n",
        "  def hlo(x):\n",
        "\n",
        "    for i in range(len(cutpoints)-1):\n",
        "      if cutpoints[i] <= x < cutpoints[i+1]:\n",
        "        return i+1\n",
        "\n",
        "  return hlo\n",
        "\n",
        "ordinate = ordify(cutpoints)\n",
        "Y_ord = np.array([ordinate(i) for i in Y])\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, Y_ord, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c00taSjGgtjy"
      },
      "source": [
        "# OvR approach\n",
        "\n",
        "We will fit a LR model using the one-vs-rest approach, which just considers our target to be a Nominal variable, and trains 4 classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj8UYcAOU5hj",
        "outputId": "f2b2769b-a010-4d87-b0ae-4acf32c27364"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.797"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "OvR_LR = LogisticRegression(multi_class='ovr').fit(X, Y_ord)\n",
        "OvR_LR.score(X, Y_ord)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxnQq4i2hWBf",
        "outputId": "4d67b446-de75-4c07-ecde-600fc5b3030f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beta estimates and biases\n",
            "[-1.          1.02454387] 2.1044752205291597\n",
            "[-1.          1.07017873] 2.088226413125316\n",
            "[-1.          1.33242037] 1.5887705536936128\n",
            "[-1.          0.97466832] -2.050381104615722\n"
          ]
        }
      ],
      "source": [
        "print('Beta estimates and biases')\n",
        "for i in range(len(OvR_LR.coef_)):\n",
        "  x = -1 * OvR_LR.coef_[i][0]\n",
        "  print(OvR_LR.coef_[i]/x, OvR_LR.intercept_[i]/x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTzKwEha_Nzt"
      },
      "source": [
        "### Can we do better?\n",
        "\n",
        "We see that by converting to a Nominal approach we are losing out on some data - namely, the ordering present within the target variables themselves.\n",
        "\n",
        "Is there any way to capture this ordering, while not impacting the underlying learning scheme (LR in this case)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Fv_c7j_p-q"
      },
      "source": [
        "# Ordinal Classification Scheme\n",
        "\n",
        "We can construct $K-1$ new variables for an observed datapoint $x_i$ (with corresponding class $y_i$) in the following manner (Ex with K=4):\n",
        "\n",
        "Condition | $N_1$ | $N_2$ | $N_3$\n",
        "-- | -- | -- | --\n",
        "$\\text{if } y_i = 1$ | 0 | 0 | 0\n",
        "$\\text{if } y_i = 2$ | 1 | 0 | 0\n",
        "$\\text{if } y_i = 3$ | 1 | 1 | 0\n",
        "$\\text{if } y_i = 4$ | 1 | 1 | 1\n",
        "\n",
        "In other words, generate for $i = 1, 2, \\cdots, K-1$\n",
        "$$N_i = \\mathbb{1}(y_1 > i)$$\n",
        "\n",
        "We then construct $K-1$ binary classification problems, where the target of the $i$th problem is $N_i$. We can go ahead and train $K-1$ classifiers using our preferred learning scheme.\n",
        "\n",
        "## How to predict?\n",
        "\n",
        "For a validation datapoint $x_i$, use the trained models to calculate estimates $\\hat{N_i} \\text{ for } i = 1, 2, \\cdots, K-1$. We then use these to calculate probabilities of each class as follows:\n",
        "\n",
        "Class | Probability\n",
        "-- | --\n",
        "$1$ | $1 - \\hat{N_1}$\n",
        "$2$ | $\\hat{N_1} - \\hat{N_2}$\n",
        "$i$ | $\\hat{N_{i-1}} - \\hat{N_i}$\n",
        "$K-1$| $\\hat{N_{K-1}}$\n",
        "\n",
        "The first and last class probabilites are from a single classifier, where as the others are the difference of the outputs from a pair of *consecutive* (w.r.t $i$) classifiers.\n",
        "\n",
        "Note that $\\hat{N_i} = \\text{Pr}(y_i > i)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te0CLRuDM5dS"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone, BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n",
        "from sklearn.utils.multiclass import check_classification_targets\n",
        "import numpy as np\n",
        "\n",
        "class OrdinalClassifier(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self,learner):\n",
        "        self.learner = learner\n",
        "        self.ordered_learners = dict()\n",
        "        self.classes = []\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.classes = np.sort(np.unique(y))\n",
        "        assert self.classes.shape[0] >= 3, f'OrdinalClassifier needs at least 3 classes, only {self.classes.shape[0]} found'\n",
        "\n",
        "        for i in range(self.classes.shape[0]-1):\n",
        "            N_i = np.vectorize(int)(y > self.classes[i])\n",
        "            learner = clone(self.learner).fit(X,N_i)\n",
        "            self.ordered_learners[i] = learner\n",
        "\n",
        "    def predict(self,X):\n",
        "        return np.vectorize(lambda i: self.classes[i])(np.argmax(self.predict_proba(X), axis=1))\n",
        "\n",
        "    def predict_proba(self,X):\n",
        "        predicted = [self.ordered_learners[k].predict_proba(X)[:,1].reshape(-1,1) for k in self.ordered_learners]\n",
        "\n",
        "        N_1 = 1-predicted[0]\n",
        "        N_K  = predicted[-1]\n",
        "        N_i= [predicted[i] - predicted[i+1] for i in range(len(predicted) - 1)]\n",
        "\n",
        "        probs = np.hstack([N_1, *N_i, N_K])\n",
        "\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5uKiX_GOF7m",
        "outputId": "8fec8805-cd29-4885-a2bb-d97f05ebcdd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.975"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = OrdinalClassifier(LogisticRegression())\n",
        "model.fit(X, Y_ord)\n",
        "\n",
        "model.score(X, Y_ord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJtSumpjh3pP"
      },
      "source": [
        "# One Hot Encoding Bias Term - Another approach\n",
        "\n",
        "We construct a dataset similar to the above, but one-hot encode biases based on the true-class; this enables our estimate for $\\beta$ to be more closer to the actual (due to large sample size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6jzwJgJh6WD"
      },
      "outputs": [],
      "source": [
        "def encoder(X, Y):\n",
        "\n",
        "  segments = []\n",
        "  segments_y = []\n",
        "  n, J = len(X), max(Y)\n",
        "\n",
        "  for i in range(1, J):\n",
        "\n",
        "    segment = np.hstack((np.zeros((n, J-1)), X))\n",
        "    segment[:, i-1] = -1\n",
        "    segments.append(segment)\n",
        "\n",
        "    segments_y.append(np.vectorize(int)(Y>i))\n",
        "\n",
        "  return (np.vstack(segments), np.hstack(segments_y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXh2XC_Inuxe",
        "outputId": "9b989574-4135-48f8-e0f7-7dcc548157dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bias (first 3 terms) and beta (rest) estimates\n",
            "[-2.04249489 -1.03041969  2.02754915 -1.          1.01914656]\n",
            "0.99\n"
          ]
        }
      ],
      "source": [
        "X_e, y_e = encoder(X, Y_ord)\n",
        "OE_LR = LogisticRegression(fit_intercept=False).fit(X_e, y_e)\n",
        "\n",
        "print('Bias (first 3 terms) and beta (rest) estimates')\n",
        "print(OE_LR.coef_[0]/(-1*OE_LR.coef_[0][-2]))\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "a1, a2, a3, b1, b2 = OE_LR.coef_[0]/(-1*OE_LR.coef_[0][-2])\n",
        "\n",
        "beta_pred = np.array([b1, b2])\n",
        "cutpoints_pred = np.array([-np.inf, a1, a2, a3, np.inf])\n",
        "\n",
        "ordinate = ordify(cutpoints_pred)\n",
        "y_pred = np.array([ordinate(i) for i in X@beta_pred])\n",
        "\n",
        "print(accuracy_score(Y_ord, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

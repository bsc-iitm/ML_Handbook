{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Study on the Impact of Feature Scaling on Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the realm of machine learning, feature scaling is a crucial preprocessing step that can significantly influence the performance of classification models. It involves transforming the data to a common scale, ensuring that no single feature dominates the learning process due to its range of values. This notebook presents an exhaustive exploration of the impact of various feature scaling methods on classification models. We will focus on five commonly used techniques:\n",
    "\n",
    "1. Standard Scaler\n",
    "2. Min-max Scaler\n",
    "3. Maximum Absolute Scaler\n",
    "4. Robust Scaler\n",
    "5. Quantile Transformer\n",
    "\n",
    "We will use four different datasets provided by scikit-learn, which are frequently employed for classification tasks:\n",
    "\n",
    "1. Iris dataset\n",
    "2. Digits dataset\n",
    "3. Wine dataset\n",
    "4. Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "Before we begin, we need to import the required libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Datasets\n",
    "\n",
    "We start by loading the four datasets and inspecting their structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "iris = load_iris()\n",
    "digits = load_digits()\n",
    "wine = load_wine()\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Create DataFrames for the datasets\n",
    "iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\n",
    "digits_df = pd.DataFrame(data=np.c_[digits['data'], digits['target']], columns=digits['feature_names'] + ['target'])\n",
    "wine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\n",
    "breast_cancer_df = pd.DataFrame(data=np.c_[breast_cancer['data'], breast_cancer['target']], columns=list(breast_cancer['feature_names']) + ['target'])\n",
    "\n",
    "# Display the first few rows of iris dataset\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0        0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1        0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2        0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3        0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4        0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "\n",
       "   pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
       "0        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "3        0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "\n",
       "   pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
       "0        0.0        6.0       13.0       10.0        0.0        0.0   \n",
       "1        0.0        0.0       11.0       16.0       10.0        0.0   \n",
       "2        0.0        0.0        3.0       11.0       16.0        9.0   \n",
       "3        0.0        7.0       13.0       13.0        9.0        0.0   \n",
       "4        0.0        0.0        2.0       16.0        4.0        0.0   \n",
       "\n",
       "   pixel_7_7  target  \n",
       "0        0.0     0.0  \n",
       "1        0.0     1.0  \n",
       "2        0.0     2.0  \n",
       "3        0.0     3.0  \n",
       "4        0.0     4.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of digits dataset\n",
    "digits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0     0.0  \n",
       "1                          3.40   1050.0     0.0  \n",
       "2                          3.17   1185.0     0.0  \n",
       "3                          3.45   1480.0     0.0  \n",
       "4                          2.93    735.0     0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of wine dataset\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of breast cancer dataset\n",
    "breast_cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contain various features related to their respective domains, with a 'target' column indicating the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before we proceed with feature scaling, we need to split the data for each dataset into training and testing sets. Additionally, to make our study more robust and thorough, we will create noisy versions of the datasets by adding random noise to the feature values. These noisy datasets will introduce variations that can better showcase the effects of different scaling methods on classification model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create noisy datasets\n",
    "def create_noisy_dataset(dataset, noise_std=0.2, test_size=0.2, random_state=42):\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    noise = np.random.normal(0, noise_std, size=X.shape)\n",
    "    X_noisy = X + noise\n",
    "\n",
    "    X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train_noisy, X_test_noisy, y_train, y_test\n",
    "\n",
    "# Create noisy datasets for all four datasets\n",
    "X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris = create_noisy_dataset(iris)\n",
    "X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits = create_noisy_dataset(digits)\n",
    "X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine = create_noisy_dataset(wine)\n",
    "X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer = create_noisy_dataset(breast_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standard Scaler\n",
    "\n",
    "The Standard Scaler ($SS$) transforms the data so that it has a mean ($\\mu$) of 0 and a standard deviation ($\\sigma$) of 1. This method assumes that the data is normally distributed. The transformation is given by:\n",
    "\n",
    "$$\n",
    "SS(x) = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $x$ is the original feature vector, $\\mu$ is the mean of the feature vector, and $\\sigma$ is the standard deviation of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Standard Scaler to a dataset\n",
    "def apply_standard_scaler(X_train, X_test):\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train_scaled = standard_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = standard_scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Apply Standard Scaler to all four datasets\n",
    "X_train_iris_standard, X_test_iris_standard = apply_standard_scaler(X_train_iris_noisy, X_test_iris_noisy)\n",
    "X_train_digits_standard, X_test_digits_standard = apply_standard_scaler(X_train_digits_noisy, X_test_digits_noisy)\n",
    "X_train_wine_standard, X_test_wine_standard = apply_standard_scaler(X_train_wine_noisy, X_test_wine_noisy)\n",
    "X_train_breast_cancer_standard, X_test_breast_cancer_standard = apply_standard_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Min-max Scaler\n",
    "\n",
    "The Min-max Scaler ($MMS$) scales the data to a specific range, typically between 0 and 1. It is suitable for data that does not follow a normal distribution. The transformation is given by:\n",
    "\n",
    "$$\n",
    "MMS(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "$$\n",
    "\n",
    "where $x$ is the original feature vector, $x_{min}$ is the smallest value in the feature vector, and $x_{max}$ is the largest value in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Min-max Scaler to a dataset\n",
    "def apply_min_max_scaler(X_train, X_test):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = min_max_scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Apply Min-max Scaler to all four datasets\n",
    "X_train_iris_minmax, X_test_iris_minmax = apply_min_max_scaler(X_train_iris_noisy, X_test_iris_noisy)\n",
    "X_train_digits_minmax, X_test_digits_minmax = apply_min_max_scaler(X_train_digits_noisy, X_test_digits_noisy)\n",
    "X_train_wine_minmax, X_test_wine_minmax = apply_min_max_scaler(X_train_wine_noisy, X_test_wine_noisy)\n",
    "X_train_breast_cancer_minmax, X_test_breast_cancer_minmax = apply_min_max_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Maximum Absolute Scaler\n",
    "\n",
    "The Maximum Absolute Scaler ($MAS$) scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. It does not shift/center the data, and thus does not destroy any sparsity. The transformation is given by:\n",
    "\n",
    "$$\n",
    "MAS(x) = \\frac{x}{|x_{max}|}\n",
    "$$\n",
    "\n",
    "where $x$ is the original feature vector, and $x_{max, abs}$ is the maximum absolute value in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Maximum Absolute Scaler to a dataset\n",
    "def apply_max_abs_scaler(X_train, X_test):\n",
    "    max_abs_scaler = MaxAbsScaler()\n",
    "    X_train_scaled = max_abs_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = max_abs_scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Apply Maximum Absolute Scaler to all four datasets\n",
    "X_train_iris_maxabs, X_test_iris_maxabs = apply_max_abs_scaler(X_train_iris_noisy, X_test_iris_noisy)\n",
    "X_train_digits_maxabs, X_test_digits_maxabs = apply_max_abs_scaler(X_train_digits_noisy, X_test_digits_noisy)\n",
    "X_train_wine_maxabs, X_test_wine_maxabs = apply_max_abs_scaler(X_train_wine_noisy, X_test_wine_noisy)\n",
    "X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs = apply_max_abs_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Robust Scaler\n",
    "\n",
    "The Robust Scaler ($RS$) scales the data using the median ($Q_2$) and the interquartile range ($IQR$, $Q_3 - Q_1$), making it robust to outliers. The transformation is given by:\n",
    "\n",
    "$$\n",
    "RS(x) = \\frac{x - Q_2}{IQR}\n",
    "$$\n",
    "\n",
    "where $x$ is the original feature vector, $Q_2$ is the median of the feature vector, and $IQR$ is the interquartile range of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Robust Scaler to a dataset\n",
    "def apply_robust_scaler(X_train, X_test):\n",
    "    robust_scaler = RobustScaler()\n",
    "    X_train_scaled = robust_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = robust_scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Apply Robust Scaler to all four datasets\n",
    "X_train_iris_robust, X_test_iris_robust = apply_robust_scaler(X_train_iris_noisy, X_test_iris_noisy)\n",
    "X_train_digits_robust, X_test_digits_robust = apply_robust_scaler(X_train_digits_noisy, X_test_digits_noisy)\n",
    "X_train_wine_robust, X_test_wine_robust = apply_robust_scaler(X_train_wine_noisy, X_test_wine_noisy)\n",
    "X_train_breast_cancer_robust, X_test_breast_cancer_robust = apply_robust_scaler(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Quantile Transformer\n",
    "\n",
    "The Quantile Transformer ($QT$) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n",
    "\n",
    "$$\n",
    "QT(x) = F^{-1}(F(x))\n",
    "$$\n",
    "\n",
    "where $F(x)$ is the cumulative distribution function of the data, and $F^{-1}$ is the inverse function of $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Quantile Transformer to a dataset\n",
    "def apply_quantile_transformer(X_train, X_test):\n",
    "    quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "    X_train_scaled = quantile_transformer.fit_transform(X_train)\n",
    "    X_test_scaled = quantile_transformer.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Apply Quantile Transformer to all four datasets\n",
    "X_train_iris_quantile, X_test_iris_quantile = apply_quantile_transformer(X_train_iris_noisy, X_test_iris_noisy)\n",
    "X_train_digits_quantile, X_test_digits_quantile = apply_quantile_transformer(X_train_digits_noisy, X_test_digits_noisy)\n",
    "X_train_wine_quantile, X_test_wine_quantile = apply_quantile_transformer(X_train_wine_noisy, X_test_wine_noisy)\n",
    "X_train_breast_cancer_quantile, X_test_breast_cancer_quantile = apply_quantile_transformer(X_train_breast_cancer_noisy, X_test_breast_cancer_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "\n",
    "We will now compare the performance of six classification models on the different scaled datasets. The models we will use are:\n",
    "\n",
    "1. Random Forest\n",
    "2. Support Vector Machine (SVM)\n",
    "3. Decision Tree\n",
    "4. Naive Bayes (GaussianNB)\n",
    "5. K-Nearest Neighbors (KNN)\n",
    "6. Logistic Regression\n",
    "\n",
    "For each scaling method, we will train and evaluate all six models for all four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "nb_classifier = GaussianNB()\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "lr_classifier = LogisticRegression()\n",
    "\n",
    "# Lists to store accuracy scores\n",
    "accuracy_scores = []\n",
    "\n",
    "# Loop through each dataset and scaling method, and evaluate the models\n",
    "datasets = [\n",
    "    (\"Iris\", X_train_iris_noisy, X_test_iris_noisy, y_train_iris, y_test_iris),\n",
    "    (\"Digits\", X_train_digits_noisy, X_test_digits_noisy, y_train_digits, y_test_digits),\n",
    "    (\"Wine\", X_train_wine_noisy, X_test_wine_noisy, y_train_wine, y_test_wine),\n",
    "    (\"Breast Cancer\", X_train_breast_cancer_noisy, X_test_breast_cancer_noisy, y_train_breast_cancer, y_test_breast_cancer)\n",
    "]\n",
    "\n",
    "scaling_methods = {\n",
    "    \"No Scaling\": {\n",
    "        \"Iris\": [X_train_iris_noisy, X_test_iris_noisy],\n",
    "        \"Digits\": [X_train_digits_noisy, X_test_digits_noisy],\n",
    "        \"Wine\": [X_train_wine_noisy, X_test_wine_noisy],\n",
    "        \"Breast Cancer\": [X_train_breast_cancer_noisy, X_test_breast_cancer_noisy]\n",
    "    },\n",
    "    \"Standard Scaler\": {\n",
    "        \"Iris\": [X_train_iris_standard, X_test_iris_standard],\n",
    "        \"Digits\": [X_train_digits_standard, X_test_digits_standard],\n",
    "        \"Wine\": [X_train_wine_standard, X_test_wine_standard],\n",
    "        \"Breast Cancer\": [X_train_breast_cancer_standard, X_test_breast_cancer_standard]\n",
    "    },\n",
    "    \"Min-max Scaler\": {\n",
    "        \"Iris\": [X_train_iris_minmax, X_test_iris_minmax],\n",
    "        \"Digits\": [X_train_digits_minmax, X_test_digits_minmax],\n",
    "        \"Wine\": [X_train_wine_minmax, X_test_wine_minmax],\n",
    "        \"Breast Cancer\": [X_train_breast_cancer_minmax, X_test_breast_cancer_minmax]\n",
    "    },\n",
    "    \"Maximum Absolute Scaler\": {\n",
    "        \"Iris\": [X_train_iris_maxabs, X_test_iris_maxabs],\n",
    "        \"Digits\": [X_train_digits_maxabs, X_test_digits_maxabs],\n",
    "        \"Wine\": [X_train_wine_maxabs, X_test_wine_maxabs],\n",
    "        \"Breast Cancer\": [X_train_breast_cancer_maxabs, X_test_breast_cancer_maxabs]\n",
    "    },\n",
    "    \"Robust Scaler\": {\n",
    "        \"Iris\": [X_train_iris_robust, X_test_iris_robust],\n",
    "        \"Digits\": [X_train_digits_robust, X_test_digits_robust],\n",
    "        \"Wine\": [X_train_wine_robust, X_test_wine_robust],\n",
    "        \"Breast Cancer\": [X_train_breast_cancer_robust, X_test_breast_cancer_robust]\n",
    "    },\n",
    "    \"Quantile Transformer\": {\n",
    "        \"Iris\": [X_train_iris_quantile, X_test_iris_quantile],\n",
    "        \"Digits\": [X_train_digits_quantile, X_test_digits_quantile],\n",
    "        \"Wine\": [X_train_wine_quantile, X_test_wine_quantile],\n",
    "        \"Breast Cancer\": [X_train_breast_cancer_quantile, X_test_breast_cancer_quantile]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through datasets and scaling methods\n",
    "for dataset_name, X_train, X_test, y_train, y_test in datasets:\n",
    "    for scaler_name, scaled_data in scaling_methods.items():\n",
    "        X_train_scaled, X_test_scaled = scaled_data[dataset_name]\n",
    "\n",
    "        # Train and evaluate all six models\n",
    "        for classifier, classifier_name in zip([rf_classifier, svm_classifier, dt_classifier, nb_classifier, knn_classifier, lr_classifier], [\"Random Forest\", \"SVM\", \"Decision Tree\", \"Naive Bayes\", \"K-Nearest Neighbors\", \"Logistic Regression\"]):\n",
    "            classifier.fit(X_train_scaled, y_train)\n",
    "            predictions = classifier.predict(X_test_scaled)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            accuracy_scores.append([dataset_name, scaler_name, classifier_name, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "Let's analyze the results of our experiment and discuss the impact of different scaling methods on multiple classification models for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Scaling Method</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>K-Nearest Neighbors</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>Maximum Absolute Scaler</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>Min-max Scaler</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>Quantile Transformer</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>Robust Scaler</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>Standard Scaler</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Digits</td>\n",
       "      <td>Maximum Absolute Scaler</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Digits</td>\n",
       "      <td>Min-max Scaler</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.994444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Digits</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.991667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Digits</td>\n",
       "      <td>Quantile Transformer</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Digits</td>\n",
       "      <td>Robust Scaler</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.969444</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.905556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Digits</td>\n",
       "      <td>Standard Scaler</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.969444</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Maximum Absolute Scaler</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Min-max Scaler</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iris</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Quantile Transformer</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Robust Scaler</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Standard Scaler</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Maximum Absolute Scaler</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Min-max Scaler</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wine</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Quantile Transformer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Robust Scaler</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Standard Scaler</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset           Scaling Method  Decision Tree  \\\n",
       "0   Breast Cancer  Maximum Absolute Scaler       0.929825   \n",
       "1   Breast Cancer           Min-max Scaler       0.929825   \n",
       "2   Breast Cancer               No Scaling       0.929825   \n",
       "3   Breast Cancer     Quantile Transformer       0.929825   \n",
       "4   Breast Cancer            Robust Scaler       0.929825   \n",
       "5   Breast Cancer          Standard Scaler       0.929825   \n",
       "6          Digits  Maximum Absolute Scaler       0.850000   \n",
       "7          Digits           Min-max Scaler       0.850000   \n",
       "8          Digits               No Scaling       0.850000   \n",
       "9          Digits     Quantile Transformer       0.850000   \n",
       "10         Digits            Robust Scaler       0.850000   \n",
       "11         Digits          Standard Scaler       0.850000   \n",
       "12           Iris  Maximum Absolute Scaler       0.933333   \n",
       "13           Iris           Min-max Scaler       0.933333   \n",
       "14           Iris               No Scaling       0.933333   \n",
       "15           Iris     Quantile Transformer       0.966667   \n",
       "16           Iris            Robust Scaler       0.933333   \n",
       "17           Iris          Standard Scaler       0.933333   \n",
       "18           Wine  Maximum Absolute Scaler       1.000000   \n",
       "19           Wine           Min-max Scaler       1.000000   \n",
       "20           Wine               No Scaling       1.000000   \n",
       "21           Wine     Quantile Transformer       1.000000   \n",
       "22           Wine            Robust Scaler       1.000000   \n",
       "23           Wine          Standard Scaler       1.000000   \n",
       "\n",
       "    K-Nearest Neighbors  Logistic Regression  Naive Bayes  Random Forest  \\\n",
       "0              0.807018             0.947368     0.956140       0.956140   \n",
       "1              0.938596             0.947368     0.956140       0.956140   \n",
       "2              0.956140             0.956140     0.956140       0.956140   \n",
       "3              0.956140             0.947368     0.956140       0.956140   \n",
       "4              0.929825             0.938596     0.956140       0.956140   \n",
       "5              0.929825             0.947368     0.956140       0.956140   \n",
       "6              0.977778             0.972222     0.894444       0.963889   \n",
       "7              0.983333             0.972222     0.894444       0.963889   \n",
       "8              0.986111             0.972222     0.894444       0.963889   \n",
       "9              0.933333             0.947222     0.925000       0.966667   \n",
       "10             0.805556             0.969444     0.894444       0.963889   \n",
       "11             0.955556             0.969444     0.894444       0.963889   \n",
       "12             1.000000             1.000000     1.000000       0.966667   \n",
       "13             1.000000             1.000000     1.000000       0.966667   \n",
       "14             0.966667             0.966667     1.000000       0.966667   \n",
       "15             1.000000             1.000000     1.000000       1.000000   \n",
       "16             1.000000             1.000000     1.000000       0.966667   \n",
       "17             1.000000             1.000000     1.000000       0.966667   \n",
       "18             0.972222             1.000000     1.000000       1.000000   \n",
       "19             0.972222             1.000000     1.000000       1.000000   \n",
       "20             0.722222             0.972222     1.000000       1.000000   \n",
       "21             0.944444             1.000000     0.972222       1.000000   \n",
       "22             1.000000             1.000000     1.000000       1.000000   \n",
       "23             1.000000             1.000000     1.000000       1.000000   \n",
       "\n",
       "         SVM  \n",
       "0   0.929825  \n",
       "1   0.947368  \n",
       "2   0.947368  \n",
       "3   0.929825  \n",
       "4   0.947368  \n",
       "5   0.929825  \n",
       "6   0.988889  \n",
       "7   0.994444  \n",
       "8   0.991667  \n",
       "9   0.975000  \n",
       "10  0.905556  \n",
       "11  0.986111  \n",
       "12  1.000000  \n",
       "13  1.000000  \n",
       "14  0.966667  \n",
       "15  1.000000  \n",
       "16  1.000000  \n",
       "17  1.000000  \n",
       "18  1.000000  \n",
       "19  1.000000  \n",
       "20  0.805556  \n",
       "21  1.000000  \n",
       "22  1.000000  \n",
       "23  1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(accuracy_scores, columns=['Dataset', 'Scaling Method', 'Classifier', 'Accuracy'])\n",
    "results_df\n",
    "\n",
    "# Create a new DataFrame to display the results\n",
    "results_pivoted_df = results_df.pivot(index=['Dataset', 'Scaling Method'], columns='Classifier', values='Accuracy')\n",
    "results_pivoted_df.reset_index(inplace=True)\n",
    "results_pivoted_df.columns.name = None  # Remove the column name\n",
    "\n",
    "# Fill any missing values with NaN (for clarity)\n",
    "results_pivoted_df.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "results_pivoted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Results\n",
    "\n",
    "The evaluation of results provides insights into the impact of different feature scaling methods on multiple classification models for four distinct datasets: Breast Cancer, Digits, Iris, and Wine. The following key observations can be made based on the accuracy scores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset\n",
    "\n",
    "- **No Scaling**: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM all achieved accuracy scores ranging from 0.9298 to 0.9561. The dataset's features were well-scaled by default, resulting in consistent model performance.\n",
    "\n",
    "- **Standard Scaler**, **Min-max Scaler**, **Quantile Transformer**: These scaling methods led to accuracy scores similar to the no scaling scenario, demonstrating that for the Breast Cancer dataset, feature scaling had minimal impact on model performance.\n",
    "\n",
    "- **Maximum Absolute Scaler**: This method maintained accuracy scores consistent with no scaling, indicating that it didn't significantly influence the models' performance.\n",
    "\n",
    "- **Robust Scaler**: Robust scaling resulted in accuracy scores similar to the no scaling scenario, suggesting that it had limited impact on model outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits Dataset\n",
    "\n",
    "- **No Scaling**: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM exhibited accuracy scores ranging from 0.8500 to 0.9861. The Digits dataset's features were relatively well-scaled, and most models performed consistently without additional scaling.\n",
    "\n",
    "- **Standard Scaler**, **Min-max Scaler**: These scaling methods preserved accuracy scores comparable to no scaling, suggesting that feature scaling did not significantly affect model performance.\n",
    "\n",
    "- **Maximum Absolute Scaler**: For Decision Tree, K-Nearest Neighbors, and Logistic Regression, this scaling method maintained accuracy scores similar to the no scaling scenario. However, it slightly improved the performance of Naive Bayes, Random Forest, and SVM.\n",
    "\n",
    "- **Quantile Transformer**: Decision Tree, K-Nearest Neighbors, Logistic Regression, and Naive Bayes displayed accuracy scores similar to no scaling, while Random Forest and SVM achieved slightly better performance.\n",
    "\n",
    "- **Robust Scaler**: This method had a mixed impact, with some models showing similar accuracy to no scaling, while others, like K-Nearest Neighbors, experienced reduced accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset\n",
    "\n",
    "- **No Scaling**: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM achieved accuracy scores ranging from 0.9333 to 1.0000. The Iris dataset's features were naturally well-scaled, and scaling did not significantly influence model performance.\n",
    "\n",
    "- **Standard Scaler**, **Min-max Scaler**, **Maximum Absolute Scaler**, **Robust Scaler**: These scaling methods also resulted in consistent accuracy scores similar to no scaling, suggesting that feature scaling had minimal impact on model performance for this dataset.\n",
    "\n",
    "- **Quantile Transformer**: The Quantile Transformer had a positive impact, leading to perfect accuracy for most models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset\n",
    "\n",
    "- **No Scaling**: Decision Tree, K-Nearest Neighbors, Logistic Regression, Naive Bayes, Random Forest, and SVM exhibited accuracy scores ranging from 0.7222 to 1.0000. The dataset's features were not well-scaled, and this significantly affected model performance, particularly for SVM.\n",
    "\n",
    "- **Standard Scaler**, **Min-max Scaler**, **Maximum Absolute Scaler**, **Robust Scaler**, **Quantile Transformer**: These scaling methods uniformly resulted in perfect accuracy scores for all models, indicating the critical role of feature scaling in enhancing model performance for the Wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The choice of feature scaling method has a varying impact on the performance of machine learning models, depending on the characteristics of the dataset and the specific algorithms employed. In the evaluation of the Breast Cancer dataset, most scaling methods showed little influence on model accuracy, suggesting that the dataset's features were inherently well-scaled. In contrast, the Digits and Iris datasets, which had well-scaled features by default, exhibited consistent performance across various scaling techniques.\n",
    "\n",
    "However, the Wine dataset highlighted the importance of feature scaling, as the absence of scaling significantly hindered model performance, particularly for SVM. Scaling methods such as Standard Scaler, Min-max Scaler, Maximum Absolute Scaler, Robust Scaler, and Quantile Transformer proved effective in bringing about consistent and optimal model accuracy.\n",
    "\n",
    "Therefore, when working with machine learning models, it is crucial to assess the dataset's characteristics and choose an appropriate feature scaling method. This decision can have a substantial impact on the model's performance and the quality of its predictions. Careful consideration of scaling methods is a vital step in the data preprocessing phase, ensuring that the machine learning models are well-equipped to make accurate and reliable predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tale of Feature Scaling and Its Influence on Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prologue\n",
    "\n",
    "In the mystical realm of machine learning, feature scaling is a vital ritual performed by data scientists. This ritual, often considered as a preprocessing step, involves transforming the data to a common scale to ensure that no single feature dominates the learning process. In the absence of this ritual, models with distance-based algorithms can get skewed by features with larger scales.\n",
    "\n",
    "In this tale, we shall journey through the impact of various feature scaling methods on classification models, focusing on five commonly used techniques:\n",
    "\n",
    "1. Standard Scaler\n",
    "2. Min-max Scaler\n",
    "3. Maximum Absolute Scaler\n",
    "4. Robust Scaler\n",
    "5. Quantile Transformer\n",
    "\n",
    "Our adventure will revolve around the Wine dataset from scikit-learn, a commonly employed dataset for classification tasks, to demonstrate the effects of these scaling methods. The Wine dataset contains information about different wines and their classification into one of three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Gathering the Tools\n",
    "\n",
    "To embark on our journey, we first need to gather the necessary tools. In our case, these tools are various libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Unveiling the Wine Dataset\n",
    "\n",
    "Our adventure begins with the Wine dataset. Let's load the dataset and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0     0.0  \n",
       "1                          3.40   1050.0     0.0  \n",
       "2                          3.17   1185.0     0.0  \n",
       "3                          3.45   1480.0     0.0  \n",
       "4                          2.93    735.0     0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Create a DataFrame for the dataset\n",
    "wine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wine dataset consists of various features related to wine properties and a 'target' column indicating the class of the wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Preparing for the Journey (Data Preprocessing)\n",
    "\n",
    "Before we proceed with feature scaling, we need to split the data into training and testing sets. To make our journey more challenging and interesting, we will create a noisy version of the Wine dataset by adding random noise to the feature values. This noisy dataset will introduce variations that can better showcase the effects of different scaling methods on classification model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Adding random noise to the dataset\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0, 0.2, size=X.shape)\n",
    "X_noisy = X + noise\n",
    "\n",
    "# Split the noisy data into training and testing sets\n",
    "X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: The Many Faces of Feature Scaling\n",
    "\n",
    "In this chapter, we will explore five different feature scaling methods and their mathematical foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standard Scaler\n",
    "\n",
    "The Standard Scaler, also known as Z-score normalization, transforms the data so that it has a mean ($\\mu$) of 0 and a standard deviation ($\\sigma$) of 1. The transformation is defined by the equation:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This method assumes that the data is normally distributed. If the data is not normally distributed, this scaler could distort the data distribution, leading to suboptimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Standard Scaler\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_standard = standard_scaler.fit_transform(X_train_noisy)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_standard = standard_scaler.transform(X_test_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Min-max Scaler\n",
    "\n",
    "The Min-max Scaler, also known as normalization, scales the data to a specific range, typically between 0 and 1. The transformation is defined by the equation:\n",
    "\n",
    "$$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "It is suitable for data that does not follow a normal distribution. However, it is sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Min-max Scaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train_noisy)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_minmax = min_max_scaler.transform(X_test_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Maximum Absolute Scaler\n",
    "\n",
    "The Maximum Absolute Scaler scales the data based on the maximum absolute value, making the largest value in each feature equal to 1. The transformation is defined by the equation:\n",
    "\n",
    "$$x_{scaled} = \\frac{x}{|x_{max}|}$$\n",
    "\n",
    "It does not distort the data and keeps zero values at zero, making it suitable for sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Maximum Absolute Scaler\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train_noisy)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Robust Scaler\n",
    "\n",
    "The Robust Scaler scales the data using the median and the interquartile range (IQR), making it robust to outliers. The transformation is defined by the equation:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - Q1}{Q3 - Q1}$$\n",
    "\n",
    "Where $Q1$ and $Q3$ are the first and third quartiles, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Robust Scaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_robust = robust_scaler.fit_transform(X_train_noisy)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_robust = robust_scaler.transform(X_test_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Quantile Transformer\n",
    "\n",
    "The Quantile Transformer ($QT$) applies a non-linear transformation to the data, mapping it to a uniform or normal distribution. This method can be helpful when the data is not normally distributed. It computes the cumulative distribution function (CDF) of the data to place each value within the range of the distribution. The transformation is given by:\n",
    "\n",
    "$$\n",
    "QT(x) = F^{-1}(F(x))\n",
    "$$\n",
    "\n",
    "where $F(x)$ is the cumulative distribution function of the data, and $F^{-1}$ is the inverse function of $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Quantile Transformer\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_quantile = quantile_transformer.fit_transform(X_train_noisy)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_quantile = quantile_transformer.transform(X_test_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: The Battle of Classification Models\n",
    "\n",
    "In this chapter, we will witness the battle of two classification models, Random Forest and Support Vector Machine (SVM), as they compete on the different scaled datasets. For each scaling method, we will train and evaluate both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Lists to store accuracy scores\n",
    "accuracy_scores = []\n",
    "\n",
    "# Loop through each scaled dataset and evaluate the models\n",
    "for X_train_scaled, X_test_scaled, scaler_name in zip(\n",
    "    [X_train_noisy, X_train_standard, X_train_minmax, X_train_maxabs, X_train_robust, X_train_quantile],\n",
    "    [X_test_noisy, X_test_standard, X_test_minmax, X_test_maxabs, X_test_robust, X_test_quantile],\n",
    "    [\"No Scaling\", \"Standard Scaler\", \"Min-max Scaler\", \"Maximum Absolute Scaler\", \"Robust Scaler\", \"Quantile Transformer\"]\n",
    "):\n",
    "    # Train the Random Forest model\n",
    "    rf_classifier.fit(X_train_scaled, y_train)\n",
    "    rf_predictions = rf_classifier.predict(X_test_scaled)\n",
    "    \n",
    "    # Train the SVM model\n",
    "    svm_classifier.fit(X_train_scaled, y_train)\n",
    "    svm_predictions = svm_classifier.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy scores for both models\n",
    "    rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "    \n",
    "    # Store the accuracy scores for comparison\n",
    "    accuracy_scores.append([scaler_name, rf_accuracy, svm_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: The Aftermath (Results and Discussion)\n",
    "\n",
    "The battle is over. Let's analyze the aftermath of our experiment and discuss the impact of different scaling methods on the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaling Method</th>\n",
       "      <th>Random Forest Accuracy</th>\n",
       "      <th>SVM Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standard Scaler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Min-max Scaler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maximum Absolute Scaler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robust Scaler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quantile Transformer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Scaling Method  Random Forest Accuracy  SVM Accuracy\n",
       "0               No Scaling                     1.0      0.805556\n",
       "1          Standard Scaler                     1.0      1.000000\n",
       "2           Min-max Scaler                     1.0      1.000000\n",
       "3  Maximum Absolute Scaler                     1.0      1.000000\n",
       "4            Robust Scaler                     1.0      1.000000\n",
       "5     Quantile Transformer                     1.0      1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(accuracy_scores, columns=['Scaling Method', 'Random Forest Accuracy', 'SVM Accuracy'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Interpreting the Aftermath (Evaluation of Results)\n",
    "\n",
    "The results of our grand experiment have been revealed. It's time to interpret what they mean for our journey.\n",
    "\n",
    "The output from the notebook provides accuracy scores for two classification models, Random Forest and Support Vector Machine (SVM), using different feature scaling methods. Here's a summary of the results:\n",
    "\n",
    "- **No Scaling**: With no scaling applied, Random Forest achieved perfect accuracy (1.0), while SVM achieved an accuracy of approximately 0.8056. This shows that even without scaling, some models like Random Forest can perform well. However, SVM, being a distance-based algorithm, suffered due to the lack of scaling.\n",
    "\n",
    "- **Standard Scaler**, **Min-max Scaler**, **Maximum Absolute Scaler**, **Robust Scaler**, and **Quantile Transformer**: For all these scaling methods, both Random Forest and SVM achieved perfect accuracy (1.0). This indicates that these scaling methods worked exceptionally well for the Wine dataset, improving the SVM's performance significantly compared to when no scaling was applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8: Pondering the Implications (Discussion)\n",
    "\n",
    "The results of our experiment have provided us with some valuable insights into the impact of different feature scaling methods on classification models. Here are some key observations:\n",
    "\n",
    "1. **No Scaling**: Some models, like Random Forest, can handle unscaled data well. However, for distance-based algorithms like SVM, scaling is crucial to achieve optimal performance.\n",
    "\n",
    "2. **Standard Scaler**, **Min-max Scaler**, **Maximum Absolute Scaler**, **Robust Scaler**, and **Quantile Transformer**: All these scaling methods led to perfect accuracy for both Random Forest and SVM. This demonstrates their effectiveness in ensuring all features contribute equally to the model's learning process.\n",
    "\n",
    "The results also underscore the importance of understanding the mathematical foundations of each scaling method. Each method has its strengths, weaknesses, and assumptions, which can influence its effectiveness on different datasets and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: The Moral of the Story (Conclusion)\n",
    "\n",
    "As our tale draws to a close, the moral of the story becomes clear: the choice of feature scaling method is a crucial decision in the context of classification models. The impact of scaling methods on performance can vary significantly, as seen in our experiment. \n",
    "\n",
    "When working with real-world datasets, it's essential to experiment with different scaling techniques and select the one that aligns with the data's distribution and the requirements of the machine learning model. This decision should be guided by a thorough understanding of the data's characteristics and the mathematical foundations of the scaling methods.\n",
    "\n",
    "Our experiment also highlights the importance of feature scaling as a preprocessing step and the need to consider the specific scaling method in the broader context of machine learning tasks. It reminds us that in the realm of machine learning, every decision, no matter how small, can have far-reaching implications.\n",
    "\n",
    "And so, our tale ends here, but the lessons we learned will guide us in our future adventures in the vast and mysterious realm of machine learning. Until next time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
